{
  "articles": [
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the assignments.Rmd\nfile, in the project root, to delete this placeholder text and customize\nwith your own!\n\nAssignment materials and\ndates\nAssignment materials\nAssigned\nDue date\nA link to the repo\n2021-01-01\n2021-01-08\nA link to the repo\n2021-01-15\n2021-01-23\nA link to the repo\n2021-02-01\n2021-02-09\nA link to the repo\n2021-02-14\n2021-02-23\nAssignment expectations and\ngrading\nThis might also go on the home page & in syllabus\nOr could reinforce here\n\n\n\n\n",
      "last_modified": "2022-04-29T14:36:17-07:00"
    },
    {
      "path": "Group_Project.html",
      "title": "Group Project",
      "author": [],
      "contents": "\nFinal Group Text Analysis\nProject\nGoal: The goal of this project is to use text\nanalysis to explore an environmental data science question of interest\nto your group. You will consult background literature, formulate a\nresearch question, identify and collect relevant data, carry out\nappropriate text analyses and create data visuals that will facilitate a\nfinal presentation to the rest of class.\nImportant dates\nMay 6th, 2022: Project proposal due\nJune 1st, 2022: Final report & presentation\ndue\nProject\nProposal - Due at the end of Week 6 (5/6 by 5:00 PM).\n\nEmail me your proposal at mateorobbins@gmail.com by 5pm on May 6th, 2022.\nThe purpose of the project proposal is to ensure that you have a\nfeasible project to continue to move forward with during the rest of the\nquarter. You can adjust your project plan (check in with me) if you’d\nlike to include analyses that we cover in class after Week 6 or as you\nbegin working with your data.\nIn your proposal you will describe these four components:\nBackground and Research question - Why is this\nquestion important? Does it fill gaps in our knowledge? To which\naudience(s) would it be of interest? (2-3 paragraphs) Include at least\n1-2 citations to existing literature.\nData collection plan - Where will you access\nyour data? How was this data created and by whom? How big will your data\nset be (number of reports, tweets, comments, articles, etc.). (2-3\nparagraphs) Make sure to verify that you are able to access and download\nthe data you are proposing to use.\nAnalysis plan - What is your analysis plan? Why\ndid you choose this analysis, given your data and question? What are the\nlimitations? (2-3 paragraphs, at least 1 citation to an example in the\nliterature where your methods were used)\nResults - What are your anticipated findings?\nWhat visual methods will you use to present your findings? (2-3\nparagraphs)\nProject\nwrite-up: Due Before Class Week 10 ( May 31 by 5:00 PM)\nFor the final presentation/submission, you will revisit and update\nthe four components from the proposal (see above) and add an additional\nsection (the ‘Avenues for further research’ section). The Results\nsection in particular will be different and more developed:\nResults (updated) - Show us your results using\ndiagrams, figures and/or tables. Describe in the text (and orally during\nthe presentation) what you found, and how these results either do or do\nnot help you answer your question. Discuss some implications of your\nfindings.\nAvenues for further research - Based on your\nresults and experience with this project, what further research would\nyou recommend? What additional data or analyses would help you answer\nthis question better?\nFinal\nPresentation: Due Week 10 (Presented in class on June 1)\nYour in-class presentation will last 5-7 minutes, with a few\nadditional minutes (3 minutes) to field questions from the other\nstudents. You can use whichever visual tools you think best convey your\nproject (Google Slides, R Markdown, R Shiny etc.)\nGroup Composition\nGroup\nMembers\nA\nPaloma, Allie, Wylie, Ben, Julia\nB\nSteven, Joe, Connor, Shale, Grace\nC\nAlex, Halina, Desik, Alexandra\nD\nJuliet, Charles, Scout, Peter\nE\nClarissa, Jake, Daniel, Ryan, Ilia\nF\nFelicia, Mia, Cullen, Marie\n\n\n\n",
      "last_modified": "2022-05-01T08:50:46-07:00"
    },
    {
      "path": "index.html",
      "title": "EDS 231: Text and Sentiment Analysis for Environmental Problems",
      "description": "Master’s of Environmental Data Science Program, UC Santa Barbara",
      "author": [],
      "contents": "\n\n\n\nCourse description\nThis course will cover foundations and applications of natural\nlanguage processing. Problem sets and class projects will leverage\ncommon and emerging text-based data sources relevant to environmental\nproblems, including but not limited to social media feeds (e.g.,\nTwitter) and text documents (e.g., agency reports), and will build\ncapacity and experience in common tools, including text processing and\nclassification, semantics, and natural language parsing.\nInstructor\nMateo Robbins (mateorobbins@gmail.com)\nOffice: MEDS Group Office\nOffice hours: Wednesday 11:00 - 12:00\nImportant links\nLink\nto full course syllabus\nWeekly course schedule\nLectures: W 9:30 - 10:45am (NCEAS Classroom)\nLearning objectives\nThe goal of EDS 231 (Text and Sentiment Analysis for Environmental\nProblems) is to expose students to a range of text analysis and natural\nlanguage processing data sources, techniques and tools for analysis that\ncan be applied to environmental problems. During this course, students\nwill:\nBecome familiar with the R packages used in text-as-data\napplications\nConduct and explain each step in the text data collection,\nanalysis, and presentation pipeline\nEvaluate examples of text analysis in the environmental science\nliterature\nWork with peers on a group text analysis project, then\ncommunicate the analysis to the rest of the class\nCourse requirements\nComputing\nMinimum MEDS device requirements (bring to all sessions +\ncharger!)\nUp-to-date R and RStudio\nPython version 3.x installed (although most, if not all, work\nwill be done in RStudio)\nTextbook\nReadings will be drawn from free online ebooks:\nText Mining with\nR\nQuanteda\nTutorials\n\nTentative topics\nWeek\nLecture\nReading\nAssignment\n1 (3/30)\nCourse\nIntro and Text Analysis Overview\nn/a\nn/a\n2 (4/06)\nText\nData in R\nRDS (14.1 -\n14.3.1), TMR\nCh. 1\nTopic\n2\n3 (4/13)\nSentiment analysis I\nTMR Ch.\n2\nTopic\n3\n4 (4/20)\nSentiment analysis II\n-\nTopic\n4\n5 (4/27)\nWord relationship analysis\nTMR Ch. 4,\nQT Ch.\n4\nTopic\n5\n6 (5/4)\nTopic\nanalysis\nTMR Ch.\n6\nTopic\n6\n7 (5/11)\nWord Embeddings\nSMLTAR Ch. 5\nTopic\n7\n8 (5/18)\nClassification\nlink\nTopic 8\n9 (5/25)\nTBD\nlink\nTBA\n10 (6/01)\nGroup Presentations\nlink\nTBA\n\n\n\n",
      "last_modified": "2022-05-11T00:31:38-07:00"
    },
    {
      "path": "resources.html",
      "title": "Course resources",
      "author": [],
      "contents": "\n\nData sources\nUCSB Library\nAll\nlibrary text mining resources\nScholarly\nJournals\nTwitter\ndata\nKaggle\n#NLP\nAPIs\nNew York Times\nGoogle\nGoogle\nTrends\nGovernment Sources\nRegulations.gov Public\nComments\nDepartment\nof Transportation Pipeline Incident Reports\nResearch Projects\nJaime’s\nSB Twitter Tourism Analysis\n\n\n\n",
      "last_modified": "2022-05-01T08:50:46-07:00"
    },
    {
      "path": "topic_1.html",
      "title": "Topic 1: Add/remove a site page",
      "author": [],
      "contents": "\nNOTE: There are 10 toy Topic sections here,\nexpecting that some teachers may want to have one page per week (for a\n10 week course). You are encouraged to structure your course\nhowever works best for your class. All of your course\ninformation could be on a single page, or you might have a different\nnumber of topics, or organize weekly, or any other organization that\nworks for you.\n\nTO UPDATE THIS PAGE: Open and edit the topic_1.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nAdd a site page\nThe quick version:\nMake a new R Markdown document, save\nAdd the rendered .html to _site.yaml so the page exists on the\nsite\nBuild to see updated site\nBelow for a bit more detail…\nMake each page an R\nMarkdown document\nTo make a new page:\nWithin your website Project, create a new .Rmd (File > New\nFile > R Markdown). Save it to the project root. For this example,\nlet’s say you’ve saved it as new_page.Rmd.\nIn that .Rmd file, remove everything but the title (which you can\nchange) from the YAML - that’s the top section of the .Rmd, where by\ndefault it has title, author, date, etc.\nUpdate the .Rmd to contain whatever you want to have on that\npage. Don’t know a lot about markdown? Considering switching over to the\nVisual\nEditor in RStudio (versions >= 1.4).\nSave the .Rmd\nAdd it to your navigation\nbar\nOpen the _site.yml file in your Project\nAdd the information to the YAML navbar section, which will almost\nalways be the text that you want to appear in the navigation bar, and\nthe file name of the knitted html that will be\nautomatically rendered to /docs when you Build your website. That would\nbe new_page.html for this example (since the .Rmd it is\nrendered from is new_page.Rmd). So in the\n_site.yml I would need to add this to the navbar\nsection:\n    - text: \"A new page!\"\n      href: new_page.html\nNote: YAML is space & indentation specific.\nFollow the structure that already exists in this template to avoid YAML\nerrors.\nHow is the website finding the html? Notice in the\n_site.yml file, the output_dir is set to\ndocs. That means when we press ‘Build website’ (in the\nBuild tab in RStudio), our .Rmd pages are knitted to HTML & sent to\nthe _docs folder. This is also important because when we\ndeploy the site (make it live), we will want to deploy from that\ndocs folder using GitHub pages.\nTake a look at some other pages in this template (Resources,\nAssignments, etc.) to see the structure, & give it a shot!\nDelete/disappear a site page\nThe safest thing to do if you don’t want a page to show up\nis to remove it from the _site.yml navbar listings. That\nway, the material on the page still exists as a file in your project,\nbut doesn’t show up on the website – don’t delete a page file unless you\nare REALLY SURE that you’re never going to want the material on that\npage ever again.\n\n\n\n",
      "last_modified": "2022-04-29T14:36:26-07:00"
    },
    {
      "path": "topic_10.html",
      "title": "Topic 10: Inserting tables",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2022-04-29T14:36:28-07:00"
    },
    {
      "path": "topic_2_copy.html",
      "title": "topic_2_copy",
      "author": [],
      "contents": "\nToday we will be grabbing some data from the New York Times database\nvia their API, then running some basic string manipulations, trying out\nthe tidytext format, and creating some basic plots.\nConnect to\nthe New York Times API and send a query\n\n\nlibrary(jsonlite) #convert results from API queries into R-friendly formats \nlibrary(tidyverse) \nlibrary(tidytext) #text data management and analysis\nlibrary(ggplot2) #plot word frequencies and publication dates\n\n\n\nWe have to decide which New York Times articles we are interested in\nexamining. For this exercise, I chose articles about Deb Haaland, the\ncurrent US Secretary of the Interior. As a member of the Laguna Pueblo\nTribe, Haaland is the first Native American to serve as Cabinet\nsecretary. Very cool!\n\n\n\n#create an object called x with the results of our query (\"haaland\")\n# the from JSON flatten the JSON object, then convert to a data frame\nt <- fromJSON(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", flatten = TRUE) #the string following \"key=\" is your API key \n\nclass(t) #what type of object is x?\n\nt <- t %>% \n  data.frame()\n\n\n#Inspect our data\nclass(t) #now what is it?\ndim(t) # how big is it?\nnames(t) # what variables are we working with?\n#t <- readRDS(\"nytDat.rds\") #in case of emergency :)\n\n\n\nThe name format, response.xxx.xxx…, is a legacy of the json nested\nhierarchy.\nLet’s look at a piece of text. Our data object has a variable called\n“response.docs.snippet” that contains a short excerpt, or “snippet” from\nthe article. Let’s grab a snippet and try out some basic ‘stringr’\nfunctions.\n\n\nt$response.docs.snippet[9]\n\n#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.\n\nx <- \"Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance.\" \n\ntolower(x)\nstr_split(x, ','); str_split(x, 't')\nstr_replace(x, 'historic', 'without precedent')\nstr_replace(x, ' ', '_') #first one\nstr_replace_all(x, ' ', '_') #all of them\nstr_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F\nstr_locate(x, 't'); str_locate_all(x, 'as')\n\n\n\nOK,\nit’s working but we want more data. Let’s set some parameters for a\nbigger query\n\n\nterm <- \"Haaland\" # Need to use + to string together separate words\nbegin_date <- \"20210120\"\nend_date <- \"20220401\"\n\n#construct the query url using API operators\nbaseurl <- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term,\n                  \"&begin_date=\",begin_date,\"&end_date=\",end_date,\n                  \"&facet_filter=true&api-key=\",\"NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", sep=\"\")\n\n#examine our query url\nbaseurl\n\n\n[1] \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Haaland&begin_date=20210120&end_date=20220401&facet_filter=true&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\"\n\n\n\n initialQuery <- fromJSON(baseurl)\nmaxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) \n\npages <- list()\nfor(i in 0:maxPages){\n  nytSearch <- fromJSON(paste0(baseurl, \"&page=\", i), flatten = TRUE) %>% data.frame() \n  message(\"Retrieving page \", i)\n  pages[[i+1]] <- nytSearch \n  Sys.sleep(1) \n}\nclass(nytSearch)\n\nnytDat <- rbind_pages(pages)\nnytDat <- as_tibble(nytDat)\n\n\n\n\n\nnytDat <- read.csv(\"nytDat.csv\")\n\nnytDat %>% \n  group_by(response.docs.type_of_material) %>%\n  summarize(count=n()) %>%\n  mutate(percent = (count / sum(count))*100) %>%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = \"identity\") + coord_flip()\n\n\n\n\n\n\nnytDat %>%\n  mutate(pubDay=gsub(\"T.*\",\"\",response.docs.pub_date)) %>%\n  group_by(pubDay) %>%\n  summarise(count=n()) %>%\n  filter(count >= 2) %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(pubDay, count), y=count), stat=\"identity\") + coord_flip()\n\n\n\n\nThe New York Times doesn’t make full text of the articles available\nthrough the API. But we can use the first paragraph of each article.\n\n\nnames(nytDat)\n\n\n [1] \"status\"                               \n [2] \"copyright\"                            \n [3] \"response.docs.abstract\"               \n [4] \"response.docs.web_url\"                \n [5] \"response.docs.snippet\"                \n [6] \"response.docs.lead_paragraph\"         \n [7] \"response.docs.source\"                 \n [8] \"response.docs.multimedia\"             \n [9] \"response.docs.keywords\"               \n[10] \"response.docs.pub_date\"               \n[11] \"response.docs.document_type\"          \n[12] \"response.docs.news_desk\"              \n[13] \"response.docs.section_name\"           \n[14] \"response.docs.type_of_material\"       \n[15] \"response.docs._id\"                    \n[16] \"response.docs.word_count\"             \n[17] \"response.docs.uri\"                    \n[18] \"response.docs.print_section\"          \n[19] \"response.docs.print_page\"             \n[20] \"response.docs.subsection_name\"        \n[21] \"response.docs.headline.main\"          \n[22] \"response.docs.headline.kicker\"        \n[23] \"response.docs.headline.content_kicker\"\n[24] \"response.docs.headline.print_headline\"\n[25] \"response.docs.headline.name\"          \n[26] \"response.docs.headline.seo\"           \n[27] \"response.docs.headline.sub\"           \n[28] \"response.docs.byline.original\"        \n[29] \"response.docs.byline.person\"          \n[30] \"response.docs.byline.organization\"    \n[31] \"response.meta.hits\"                   \n[32] \"response.meta.offset\"                 \n[33] \"response.meta.time\"                   \n\nparagraph <- names(nytDat)[6] #The 6th column, \"response.doc.lead_paragraph\", is the one we want here.  \ntokenized <- nytDat %>%\n  unnest_tokens(word, paragraph)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>% #illegible with all the words displayed\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nUh oh, who knows what we need to do here?\n\n\ndata(stop_words)\n\ntokenized <- tokenized %>%\n  anti_join(stop_words)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nOK, but look at the most common words. Does one stick out?\n\n\n#inspect the list of tokens (words)\ntokenized$word\n\nclean_tokens <- str_replace_all(tokenized$word,\"land[a-z,A-Z]*\",\"land\") #stem tribe words\nclean_tokens <- str_remove_all(clean_tokens, \"[:digit:]\") #remove all numbers\nclean_tokens <- str_remove_all(clean_tokens, \"washington\")\nclean_tokens <- gsub(\"’s\", '', clean_tokens)\n\ntokenized$clean <- clean_tokens\n\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n#remove the empty strings\ntib <-subset(tokenized, clean!=\"\")\n\n#reassign\ntokenized <- tib\n\n#try again\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\nAssignment (Due by Week 3)\nCreate a free New York Times account (https://developer.nytimes.com/get-started)\nPick an interesting environmental key word(s) and use the\njsonlite package to query the API. Pick something high profile enough\nand over a large enough time frame that your query yields enough\narticles for an interesting examination.\nRecreate the publications per day and word frequency plots using\nthe first paragraph\nMake some (at least 3) transformations to the corpus (add\nstopword(s), stem a key term and its variants, remove numbers)\nRecreate the publications per day and word frequency plots using the\nheadlines variable (response.docs.headline.main). Compare the\ndistributions of word frequencies between the first paragraph and\nheadlines. Do you see any difference?\n\n\n\n",
      "last_modified": "2022-04-29T14:36:46-07:00"
    },
    {
      "path": "topic_2.html",
      "title": "Topic 2: Text Data in R",
      "author": [],
      "contents": "\nToday we will be grabbing some data from the New York Times database\nvia their API, then running some basic string manipulations, trying out\nthe tidytext format, and creating some basic plots.\n#https://developer.nytimes.com/\nConnect to\nthe New York Times API and send a query\n\n\nlibrary(jsonlite) #convert results from API queries into R-friendly formats \nlibrary(tidytext) #text data management and analysis\nlibrary(ggplot2) #plot word frequencies and publication dates\n\n\n\nWe have to decide which New York Times articles we are interested in\nexamining. For this exercise, I chose articles about Deb Haaland, the\ncurrent US Secretary of the Interior. As a member of the Laguna Pueblo\nTribe, Haaland is the first Native American to serve as Cabinet\nsecretary. Very cool!\n\n\n\n#create an object called x with the results of our query (\"haaland\")\n# the from JSON flatten the JSON object, then convert to a data frame\nt <- fromJSON(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", flatten = TRUE) #the string following \"key=\" is your API key \n\nclass(t) #what type of object is t?\n\nt <- t %>% \n  data.frame()\n\n\n#Inspect our data\n#now what class is it?\n# how big is it?\ndim(t)\n# what variables are we working with?\nnames(t)\n\n\n#t <- readRDS(\"nytDat.rds\") #in case of API emergency :)\n\n\n\nThe name format, response.xxx.xxx…, is a legacy of the json nested\nhierarchy.\nLet’s look at a piece of text. Our data object has a variable called\n“response.docs.snippet” that contains a short excerpt, or “snippet” from\nthe article. Let’s grab a snippet and try out some basic ‘stringr’\nfunctions.\n\n\nt$response.docs.snippet[9]\n\n#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.\n\nx <- \"Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance.\" \n\ntolower(x)\nstr_split(x, ','); str_split(x, 't')\nstr_replace(x, 'historic', 'without precedent')\nstr_replace(x, ' ', '_') #first one\n#how do we replace all of them?\nstr_replace_all(x,' ', '_')\nstr_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F\nstr_locate(x, 't'); str_locate_all(x, 'as')\n\n\n\nOK,\nit’s working but we want more data. Let’s set some parameters for a\nbigger query\n\n\nterm <- \"Haaland\" # Need to use + to string together separate words\nbegin_date <- \"20210120\"\nend_date <- \"20220401\"\n\n#construct the query url using API operators\nbaseurl <- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term,\n                  \"&begin_date=\",begin_date,\"&end_date=\",end_date,\n                  \"&facet_filter=true&api-key=\",\"NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", sep=\"\")\n\n#examine our query url\nbaseurl\n\n\n[1] \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Haaland&begin_date=20210120&end_date=20220401&facet_filter=true&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\"\n\n\n\n#this code allows for obtaining multiple pages of query results \n initialQuery <- fromJSON(baseurl)\nmaxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) \n\npages <- list()\nfor(i in 0:maxPages){\n  nytSearch <- fromJSON(paste0(baseurl, \"&page=\", i), flatten = TRUE) %>% data.frame() \n  message(\"Retrieving page \", i)\n  pages[[i+1]] <- nytSearch \n  Sys.sleep(6) \n}\nclass(nytSearch)\n\n#need to bind the pages and create a tibble from nytDa\nnytDat <- rbind(pages)\n\n\n\n\n\nnytDat <- read.csv(\"nytDat.csv\") # obtained from \ndim(nytDat)\n\n\n[1] 120  33\n\nnytDat %>% \n  group_by(response.docs.type_of_material) %>%\n  summarize(count=n()) %>%\n  mutate(percent = (count / sum(count))*100) %>%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = \"identity\") + coord_flip()\n\n\n\n\n\n\nnytDat %>%\n  mutate(pubDay=gsub(\"T.*\",\"\",response.docs.pub_date)) %>%\n  group_by(pubDay) %>%\n  summarise(count=n()) %>%\n  filter(count >= 2) %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(pubDay, count), y=count), stat=\"identity\") + coord_flip()\n\n\n\n\nThe New York Times doesn’t make full text of the articles available\nthrough the API. But we can use the first paragraph of each article.\n\n\nnames(nytDat)\n\n\n [1] \"status\"                               \n [2] \"copyright\"                            \n [3] \"response.docs.abstract\"               \n [4] \"response.docs.web_url\"                \n [5] \"response.docs.snippet\"                \n [6] \"response.docs.lead_paragraph\"         \n [7] \"response.docs.source\"                 \n [8] \"response.docs.multimedia\"             \n [9] \"response.docs.keywords\"               \n[10] \"response.docs.pub_date\"               \n[11] \"response.docs.document_type\"          \n[12] \"response.docs.news_desk\"              \n[13] \"response.docs.section_name\"           \n[14] \"response.docs.type_of_material\"       \n[15] \"response.docs._id\"                    \n[16] \"response.docs.word_count\"             \n[17] \"response.docs.uri\"                    \n[18] \"response.docs.print_section\"          \n[19] \"response.docs.print_page\"             \n[20] \"response.docs.subsection_name\"        \n[21] \"response.docs.headline.main\"          \n[22] \"response.docs.headline.kicker\"        \n[23] \"response.docs.headline.content_kicker\"\n[24] \"response.docs.headline.print_headline\"\n[25] \"response.docs.headline.name\"          \n[26] \"response.docs.headline.seo\"           \n[27] \"response.docs.headline.sub\"           \n[28] \"response.docs.byline.original\"        \n[29] \"response.docs.byline.person\"          \n[30] \"response.docs.byline.organization\"    \n[31] \"response.meta.hits\"                   \n[32] \"response.meta.offset\"                 \n[33] \"response.meta.time\"                   \n\nparagraph <- names(nytDat)[6] #The 6th column, \"response.doc.lead_paragraph\", is the one we want here.  \ntokenized <- nytDat %>%\n  unnest_tokens(word, paragraph)\n\ntokenized[,34]\n\n\n   [1] \"the\"               \"map\"               \"dots\"             \n   [4] \"resembling\"        \"a\"                 \"scattergram\"      \n   [7] \"of\"                \"america\"           \"point\"            \n  [10] \"to\"                \"snow\"              \"covered\"          \n  [13] \"pinnacles\"         \"remote\"            \"islands\"          \n  [16] \"and\"               \"places\"            \"in\"               \n  [19] \"between\"           \"washington\"        \"the\"              \n  [22] \"united\"            \"states\"            \"government\"       \n  [25] \"netted\"            \"a\"                 \"record\"           \n  [28] \"4.37\"              \"billion\"           \"on\"               \n  [31] \"friday\"            \"from\"              \"the\"              \n  [34] \"sale\"              \"of\"                \"six\"              \n  [37] \"offshore\"          \"wind\"              \"leases\"           \n  [40] \"off\"               \"the\"               \"coasts\"           \n  [43] \"of\"                \"new\"               \"york\"             \n  [46] \"and\"               \"new\"               \"jersey\"           \n  [49] \"a\"                 \"major\"             \"step\"             \n  [52] \"in\"                \"the\"               \"biden\"            \n  [55] \"administration’s\"  \"goal\"              \"of\"               \n  [58] \"ushering\"          \"in\"                \"a\"                \n  [61] \"future\"            \"powered\"           \"by\"               \n  [64] \"renewable\"         \"energy\"            \"that\"             \n  [67] \"first\"             \"day\"               \"for\"              \n  [70] \"an\"                \"incoming\"          \"manager\"          \n  [73] \"at\"                \"a\"                 \"new\"              \n  [76] \"club\"              \"must\"              \"be\"               \n  [79] \"overwhelming\"      \"there\"             \"is\"               \n  [82] \"an\"                \"entire\"            \"squad\"            \n  [85] \"of\"                \"players\"           \"to\"               \n  [88] \"meet\"              \"to\"                \"get\"              \n  [91] \"to\"                \"know\"              \"to\"               \n  [94] \"win\"               \"over\"              \"there\"            \n  [97] \"is\"                \"a\"                 \"staff\"            \n [100] \"nervous\"           \"of\"                \"your\"             \n [103] \"intentions\"        \"and\"               \"fearful\"          \n [106] \"of\"                \"what\"              \"the\"              \n [109] \"future\"            \"may\"               \"hold\"             \n [112] \"to\"                \"convince\"          \"and\"              \n [115] \"hopefully\"         \"to\"                \"command\"          \n [118] \"washington\"        \"the\"               \"interior\"         \n [121] \"department\"        \"on\"                \"friday\"           \n [124] \"recommended\"       \"that\"              \"the\"              \n [127] \"federal\"           \"government\"        \"raise\"            \n [130] \"the\"               \"fees\"              \"that\"             \n [133] \"oil\"               \"and\"               \"gas\"              \n [136] \"companies\"         \"pay\"               \"to\"               \n [139] \"drill\"             \"on\"                \"public\"           \n [142] \"lands\"             \"the\"               \"first\"            \n [145] \"increase\"          \"in\"                \"those\"            \n [148] \"rent\"              \"and\"               \"royalty\"          \n [151] \"rates\"             \"since\"             \"1920\"             \n [154] \"washington\"        \"a\"                 \"decision\"         \n [157] \"by\"                \"the\"               \"trump\"            \n [160] \"administration\"    \"to\"                \"move\"             \n [163] \"the\"               \"headquarters\"      \"of\"               \n [166] \"the\"               \"bureau\"            \"of\"               \n [169] \"land\"              \"management\"        \"to\"               \n [172] \"grand\"             \"junction\"          \"colo\"             \n [175] \"from\"              \"washington\"        \"left\"             \n [178] \"the\"               \"agency\"            \"with\"             \n [181] \"high\"              \"vacancy\"           \"rates\"            \n [184] \"as\"                \"veteran\"           \"employees\"        \n [187] \"especially\"        \"african\"           \"americans\"        \n [190] \"quit\"              \"rather\"            \"than\"             \n [193] \"relocate\"          \"a\"                 \"government\"       \n [196] \"watchdog\"          \"said\"              \"in\"               \n [199] \"a\"                 \"report\"            \"issued\"           \n [202] \"this\"              \"week\"              \"somewhere\"        \n [205] \"in\"                \"a\"                 \"darkened\"         \n [208] \"room\"              \"erling\"            \"haaland\"          \n [211] \"was\"               \"watching\"          \"injury\"           \n [214] \"meant\"             \"he\"                \"would\"            \n [217] \"not\"               \"be\"                \"able\"             \n [220] \"to\"                \"take\"              \"the\"              \n [223] \"field\"             \"for\"               \"norway’s\"         \n [226] \"most\"              \"significant\"       \"match\"            \n [229] \"in\"                \"20\"                \"years\"            \n [232] \"the\"               \"netherlands\"       \"return\"           \n [235] \"to\"                \"partial\"           \"lockdown\"         \n [238] \"last\"              \"weekend\"           \"meant\"            \n [241] \"with\"              \"the\"               \"game\"             \n [244] \"played\"            \"behind\"            \"closed\"           \n [247] \"doors\"             \"he\"                \"would\"            \n [250] \"not\"               \"even\"              \"be\"               \n [253] \"able\"              \"to\"                \"support\"          \n [256] \"his\"               \"national\"          \"team\"             \n [259] \"from\"              \"the\"               \"stands\"           \n [262] \"washington\"        \"president\"         \"biden\"            \n [265] \"announced\"         \"on\"                \"monday\"           \n [268] \"that\"              \"his\"               \"administration\"   \n [271] \"was\"               \"moving\"            \"to\"               \n [274] \"block\"             \"new\"               \"federal\"          \n [277] \"oil\"               \"and\"               \"gas\"              \n [280] \"leasing\"           \"within\"            \"a\"                \n [283] \"10\"                \"mile\"              \"radius\"           \n [286] \"around\"            \"chaco\"             \"canyon\"           \n [289] \"in\"                \"new\"               \"mexico\"           \n [292] \"one\"               \"of\"                \"the\"              \n [295] \"nation’s\"          \"oldest\"            \"and\"              \n [298] \"most\"              \"culturally\"        \"significant\"      \n [301] \"native\"            \"american\"          \"sites\"            \n [304] \"deb\"               \"haaland\"           \"president\"        \n [307] \"biden’s\"           \"secretary\"         \"of\"               \n [310] \"the\"               \"interior\"          \"was\"              \n [313] \"among\"             \"this\"              \"year’s\"           \n [316] \"more\"              \"recognizable\"      \"amateur\"          \n [319] \"runners\"           \"forget\"            \"pantsuit\"         \n [322] \"nation\"            \"the\"               \"washington\"       \n [325] \"dress\"             \"code\"              \"is\"               \n [328] \"changing\"          \"one\"               \"swearing\"         \n [331] \"in\"                \"at\"                \"a\"                \n [334] \"time\"              \"washington\"        \"senator\"          \n [337] \"joe\"               \"manchin\"           \"iii\"              \n [340] \"the\"               \"west\"              \"virginia\"         \n [343] \"democrat\"          \"who\"               \"heads\"            \n [346] \"the\"               \"senate\"            \"energy\"           \n [349] \"committee\"         \"announced\"         \"wednesday\"        \n [352] \"that\"              \"he\"                \"would\"            \n [355] \"vote\"              \"to\"                \"confirm\"          \n [358] \"representative\"    \"deb\"               \"haaland\"          \n [361] \"of\"                \"new\"               \"mexico\"           \n [364] \"to\"                \"head\"              \"the\"              \n [367] \"interior\"          \"department\"        \"most\"             \n [370] \"likely\"            \"ensuring\"          \"that\"             \n [373] \"one\"               \"of\"                \"president\"        \n [376] \"biden’s\"           \"most\"              \"embattled\"        \n [379] \"cabinet\"           \"nominees\"          \"will\"             \n [382] \"be\"                \"confirmed\"         \"to\"               \n [385] \"office\"            \"despite\"           \"escalating\"       \n [388] \"opposition\"        \"to\"                \"her\"              \n [391] \"from\"              \"republicans\"       \"the\"              \n [394] \"senate\"            \"approved\"          \"representative\"   \n [397] \"deb\"               \"haaland\"           \"of\"               \n [400] \"new\"               \"mexico\"            \"to\"               \n [403] \"lead\"              \"the\"               \"interior\"         \n [406] \"department\"        \"making\"            \"her\"              \n [409] \"the\"               \"first\"             \"native\"           \n [412] \"american\"          \"to\"                \"lead\"             \n [415] \"a\"                 \"cabinet\"           \"agency\"           \n [418] \"on\"                \"thursday\"          \"deb\"              \n [421] \"haaland\"           \"made\"              \"history\"          \n [424] \"when\"              \"she\"               \"began\"            \n [427] \"her\"               \"job\"               \"as\"               \n [430] \"secretary\"         \"of\"                \"the\"              \n [433] \"interior\"          \"becoming\"          \"the\"              \n [436] \"first\"             \"native\"            \"american\"         \n [439] \"to\"                \"lead\"              \"a\"                \n [442] \"cabinet\"           \"level\"             \"agency\"           \n [445] \"washington\"        \"interior\"          \"secretary\"        \n [448] \"deb\"               \"haaland\"           \"has\"              \n [451] \"advised\"           \"president\"         \"biden\"            \n [454] \"to\"                \"restore\"           \"sweeping\"         \n [457] \"environmental\"     \"protections\"       \"to\"               \n [460] \"three\"             \"major\"             \"national\"         \n [463] \"monuments\"         \"that\"              \"had\"              \n [466] \"been\"              \"stripped\"          \"away\"             \n [469] \"by\"                \"former\"            \"president\"        \n [472] \"donald\"            \"j\"                 \"trump\"            \n [475] \"this\"              \"weekend\"           \"listen\"           \n [478] \"to\"                \"a\"                 \"collection\"       \n [481] \"of\"                \"narrated\"          \"articles\"         \n [484] \"from\"              \"around\"            \"the\"              \n [487] \"new\"               \"york\"              \"times\"            \n [490] \"read\"              \"aloud\"             \"by\"               \n [493] \"the\"               \"reporters\"         \"who\"              \n [496] \"wrote\"             \"the\"               \"story\"            \n [499] \"as\"                \"the\"               \"danger\"           \n [502] \"bubbled\"           \"to\"                \"the\"              \n [505] \"surface\"           \"there\"             \"was\"              \n [508] \"an\"                \"audible\"           \"intake\"           \n [511] \"of\"                \"breath\"            \"among\"            \n [514] \"manchester\"        \"city’s\"            \"substitutes\"      \n [517] \"once\"              \"it\"                \"had\"              \n [520] \"passed\"            \"a\"                 \"few\"              \n [523] \"seconds\"           \"later\"             \"as\"               \n [526] \"they\"              \"exchanged\"         \"glances\"          \n [529] \"of\"                \"admiration\"        \"of\"               \n [532] \"relief\"            \"came\"              \"a\"                \n [535] \"little\"            \"murmur\"            \"of\"               \n [538] \"appreciation\"      \"in\"                \"the\"              \n [541] \"silence\"           \"of\"                \"the\"              \n [544] \"stadium\"           \"you\"               \"could\"            \n [547] \"hear\"              \"the\"               \"sounds\"           \n [550] \"of\"                \"game\"              \"recognizing\"      \n [553] \"game\"              \"washington\"        \"the\"              \n [556] \"biden\"             \"administration\"    \"announced\"        \n [559] \"on\"                \"wednesday\"         \"a\"                \n [562] \"plan\"              \"to\"                \"develop\"          \n [565] \"large\"             \"scale\"             \"wind\"             \n [568] \"farms\"             \"along\"             \"nearly\"           \n [571] \"the\"               \"entire\"            \"coastline\"        \n [574] \"of\"                \"the\"               \"united\"           \n [577] \"states\"            \"the\"               \"first\"            \n [580] \"long\"              \"term\"              \"strategy\"         \n [583] \"from\"              \"the\"               \"government\"       \n [586] \"to\"                \"produce\"           \"electricity\"      \n [589] \"from\"              \"offshore\"          \"turbines\"         \n [592] \"representative\"    \"deb\"               \"haaland\"          \n [595] \"of\"                \"new\"               \"mexico\"           \n [598] \"president\"         \"biden’s\"           \"pick\"             \n [601] \"to\"                \"head\"              \"the\"              \n [604] \"interior\"          \"department\"        \"sought\"           \n [607] \"tuesday\"           \"to\"                \"find\"             \n [610] \"the\"               \"line\"              \"between\"          \n [613] \"her\"               \"past\"              \"remarks\"          \n [616] \"as\"                \"an\"                \"activist\"         \n [619] \"opposing\"          \"the\"               \"fossil\"           \n [622] \"fuel\"              \"industry\"          \"and\"              \n [625] \"her\"               \"prospective\"       \"role\"             \n [628] \"at\"                \"the\"               \"helm\"             \n [631] \"of\"                \"an\"                \"agency\"           \n [634] \"that\"              \"oversees\"          \"drilling\"         \n [637] \"and\"               \"conservation\"      \"on\"               \n [640] \"the\"               \"nation’s\"          \"more\"             \n [643] \"than\"              \"500\"               \"million\"          \n [646] \"acres\"             \"of\"                \"public\"           \n [649] \"land\"              \"a\"                 \"senate\"           \n [652] \"committee\"         \"on\"                \"thursday\"         \n [655] \"approved\"          \"deb\"               \"haaland\"          \n [658] \"to\"                \"be\"                \"the\"              \n [661] \"next\"              \"secretary\"         \"of\"               \n [664] \"the\"               \"interior\"          \"with\"             \n [667] \"the\"               \"support\"           \"of\"               \n [670] \"senator\"           \"lisa\"              \"murkowski\"        \n [673] \"of\"                \"alaska\"            \"a\"                \n [676] \"key\"               \"republican\"        \"from\"             \n [679] \"an\"                \"oil\"               \"producing\"        \n [682] \"state\"             \"virtually\"         \"ensuring\"         \n [685] \"her\"               \"confirmation\"      \"by\"               \n [688] \"the\"               \"senate\"            \"later\"            \n [691] \"this\"              \"month\"             \"senator\"          \n [694] \"joe\"               \"manchin\"           \"iii\"              \n [697] \"the\"               \"west\"              \"virginia\"         \n [700] \"democrat\"          \"who\"               \"heads\"            \n [703] \"the\"               \"senate\"            \"energy\"           \n [706] \"committee\"         \"announced\"         \"wednesday\"        \n [709] \"that\"              \"he\"                \"will\"             \n [712] \"vote\"              \"to\"                \"confirm\"          \n [715] \"representative\"    \"deb\"               \"haaland\"          \n [718] \"of\"                \"new\"               \"mexico\"           \n [721] \"to\"                \"head\"              \"the\"              \n [724] \"interior\"          \"department\"        \"washington\"       \n [727] \"president\"         \"biden\"             \"announced\"        \n [730] \"on\"                \"friday\"            \"that\"             \n [733] \"he\"                \"will\"              \"use\"              \n [736] \"his\"               \"executive\"         \"authority\"        \n [739] \"to\"                \"restore\"           \"sweeping\"         \n [742] \"environmental\"     \"protections\"       \"to\"               \n [745] \"three\"             \"major\"             \"national\"         \n [748] \"monuments\"         \"that\"              \"had\"              \n [751] \"been\"              \"stripped\"          \"away\"             \n [754] \"by\"                \"former\"            \"president\"        \n [757] \"donald\"            \"j\"                 \"trump\"            \n [760] \"washington\"        \"representative\"    \"deb\"              \n [763] \"haaland\"           \"of\"                \"new\"              \n [766] \"mexico\"            \"made\"              \"history\"          \n [769] \"on\"                \"monday\"            \"when\"             \n [772] \"the\"               \"senate\"            \"confirmed\"        \n [775] \"her\"               \"as\"                \"president\"        \n [778] \"biden’s\"           \"secretary\"         \"of\"               \n [781] \"the\"               \"interior\"          \"making\"           \n [784] \"her\"               \"the\"               \"first\"            \n [787] \"native\"            \"american\"          \"to\"               \n [790] \"lead\"              \"a\"                 \"cabinet\"          \n [793] \"agency\"            \"to\"                \"hear\"             \n [796] \"more\"              \"audio\"             \"stories\"          \n [799] \"from\"              \"publications\"      \"like\"             \n [802] \"the\"               \"new\"               \"york\"             \n [805] \"times\"             \"download\"          \"audm\"             \n [808] \"for\"               \"iphone\"            \"or\"               \n [811] \"android\"           \"washington\"        \"when\"             \n [814] \"representative\"    \"deb\"               \"haaland\"          \n [817] \"was\"               \"tapped\"            \"in\"               \n [820] \"december\"          \"to\"                \"be\"               \n [823] \"president\"         \"biden’s\"           \"interior\"         \n [826] \"secretary\"         \"the\"               \"decision\"         \n [829] \"was\"               \"hailed\"            \"as\"               \n [832] \"historic\"          \"she\"               \"was\"              \n [835] \"the\"               \"first\"             \"native\"           \n [838] \"american\"          \"ever\"              \"nominated\"        \n [841] \"to\"                \"serve\"             \"in\"               \n [844] \"cabinet\"           \"in\"                \"this\"             \n [847] \"case\"              \"to\"                \"head\"             \n [850] \"a\"                 \"department\"        \"that\"             \n [853] \"for\"               \"much\"              \"of\"               \n [856] \"the\"               \"nation’s\"          \"history\"          \n [859] \"has\"               \"mistreated\"        \"and\"              \n [862] \"neglected\"         \"indigenous\"        \"americans\"        \n [865] \"washington\"        \"the\"               \"biden\"            \n [868] \"administration\"    \"on\"                \"wednesday\"        \n [871] \"restored\"          \"protections\"       \"for\"              \n [874] \"migratory\"         \"birds\"             \"that\"             \n [877] \"were\"              \"loosened\"          \"under\"            \n [880] \"former\"            \"president\"         \"donald\"           \n [883] \"j\"                 \"trump\"             \"a\"                \n [886] \"move\"              \"celebrated\"        \"by\"               \n [889] \"conservationists\"  \"but\"               \"expected\"         \n [892] \"to\"                \"exacerbate\"        \"tensions\"         \n [895] \"between\"           \"the\"               \"administration\"   \n [898] \"and\"               \"the\"               \"oil\"              \n [901] \"and\"               \"gas\"               \"industry\"         \n [904] \"interior\"          \"secretary\"         \"deb\"              \n [907] \"haaland\"           \"announced\"         \"friday\"           \n [910] \"that\"              \"the\"               \"bureau\"           \n [913] \"of\"                \"land\"              \"management\"       \n [916] \"will\"              \"move\"              \"its\"              \n [919] \"headquarters\"      \"back\"              \"to\"               \n [922] \"washington\"        \"a\"                 \"reversal\"         \n [925] \"of\"                \"a\"                 \"heavily\"          \n [928] \"criticized\"        \"trump\"             \"administration\"   \n [931] \"decision\"          \"to\"                \"relocate\"         \n [934] \"the\"               \"public\"            \"lands\"            \n [937] \"agency\"            \"to\"                \"grand\"            \n [940] \"junction\"          \"colo\"              \"washington\"       \n [943] \"peering\"           \"at\"                \"a\"                \n [946] \"sea\"               \"of\"                \"white\"            \n [949] \"flags\"             \"blanketing\"        \"the\"              \n [952] \"national\"          \"mall\"              \"dr\"               \n [955] \"laura\"             \"a\"                 \"valleni\"          \n [958] \"recalled\"          \"the\"               \"scores\"           \n [961] \"of\"                \"pregnant\"          \"women\"            \n [964] \"who\"               \"had\"               \"contracted\"       \n [967] \"the\"               \"coronavirus\"       \"at\"               \n [970] \"her\"               \"hospital\"          \"in\"               \n [973] \"south\"             \"carolina\"          \"babies\"           \n [976] \"have\"              \"been\"              \"born\"             \n [979] \"prematurely\"       \"mothers\"           \"have\"             \n [982] \"died\"              \"and\"               \"a\"                \n [985] \"surge\"             \"of\"                \"children\"         \n [988] \"has\"               \"overwhelmed\"       \"the\"              \n [991] \"pediatric\"         \"unit\"              \"for\"              \n [994] \"the\"               \"past\"              \"two\"              \n [997] \"months\"            \"she\"               \"said\"             \n[1000] \"phoenix\"           \"in\"                \"politics\"         \n[1003] \"issues\"            \"of\"                \"true\"             \n[1006] \"importance\"        \"aren’t\"            \"always\"           \n[1009] \"the\"               \"ones\"              \"that\"             \n[1012] \"consume\"           \"our\"               \"attention\"        \n[1015] \"my\"                \"candidate\"         \"for\"              \n[1018] \"the\"               \"most\"              \"underrated\"       \n[1021] \"of\"                \"all\"               \"public\"           \n[1024] \"concerns\"          \"is\"                \"the\"              \n[1027] \"treatment\"         \"of\"                \"animals\"          \n[1030] \"an\"                \"issue\"             \"as\"               \n[1033] \"revealing\"         \"as\"                \"any\"              \n[1036] \"about\"             \"our\"               \"character\"        \n[1039] \"and\"               \"sense\"             \"of\"               \n[1042] \"fairness\"          \"if\"                \"you\"              \n[1045] \"want\"              \"hard\"              \"evidence\"         \n[1048] \"to\"                \"track\"             \"the\"              \n[1051] \"moral\"             \"progress\"          \"of\"               \n[1054] \"humanity\"          \"watch\"             \"how\"              \n[1057] \"we\"                \"deal\"              \"with\"             \n[1060] \"other\"             \"creatures\"         \"who\"              \n[1063] \"are\"               \"defenseless\"       \"against\"          \n[1066] \"our\"               \"power\"             \"and\"              \n[1069] \"will\"              \"in\"                \"1990\"             \n[1072] \"when\"              \"congress\"          \"passed\"           \n[1075] \"a\"                 \"law\"               \"that\"             \n[1078] \"set\"               \"criteria\"          \"under\"            \n[1081] \"which\"             \"federally\"         \"recognized\"       \n[1084] \"native\"            \"american\"          \"tribes\"           \n[1087] \"could\"             \"reclaim\"           \"ancient\"          \n[1090] \"burial\"            \"remains\"           \"and\"              \n[1093] \"sacred\"            \"objects\"           \"legislators\"      \n[1096] \"hoped\"             \"to\"                \"encourage\"        \n[1099] \"the\"               \"return\"            \"of\"               \n[1102] \"items\"             \"by\"                \"museums\"          \n[1105] \"and\"               \"other\"             \"institutions\"     \n[1108] \"but\"               \"more\"              \"than\"             \n[1111] \"three\"             \"decades\"           \"later\"            \n[1114] \"some\"              \"officials\"         \"acknowledge\"      \n[1117] \"that\"              \"the\"               \"law\"              \n[1120] \"known\"             \"as\"                \"the\"              \n[1123] \"native\"            \"american\"          \"graves\"           \n[1126] \"protection\"        \"and\"               \"repatriation\"     \n[1129] \"act\"               \"or\"                \"nagpra\"           \n[1132] \"has\"               \"not\"               \"been\"             \n[1135] \"as\"                \"effective\"         \"as\"               \n[1138] \"they\"              \"had\"               \"hoped\"            \n[1141] \"representative\"    \"deb\"               \"haaland\"          \n[1144] \"of\"                \"new\"               \"mexico\"           \n[1147] \"president\"         \"biden’s\"           \"pick\"             \n[1150] \"to\"                \"head\"              \"the\"              \n[1153] \"interior\"          \"department\"        \"was\"              \n[1156] \"questioned\"        \"on\"                \"past\"             \n[1159] \"remarks\"           \"as\"                \"an\"               \n[1162] \"activist\"          \"opposing\"          \"the\"              \n[1165] \"fossil\"            \"fuel\"              \"industry\"         \n[1168] \"the\"               \"united\"            \"states\"           \n[1171] \"will\"              \"search\"            \"federal\"          \n[1174] \"boarding\"          \"schools\"           \"for\"              \n[1177] \"possible\"          \"burial\"            \"sites\"            \n[1180] \"of\"                \"native\"            \"american\"         \n[1183] \"children\"          \"hundreds\"          \"of\"               \n[1186] \"thousands\"         \"of\"                \"whom\"             \n[1189] \"were\"              \"forcibly\"          \"taken\"            \n[1192] \"from\"              \"their\"             \"communities\"      \n[1195] \"to\"                \"be\"                \"culturally\"       \n[1198] \"assimilated\"       \"in\"                \"the\"              \n[1201] \"schools\"           \"for\"               \"more\"             \n[1204] \"than\"              \"a\"                 \"century\"          \n[1207] \"the\"               \"interior\"          \"secretary\"        \n[1210] \"announced\"         \"on\"                \"tuesday\"          \n[1213] \"the\"               \"white\"             \"house\"            \n[1216] \"has\"               \"directed\"          \"the\"              \n[1219] \"interior\"          \"department’s\"      \"chief\"            \n[1222] \"of\"                \"staff\"             \"jennifer\"         \n[1225] \"van\"               \"der\"               \"heide\"            \n[1228] \"to\"                \"step\"              \"away\"             \n[1231] \"from\"              \"her\"               \"position\"         \n[1234] \"after\"             \"she\"               \"tried\"            \n[1237] \"to\"                \"plan\"              \"an\"               \n[1240] \"indoor\"            \"party\"             \"with\"             \n[1243] \"roughly\"           \"50\"                \"attendees\"        \n[1246] \"to\"                \"celebrate\"         \"the\"              \n[1249] \"confirmation\"      \"of\"                \"interior\"         \n[1252] \"secretary\"         \"deb\"               \"haaland\"          \n[1255] \"according\"         \"to\"                \"an\"               \n[1258] \"administration\"    \"official\"          \"with\"             \n[1261] \"knowledge\"         \"of\"                \"the\"              \n[1264] \"matter\"            \"see\"               \"full\"             \n[1267] \"results\"           \"and\"               \"maps\"             \n[1270] \"from\"              \"the\"               \"new\"              \n[1273] \"mexico\"            \"special\"           \"election\"         \n[1276] \"picture\"           \"this\"              \"predicament\"      \n[1279] \"described\"         \"by\"                \"our\"              \n[1282] \"climate\"           \"reporter\"          \"lisa\"             \n[1285] \"friedman\"          \"in\"                \"her\"              \n[1288] \"latest\"            \"article\"           \"as\"               \n[1291] \"a\"                 \"paradox\"           \"worthy\"           \n[1294] \"of\"                \"kafka\"             \"in\"               \n[1297] \"order\"             \"to\"                \"break\"            \n[1300] \"through\"           \"the\"               \"earth\"            \n[1303] \"and\"               \"tap\"               \"the\"              \n[1306] \"oil\"               \"in\"                \"the\"              \n[1309] \"national\"          \"petroleum\"         \"reserve\"          \n[1312] \"in\"                \"alaska\"            \"conocophillips\"   \n[1315] \"must\"              \"install\"           \"chillers\"         \n[1318] \"into\"              \"the\"               \"thawing\"          \n[1321] \"permafrost\"        \"washington\"        \"president\"        \n[1324] \"biden’s\"           \"cabinet\"           \"took\"             \n[1327] \"steps\"             \"toward\"            \"belated\"          \n[1330] \"completion\"        \"on\"                \"tuesday\"          \n[1333] \"with\"              \"the\"               \"confirmation\"     \n[1336] \"of\"                \"a\"                 \"united\"           \n[1339] \"nations\"           \"ambassador\"        \"and\"              \n[1342] \"an\"                \"agriculture\"       \"secretary\"        \n[1345] \"but\"               \"other\"             \"top\"              \n[1348] \"posts\"             \"remained\"          \"locked\"           \n[1351] \"in\"                \"partisan\"          \"confirmation\"     \n[1354] \"hearings\"          \"washington\"        \"a\"                \n[1357] \"federal\"           \"judge\"             \"in\"               \n[1360] \"louisiana\"         \"has\"               \"blocked\"          \n[1363] \"the\"               \"biden\"             \"administration’s\" \n[1366] \"suspension\"        \"of\"                \"new\"              \n[1369] \"oil\"               \"and\"               \"gas\"              \n[1372] \"leases\"            \"on\"                \"federal\"          \n[1375] \"lands\"             \"and\"               \"waters\"           \n[1378] \"in\"                \"the\"               \"first\"            \n[1381] \"major\"             \"legal\"             \"roadblock\"        \n[1384] \"for\"               \"president\"         \"biden’s\"          \n[1387] \"quest\"             \"to\"                \"cut\"              \n[1390] \"fossil\"            \"fuel\"              \"pollution\"        \n[1393] \"and\"               \"conserve\"          \"public\"           \n[1396] \"lands\"             \"melanie\"           \"stansbury\"        \n[1399] \"a\"                 \"democrat\"          \"won\"              \n[1402] \"a\"                 \"landslide\"         \"victory\"          \n[1405] \"in\"                \"a\"                 \"special\"          \n[1408] \"house\"             \"election\"          \"in\"               \n[1411] \"new\"               \"mexico\"            \"on\"               \n[1414] \"tuesday\"           \"claiming\"          \"the\"              \n[1417] \"seat\"              \"previously\"        \"held\"             \n[1420] \"by\"                \"interior\"          \"secretary\"        \n[1423] \"deb\"               \"haaland\"           \"and\"              \n[1426] \"easily\"            \"turning\"           \"back\"             \n[1429] \"a\"                 \"republican\"        \"effort\"           \n[1432] \"to\"                \"make\"              \"the\"              \n[1435] \"race\"              \"a\"                 \"referendum\"       \n[1438] \"on\"                \"rising\"            \"crime\"            \n[1441] \"in\"                \"the\"               \"albuquerque\"      \n[1444] \"based\"             \"district\"          \"washington\"       \n[1447] \"as\"                \"the\"               \"interior\"         \n[1450] \"department\"        \"awaits\"            \"its\"              \n[1453] \"new\"               \"secretary\"         \"the\"              \n[1456] \"agency\"            \"is\"                \"already\"          \n[1459] \"moving\"            \"to\"                \"lock\"             \n[1462] \"in\"                \"key\"               \"parts\"            \n[1465] \"of\"                \"president\"         \"biden’s\"          \n[1468] \"environmental\"     \"agenda\"            \"particularly\"     \n[1471] \"on\"                \"oil\"               \"and\"              \n[1474] \"gas\"               \"restrictions\"      \"laying\"           \n[1477] \"the\"               \"groundwork\"        \"to\"               \n[1480] \"fulfill\"           \"some\"              \"of\"               \n[1483] \"the\"               \"administration’s\"  \"most\"             \n[1486] \"consequential\"     \"climate\"           \"change\"           \n[1489] \"promises\"          \"manchester\"        \"england\"          \n[1492] \"there\"             \"were\"              \"borussia\"         \n[1495] \"dortmund\"          \"players\"           \"sprawled\"         \n[1498] \"on\"                \"the\"               \"turf\"             \n[1501] \"their\"             \"bodies\"            \"suddenly\"         \n[1504] \"exhausted\"         \"and\"               \"their\"            \n[1507] \"spirits\"           \"sunk\"              \"at\"               \n[1510] \"the\"               \"last\"              \"moment\"           \n[1513] \"manchester\"        \"city’s\"            \"substitutes\"      \n[1516] \"and\"               \"coaching\"          \"staff\"            \n[1519] \"had\"               \"greeted\"           \"the\"              \n[1522] \"goal\"              \"that\"              \"inflicted\"        \n[1525] \"the\"               \"damage\"            \"with\"             \n[1528] \"great\"             \"guttural\"          \"roars\"            \n[1531] \"leaping\"           \"from\"              \"their\"            \n[1534] \"seats\"             \"pumping\"           \"their\"            \n[1537] \"fists\"             \"in\"                \"the\"              \n[1540] \"air\"               \"washington\"        \"despite\"          \n[1543] \"president\"         \"biden’s\"           \"pledge\"           \n[1546] \"to\"                \"aggressively\"      \"cut\"              \n[1549] \"the\"               \"pollution\"         \"from\"             \n[1552] \"fossil\"            \"fuels\"             \"that\"             \n[1555] \"is\"                \"driving\"           \"climate\"          \n[1558] \"change\"            \"his\"               \"administration\"   \n[1561] \"has\"               \"quietly\"           \"taken\"            \n[1564] \"actions\"           \"this\"              \"month\"            \n[1567] \"that\"              \"will\"              \"guarantee\"        \n[1570] \"the\"               \"drilling\"          \"and\"              \n[1573] \"burning\"           \"of\"                \"oil\"              \n[1576] \"and\"               \"gas\"               \"for\"              \n[1579] \"decades\"           \"to\"                \"come\"             \n[1582] \"with\"              \"pressure\"          \"growing\"          \n[1585] \"from\"              \"the\"               \"biden\"            \n[1588] \"administration\"    \"two\"               \"native\"           \n[1591] \"american\"          \"tribes\"            \"in\"               \n[1594] \"oklahoma\"          \"have\"              \"agreed\"           \n[1597] \"to\"                \"consider\"          \"reversing\"        \n[1600] \"their\"             \"policies\"          \"of\"               \n[1603] \"denying\"           \"citizenship\"       \"to\"               \n[1606] \"descendants\"       \"of\"                \"black\"            \n[1609] \"people\"            \"who\"               \"were\"             \n[1612] \"enslaved\"          \"by\"                \"them\"             \n[1615] \"before\"            \"the\"               \"civil\"            \n[1618] \"war\"               \"as\"                \"he\"               \n[1621] \"guides\"            \"the\"               \"financial\"        \n[1624] \"system\"            \"through\"           \"the\"              \n[1627] \"coronavirus\"       \"pandemic\"          \"jerome\"           \n[1630] \"powell\"            \"the\"               \"chair\"            \n[1633] \"of\"                \"the\"               \"federal\"          \n[1636] \"reserve\"           \"has\"               \"worked\"           \n[1639] \"to\"                \"keep\"              \"cash\"             \n[1642] \"flowing\"           \"through\"           \"the\"              \n[1645] \"economy\"           \"he’s\"              \"done\"             \n[1648] \"it\"                \"by\"                \"tamping\"          \n[1651] \"down\"              \"interest\"          \"rates\"            \n[1654] \"and\"               \"sometimes\"         \"speaking\"         \n[1657] \"up\"                \"to\"                \"a\"                \n[1660] \"degree\"            \"that’s\"            \"rare\"             \n[1663] \"for\"               \"a\"                 \"fed\"              \n[1666] \"director\"          \"to\"                \"urge\"             \n[1669] \"legislative\"       \"action\"            \"on\"               \n[1672] \"behalf\"            \"of\"                \"workers\"          \n[1675] \"and\"               \"businesses\"        \"park\"             \n[1678] \"rapids\"            \"minn\"              \"the\"              \n[1681] \"protesters\"        \"who\"               \"gathered\"         \n[1684] \"in\"                \"the\"               \"boreal\"           \n[1687] \"forests\"           \"of\"                \"northern\"         \n[1690] \"minnesota\"         \"came\"              \"from\"             \n[1693] \"across\"            \"the\"               \"country\"          \n[1696] \"native\"            \"american\"          \"tribes\"           \n[1699] \"and\"               \"their\"             \"supporters\"       \n[1702] \"environmentalists\" \"and\"               \"religious\"        \n[1705] \"leaders\"           \"all\"               \"to\"               \n[1708] \"fight\"             \"an\"                \"expansion\"        \n[1711] \"of\"                \"line\"              \"3\"                \n[1714] \"a\"                 \"9\"                 \"billion\"          \n[1717] \"pipeline\"          \"operated\"          \"by\"               \n[1720] \"the\"               \"canadian\"          \"company\"          \n[1723] \"enbridge\"          \"that\"              \"would\"            \n[1726] \"carry\"             \"hundreds\"          \"of\"               \n[1729] \"thousands\"         \"of\"                \"barrels\"          \n[1732] \"of\"                \"oil\"               \"through\"          \n[1735] \"minnesota’s\"       \"delicate\"          \"watersheds\"       \n[1738] \"and\"               \"tribal\"            \"lands\"            \n[1741] \"to\"                \"hear\"              \"more\"             \n[1744] \"audio\"             \"stories\"           \"from\"             \n[1747] \"publications\"      \"like\"              \"the\"              \n[1750] \"new\"               \"york\"              \"times\"            \n[1753] \"download\"          \"audm\"              \"for\"              \n[1756] \"iphone\"            \"or\"                \"android\"          \n[1759] \"as\"                \"the\"               \"interior\"         \n[1762] \"department\"        \"awaits\"            \"its\"              \n[1765] \"new\"               \"secretary\"         \"the\"              \n[1768] \"agency\"            \"is\"                \"already\"          \n[1771] \"moving\"            \"to\"                \"lock\"             \n[1774] \"in\"                \"key\"               \"parts\"            \n[1777] \"of\"                \"president\"         \"biden’s\"          \n[1780] \"environmental\"     \"agenda\"            \"particularly\"     \n[1783] \"on\"                \"oil\"               \"and\"              \n[1786] \"gas\"               \"restrictions\"      \"laying\"           \n[1789] \"the\"               \"groundwork\"        \"to\"               \n[1792] \"fulfill\"           \"some\"              \"of\"               \n[1795] \"the\"               \"administration’s\"  \"most\"             \n[1798] \"consequential\"     \"promises\"          \"to\"               \n[1801] \"address\"           \"climate\"           \"change\"           \n[1804] \"when\"              \"extreme\"           \"weather\"          \n[1807] \"knocked\"           \"out\"               \"power\"            \n[1810] \"and\"               \"water\"             \"in\"               \n[1813] \"texas\"             \"last\"              \"week\"             \n[1816] \"it\"                \"represented\"       \"a\"                \n[1819] \"profound\"          \"warning\"           \"for\"              \n[1822] \"the\"               \"rest\"              \"of\"               \n[1825] \"the\"               \"country\"           \"the\"              \n[1828] \"nation’s\"          \"vital\"             \"infrastructure\"   \n[1831] \"remains\"           \"fundamentally\"     \"unprepared\"       \n[1834] \"for\"               \"the\"               \"shocks\"           \n[1837] \"of\"                \"climate\"           \"change\"           \n[1840] \"over\"              \"the\"               \"pandemic\"         \n[1843] \"year\"              \"people\"            \"took\"             \n[1846] \"to\"                \"appreciating\"      \"the\"              \n[1849] \"renewable\"         \"delights\"          \"of\"               \n[1852] \"this\"              \"ragged\"            \"and\"              \n[1855] \"tortured\"          \"planet\"            \"like\"             \n[1858] \"never\"             \"before\"            \"in\"               \n[1861] \"this\"              \"country\"           \"many\"             \n[1864] \"national\"          \"parks\"             \"saw\"              \n[1867] \"record\"            \"crowds\"            \"in\"               \n[1870] \"2020\"              \"and\"               \"eight\"            \n[1873] \"million\"           \"more\"              \"americans\"        \n[1876] \"went\"              \"hiking\"            \"than\"             \n[1879] \"in\"                \"the\"               \"year\"             \n[1882] \"before\"            \"washington\"        \"when\"             \n[1885] \"president\"         \"biden\"             \"introduced\"       \n[1888] \"representative\"    \"deb\"               \"haaland\"          \n[1891] \"of\"                \"new\"               \"mexico\"           \n[1894] \"as\"                \"his\"               \"pick\"             \n[1897] \"for\"               \"interior\"          \"secretary\"        \n[1900] \"making\"            \"her\"               \"the\"              \n[1903] \"first\"             \"native\"            \"american\"         \n[1906] \"to\"                \"be\"                \"selected\"         \n[1909] \"for\"               \"a\"                 \"cabinet\"          \n[1912] \"position\"          \"he\"                \"acknowledged\"     \n[1915] \"the\"               \"country’s\"         \"long\"             \n[1918] \"history\"           \"of\"                \"failing\"          \n[1921] \"the\"               \"land’s\"            \"first\"            \n[1924] \"citizens\"          \"we\"                \"are\"              \n[1927] \"among\"             \"the\"               \"women\"            \n[1930] \"of\"                \"bears\"             \"ears\"             \n[1933] \"indigenous\"        \"women\"             \"who\"              \n[1936] \"support\"           \"our\"               \"families\"         \n[1939] \"and\"               \"communities\"       \"in\"               \n[1942] \"the\"               \"protections\"       \"of\"               \n[1945] \"ancestral\"         \"lands\"             \"we\"               \n[1948] \"come\"              \"from\"              \"diné\"             \n[1951] \"nuche\"             \"pueblo\"            \"and\"              \n[1954] \"other\"             \"allied\"            \"native\"           \n[1957] \"nations\"           \"from\"              \"these\"            \n[1960] \"southwestern\"      \"lands\"             \"twin\"             \n[1963] \"buttes\"            \"rise\"              \"they\"             \n[1966] \"are\"               \"known\"             \"as\"               \n[1969] \"bears\"             \"ears\"              \"after\"            \n[1972] \"a\"                 \"public\"            \"outcry\"           \n[1975] \"and\"               \"a\"                 \"legal\"            \n[1978] \"fight\"             \"the\"               \"indian\"           \n[1981] \"health\"            \"service\"           \"has\"              \n[1984] \"agreed\"            \"to\"                \"keep\"             \n[1987] \"the\"               \"emergency\"         \"departments\"      \n[1990] \"and\"               \"inpatient\"         \"care\"             \n[1993] \"open\"              \"for\"               \"another\"          \n[1996] \"year\"              \"at\"                \"a\"                \n[1999] \"hospital\"          \"in\"                \"northwestern\"     \n[2002] \"new\"               \"mexico\"            \"serving\"          \n[2005] \"9,100\"             \"tribal\"            \"citizens\"         \n[2008] \"settling\"          \"at\"                \"least\"            \n[2011] \"temporarily\"       \"a\"                 \"dispute\"          \n[2014] \"that\"              \"had\"               \"become\"           \n[2017] \"a\"                 \"flash\"             \"point\"            \n[2020] \"between\"           \"tribal\"            \"communities\"      \n[2023] \"and\"               \"the\"               \"federal\"          \n[2026] \"government\"        \"over\"              \"the\"              \n[2029] \"quality\"           \"and\"               \"accessibility\"    \n[2032] \"of\"                \"health\"            \"care\"             \n[2035] \"albuquerque\"       \"in\"                \"theory\"           \n[2038] \"the\"               \"special\"           \"election\"         \n[2041] \"to\"                \"fill\"              \"interior\"         \n[2044] \"secretary\"         \"deb\"               \"haaland’s\"        \n[2047] \"seat\"              \"in\"                \"the\"              \n[2050] \"house\"             \"should\"            \"not\"              \n[2053] \"be\"                \"competitive\"       \"president\"        \n[2056] \"biden\"             \"carried\"           \"the\"              \n[2059] \"albuquerque\"       \"based\"             \"district\"         \n[2062] \"by\"                \"23\"                \"points\"           \n[2065] \"last\"              \"year\"              \"and\"              \n[2068] \"there\"             \"has\"               \"not\"              \n[2071] \"been\"              \"a\"                 \"close\"            \n[2074] \"race\"              \"for\"               \"congress\"         \n[2077] \"here\"              \"since\"             \"george\"           \n[2080] \"w\"                 \"bush\"              \"was\"              \n[2083] \"president\"         \"a\"                 \"couple\"           \n[2086] \"of\"                \"weeks\"             \"ago\"              \n[2089] \"a\"                 \"tweet\"             \"caught\"           \n[2092] \"my\"                \"eye\"               \"it\"               \n[2095] \"seemed\"            \"unexpectedly\"      \"to\"               \n[2098] \"reveal\"            \"that\"              \"a\"                \n[2101] \"continentwide\"     \"african\"           \"super\"            \n[2104] \"league\"            \"was\"               \"under\"            \n[2107] \"construction\"      \"cross\"             \"border\"           \n[2110] \"leagues\"           \"as\"                \"regular\"          \n[2113] \"readers\"           \"will\"              \"know\"             \n[2116] \"are\"               \"something\"         \"that\"             \n[2119] \"this\"              \"newsletter\"        \"generally\"        \n[2122] \"supports\"          \"they\"              \"are\"              \n[2125] \"the\"               \"most\"              \"readily\"          \n[2128] \"available\"         \"way\"               \"of\"               \n[2131] \"addressing\"        \"soccer’s\"          \"chronic\"          \n[2134] \"financial\"         \"imbalance\"         \"washington\"       \n[2137] \"the\"               \"biden\"             \"administration\"   \n[2140] \"on\"                \"tuesday\"           \"suspended\"        \n[2143] \"oil\"               \"drilling\"          \"leases\"           \n[2146] \"in\"                \"the\"               \"arctic\"           \n[2149] \"national\"          \"wildlife\"          \"refuge\"           \n[2152] \"unspooling\"        \"a\"                 \"signature\"        \n[2155] \"achievement\"       \"of\"                \"the\"              \n[2158] \"trump\"             \"presidency\"        \"and\"              \n[2161] \"delivering\"        \"on\"                \"a\"                \n[2164] \"promise\"           \"by\"                \"president\"        \n[2167] \"biden\"             \"to\"                \"protect\"          \n[2170] \"the\"               \"fragile\"           \"alaskan\"          \n[2173] \"tundra\"            \"from\"              \"fossil\"           \n[2176] \"fuel\"              \"extraction\"        \"when\"             \n[2179] \"president\"         \"biden\"             \"picked\"           \n[2182] \"neera\"             \"tanden\"            \"to\"               \n[2185] \"be\"                \"budget\"            \"director\"         \n[2188] \"at\"                \"the\"               \"prodding\"         \n[2191] \"of\"                \"his\"               \"top\"              \n[2194] \"adviser\"           \"ron\"               \"klain\"            \n[2197] \"the\"               \"white\"             \"house\"            \n[2200] \"girded\"            \"for\"               \"that\"             \n[2203] \"one\"               \"big\"               \"battle\"           \n[2206] \"comforted\"         \"by\"                \"the\"              \n[2209] \"likelihood\"        \"that\"              \"many\"             \n[2212] \"other\"             \"cabinet\"           \"confirmations\"    \n[2215] \"would\"             \"be\"                \"comparative\"      \n[2218] \"pillow\"            \"fights\"            \"did\"              \n[2221] \"you\"               \"follow\"            \"the\"              \n[2224] \"news\"              \"this\"              \"week\"             \n[2227] \"take\"              \"our\"               \"quiz\"             \n[2230] \"to\"                \"see\"               \"how\"              \n[2233] \"well\"              \"you\"               \"stack\"            \n[2236] \"up\"                \"with\"              \"other\"            \n[2239] \"times\"             \"readers\"           \"the\"              \n[2242] \"bald\"              \"eagle\"             \"population\"       \n[2245] \"in\"                \"the\"               \"lower\"            \n[2248] \"48\"                \"states\"            \"has\"              \n[2251] \"quadrupled\"        \"since\"             \"2009\"             \n[2254] \"researchers\"       \"said\"              \"this\"             \n[2257] \"week\"              \"underscoring\"      \"decades\"          \n[2260] \"of\"                \"efforts\"           \"to\"               \n[2263] \"protect\"           \"a\"                 \"species\"          \n[2266] \"that\"              \"was\"               \"once\"             \n[2269] \"on\"                \"the\"               \"brink\"            \n[2272] \"of\"                \"extinction\"        \"muriel\"           \n[2275] \"lezak\"             \"a\"                 \"neuropsychologist\"\n[2278] \"who\"               \"wrote\"             \"a\"                \n[2281] \"landmark\"          \"textbook\"          \"in\"               \n[2284] \"the\"               \"early\"             \"days\"             \n[2287] \"of\"                \"her\"               \"discipline\"       \n[2290] \"that\"              \"became\"            \"an\"               \n[2293] \"essential\"         \"guide\"             \"to\"               \n[2296] \"the\"               \"description\"       \"and\"              \n[2299] \"evaluation\"        \"of\"                \"brain\"            \n[2302] \"injuries\"          \"and\"               \"disorders\"        \n[2305] \"died\"              \"on\"                \"oct\"              \n[2308] \"6\"                 \"in\"                \"portland\"         \n[2311] \"ore\"               \"she\"               \"was\"              \n[2314] \"94\"                \"the\"               \"biden\"            \n[2317] \"administration\"    \"on\"                \"tuesday\"          \n[2320] \"announced\"         \"its\"               \"final\"            \n[2323] \"approval\"          \"of\"                \"the\"              \n[2326] \"nation’s\"          \"first\"             \"commercial\"       \n[2329] \"scale\"             \"offshore\"          \"wind\"             \n[2332] \"farm\"              \"a\"                 \"major\"            \n[2335] \"step\"              \"toward\"            \"president\"        \n[2338] \"biden’s\"           \"goal\"              \"of\"               \n[2341] \"expanding\"         \"renewable\"         \"energy\"           \n[2344] \"production\"        \"across\"            \"the\"              \n[2347] \"united\"            \"states\"            \"at\"               \n[2350] \"a\"                 \"memorial\"          \"for\"              \n[2353] \"the\"               \"passengers\"        \"who\"              \n[2356] \"fought\"            \"back\"              \"against\"          \n[2359] \"terrorists\"        \"aboard\"            \"united\"           \n[2362] \"airlines\"          \"flight\"            \"93\"               \n[2365] \"vice\"              \"president\"         \"kamala\"           \n[2368] \"harris\"            \"warned\"            \"that\"             \n[2371] \"the\"               \"tragedy\"           \"of\"               \n[2374] \"the\"               \"sept\"              \"11\"               \n[2377] \"attacks\"           \"had\"               \"shown\"            \n[2380] \"how\"               \"fear\"              \"can\"              \n[2383] \"be\"                \"used\"              \"to\"               \n[2386] \"sow\"               \"division\"          \"and\"              \n[2389] \"stressed\"          \"that\"              \"america’s\"        \n[2392] \"diversity\"         \"was\"               \"its\"              \n[2395] \"greatest\"          \"asset\"             \"it\"               \n[2398] \"does\"              \"not\"               \"require\"          \n[2401] \"a\"                 \"great\"             \"leap\"             \n[2404] \"of\"                \"the\"               \"imagination\"      \n[2407] \"to\"                \"envision\"          \"the\"              \n[2410] \"final\"             \"few\"               \"weeks\"            \n[2413] \"of\"                \"the\"               \"season\"           \n[2416] \"playing\"           \"out\"               \"like\"             \n[2419] \"this\"              \"after\"             \"a\"                \n[2422] \"year\"              \"that\"              \"provided\"         \n[2425] \"stark\"             \"new\"               \"evidence\"         \n[2428] \"of\"                \"how\"               \"racial\"           \n[2431] \"inequities\"        \"and\"               \"a\"                \n[2434] \"lack\"              \"of\"                \"federal\"          \n[2437] \"funding\"           \"had\"               \"left\"             \n[2440] \"tribal\"            \"communities\"       \"and\"              \n[2443] \"indigenous\"        \"people\"            \"especially\"       \n[2446] \"vulnerable\"        \"to\"                \"crises\"           \n[2449] \"like\"              \"the\"               \"pandemic\"         \n[2452] \"president\"         \"biden\"             \"and\"              \n[2455] \"democrats\"         \"in\"                \"congress\"         \n[2458] \"are\"               \"seeking\"           \"to\"               \n[2461] \"address\"           \"those\"             \"longstanding\"     \n[2464] \"issues\"            \"with\"              \"a\"                \n[2467] \"huge\"              \"infusion\"          \"of\"               \n[2470] \"federal\"           \"aid\"               \"patrisse\"         \n[2473] \"cullors\"           \"a\"                 \"founder\"          \n[2476] \"of\"                \"the\"               \"black\"            \n[2479] \"lives\"             \"matter\"            \"movement\"         \n[2482] \"on\"                \"women’s\"           \"history\"          \n[2485] \"month\"             \"with\"              \"their\"            \n[2488] \"signature\"         \"1.9\"               \"trillion\"         \n[2491] \"coronavirus\"       \"rescue\"            \"plan\"             \n[2494] \"enshrined\"         \"in\"                \"law\"              \n[2497] \"president\"         \"biden\"             \"and\"              \n[2500] \"congressional\"     \"democrats\"         \"will\"             \n[2503] \"shift\"             \"gears\"             \"this\"             \n[2506] \"week\"              \"to\"                \"try\"              \n[2509] \"to\"                \"sell\"              \"the\"              \n[2512] \"public\"            \"on\"                \"what\"             \n[2515] \"they\"              \"have\"              \"done\"             \n[2518] \"and\"               \"begin\"             \"tackling\"         \n[2521] \"other\"             \"thornier\"          \"pieces\"           \n[2524] \"of\"                \"their\"             \"policy\"           \n[2527] \"agenda\"            \"former\"            \"president\"        \n[2530] \"donald\"            \"j\"                 \"trump\"            \n[2533] \"endorsed\"          \"kelly\"             \"tshibaka\"         \n[2536] \"on\"                \"friday\"            \"in\"               \n[2539] \"her\"               \"race\"              \"against\"          \n[2542] \"senator\"           \"lisa\"              \"murkowski\"        \n[2545] \"of\"                \"alaska\"            \"giving\"           \n[2548] \"his\"               \"support\"           \"to\"               \n[2551] \"an\"                \"outsider\"          \"candidate\"        \n[2554] \"who\"               \"promoted\"          \"false\"            \n[2557] \"claims\"            \"of\"                \"election\"         \n[2560] \"fraud\"             \"last\"              \"year\"             \n[2563] \"and\"               \"has\"               \"written\"          \n[2566] \"articles\"          \"in\"                \"support\"          \n[2569] \"of\"                \"gay\"               \"conversion\"       \n[2572] \"therapy\"           \"the\"               \"american\"         \n[2575] \"west\"              \"embraces\"          \"more\"             \n[2578] \"than\"              \"its\"               \"share\"            \n[2581] \"of\"                \"spectacular\"       \"landscapes\"       \n[2584] \"but\"               \"there’s\"           \"nothing\"          \n[2587] \"else\"              \"quite\"             \"like\"             \n[2590] \"the\"               \"vast\"              \"swath\"            \n[2593] \"of\"                \"canyons\"           \"mesas\"            \n[2596] \"sandstone\"         \"spires\"            \"and\"              \n[2599] \"arches\"            \"that\"              \"stretches\"        \n[2602] \"some\"              \"80\"                \"miles\"            \n[2605] \"from\"              \"north\"             \"to\"               \n[2608] \"south\"             \"in\"                \"the\"              \n[2611] \"southeast\"         \"corner\"            \"of\"               \n[2614] \"utah\"              \"ranging\"           \"in\"               \n[2617] \"altitude\"          \"from\"              \"sagebrush\"        \n[2620] \"flats\"             \"to\"                \"pinyon\"           \n[2623] \"and\"               \"juniper\"           \"forests\"          \n[2626] \"and\"               \"old\"               \"growth\"           \n[2629] \"stands\"            \"of\"                \"ponderosa\"        \n[2632] \"pine\"              \"and\"               \"douglas\"          \n[2635] \"fir\"               \"livingston\"        \"mont\"             \n[2638] \"a\"                 \"slaughter\"         \"of\"               \n[2641] \"wolves\"            \"is\"                \"underway\"         \n[2644] \"in\"                \"montana\"           \"idaho\"            \n[2647] \"and\"               \"wyoming\"           \"that\"             \n[2650] \"has\"               \"no\"                \"precedent\"        \n[2653] \"in\"                \"the\"               \"past\"             \n[2656] \"100\"               \"years\"             \"washington\"       \n[2659] \"centuries\"         \"of\"                \"land\"             \n[2662] \"loss\"              \"and\"               \"forced\"           \n[2665] \"relocation\"        \"have\"              \"left\"             \n[2668] \"native\"            \"americans\"         \"significantly\"    \n[2671] \"more\"              \"exposed\"           \"to\"               \n[2674] \"the\"               \"effects\"           \"of\"               \n[2677] \"climate\"           \"change\"            \"new\"              \n[2680] \"data\"              \"show\"              \"adding\"           \n[2683] \"to\"                \"the\"               \"debate\"           \n[2686] \"over\"              \"how\"               \"to\"               \n[2689] \"address\"           \"climate\"           \"change\"           \n[2692] \"and\"               \"racial\"            \"inequity\"         \n[2695] \"in\"                \"the\"               \"united\"           \n[2698] \"states\"            \"the\"               \"biden\"            \n[2701] \"administration\"    \"plans\"             \"to\"               \n[2704] \"restore\"           \"environmental\"     \"protections\"      \n[2707] \"to\"                \"tongass\"           \"national\"         \n[2710] \"forest\"            \"in\"                \"alaska\"           \n[2713] \"one\"               \"of\"                \"the\"              \n[2716] \"world’s\"           \"largest\"           \"intact\"           \n[2719] \"temperate\"         \"rain\"              \"forests\"          \n[2722] \"that\"              \"had\"               \"been\"             \n[2725] \"stripped\"          \"away\"              \"by\"               \n[2728] \"former\"            \"president\"         \"donald\"           \n[2731] \"j\"                 \"trump\"             \"the\"              \n[2734] \"biden\"             \"administration\"    \"said\"             \n[2737] \"wednesday\"         \"that\"              \"it\"               \n[2740] \"had\"               \"canceled\"          \"two\"              \n[2743] \"mining\"            \"leases\"            \"that\"             \n[2746] \"would\"             \"have\"              \"allowed\"          \n[2749] \"a\"                 \"copper\"            \"mine\"             \n[2752] \"to\"                \"be\"                \"built\"            \n[2755] \"near\"              \"an\"                \"area\"             \n[2758] \"of\"                \"pristine\"          \"wilderness\"       \n[2761] \"in\"                \"minnesota\"         \"it\"               \n[2764] \"was\"               \"a\"                 \"busy\"             \n[2767] \"day\"               \"on\"                \"capitol\"          \n[2770] \"hill\"              \"washington\"        \"the\"              \n[2773] \"supreme\"           \"court\"             \"agreed\"           \n[2776] \"on\"                \"monday\"            \"to\"               \n[2779] \"hear\"              \"a\"                 \"challenge\"        \n[2782] \"to\"                \"the\"               \"constitutionality\"\n[2785] \"of\"                \"the\"               \"indian\"           \n[2788] \"child\"             \"welfare\"           \"act\"              \n[2791] \"of\"                \"1978\"              \"which\"            \n[2794] \"makes\"             \"it\"                \"hard\"             \n[2797] \"to\"                \"remove\"            \"native\"           \n[2800] \"american\"          \"children\"          \"from\"             \n[2803] \"their\"             \"parents\"           \"their\"            \n[2806] \"tribes\"            \"and\"               \"their\"            \n[2809] \"heritage\"          \"washington\"        \"a\"                \n[2812] \"federal\"           \"judge\"             \"in\"               \n[2815] \"alaska\"            \"on\"                \"wednesday\"        \n[2818] \"blocked\"           \"construction\"      \"permits\"          \n[2821] \"for\"               \"an\"                \"expansive\"        \n[2824] \"oil\"               \"drilling\"          \"project\"          \n[2827] \"on\"                \"the\"               \"state’s\"          \n[2830] \"north\"             \"slope\"             \"that\"             \n[2833] \"was\"               \"designed\"          \"to\"               \n[2836] \"produce\"           \"more\"              \"than\"             \n[2839] \"100,000\"           \"barrels\"           \"of\"               \n[2842] \"oil\"               \"a\"                 \"day\"              \n[2845] \"for\"               \"the\"               \"next\"             \n[2848] \"30\"                \"years\"             \"native\"           \n[2851] \"americans\"         \"have\"              \"been\"             \n[2854] \"systematically\"    \"dispossessed\"      \"of\"               \n[2857] \"their\"             \"ancestral\"         \"lands\"            \n[2860] \"for\"               \"more\"              \"than\"             \n[2863] \"a\"                 \"century\"           \"thanks\"           \n[2866] \"to\"                \"federal\"           \"land\"             \n[2869] \"management\"        \"policies\"          \"but\"              \n[2872] \"a\"                 \"spate\"             \"of\"               \n[2875] \"new\"               \"real\"              \"estate\"           \n[2878] \"projects\"          \"highlights\"        \"efforts\"          \n[2881] \"to\"                \"reclaim\"           \"that\"             \n[2884] \"territory\"         \"as\"                \"tribes\"           \n[2887] \"invest\"            \"in\"                \"land\"             \n[2890] \"development\"       \"in\"                \"an\"               \n[2893] \"effort\"            \"to\"                \"diversify\"        \n[2896] \"their\"             \"revenue\"           \"base\"             \n[2899] \"and\"               \"support\"           \"their\"            \n[2902] \"members\"           \"washington\"        \"democrats\"        \n[2905] \"are\"               \"preparing\"         \"to\"               \n[2908] \"muscle\"            \"through\"           \"the\"              \n[2911] \"nomination\"        \"of\"                \"tracy\"            \n[2914] \"stone\"             \"manning\"           \"to\"               \n[2917] \"head\"              \"the\"               \"bureau\"           \n[2920] \"of\"                \"land\"              \"management\"       \n[2923] \"despite\"           \"united\"            \"opposition\"       \n[2926] \"from\"              \"republicans\"       \"who\"              \n[2929] \"have\"              \"branded\"           \"her\"              \n[2932] \"an\"                \"eco\"               \"terrorist\"        \n[2935] \"because\"           \"of\"                \"her\"              \n[2938] \"involvement\"       \"in\"                \"a\"                \n[2941] \"tree\"              \"spiking\"           \"episode\"          \n[2944] \"as\"                \"a\"                 \"graduate\"         \n[2947] \"student\"           \"in\"                \"the\"              \n[2950] \"1980s\"             \"it\"                \"is\"               \n[2953] \"hard\"              \"to\"                \"overstate\"        \n[2956] \"the\"               \"joy\"               \"of\"               \n[2959] \"the\"               \"environmental\"     \"community\"        \n[2962] \"when\"              \"joe\"               \"biden\"            \n[2965] \"ascended\"          \"to\"                \"the\"              \n[2968] \"white\"             \"house\"             \"in\"               \n[2971] \"place\"             \"of\"                \"a\"                \n[2974] \"man\"               \"who\"               \"called\"           \n[2977] \"climate\"           \"change\"            \"a\"                \n[2980] \"hoax\"              \"it\"                \"got\"              \n[2983] \"someone\"           \"who\"               \"saw\"              \n[2986] \"global\"            \"warming\"           \"for\"              \n[2989] \"the\"               \"grave\"             \"threat\"           \n[2992] \"it\"                \"is\"                \"and\"              \n[2995] \"who\"               \"spoke\"             \"at\"               \n[2998] \"his\"               \"inaugural\"         \"of\"               \n[3001] \"the\"               \"world’s\"           \"duty\"             \n[3004] \"to\"                \"respond\"           \"to\"               \n[3007] \"a\"                 \"cry\"               \"for\"              \n[3010] \"survival\"          \"that\"              \"comes\"            \n[3013] \"from\"              \"the\"               \"planet\"           \n[3016] \"itself\"            \"it\"                \"got\"              \n[3019] \"someone\"           \"who\"               \"saw\"              \n[3022] \"government\"        \"regulations\"       \"not\"              \n[3025] \"as\"                \"job\"               \"killers\"          \n[3028] \"but\"               \"as\"                \"appropriate\"      \n[3031] \"levers\"            \"to\"                \"achieve\"          \n[3034] \"cleaner\"           \"air\"               \"and\"              \n[3037] \"water\"             \"it\"                \"got\"              \n[3040] \"someone\"           \"who\"               \"viewed\"           \n[3043] \"the\"               \"public\"            \"lands\"            \n[3046] \"not\"               \"as\"                \"a\"                \n[3049] \"resource\"          \"to\"                \"be\"               \n[3052] \"exploited\"         \"by\"                \"commercial\"       \n[3055] \"interests\"         \"but\"               \"as\"               \n[3058] \"nature’s\"          \"gift\"              \"to\"               \n[3061] \"future\"            \"generations\"       \"a\"                \n[3064] \"worthy\"            \"custodian\"         \"in\"               \n[3067] \"short\"             \"to\"                \"the\"              \n[3070] \"environmental\"     \"ethic\"             \"of\"               \n[3073] \"teddy\"             \"roosevelt\"         \"jimmy\"            \n[3076] \"carter\"            \"and\"               \"bill\"             \n[3079] \"clinton\"           \"and\"               \"someone\"          \n[3082] \"who\"               \"would\"             \"spend\"            \n[3085] \"trillions\"         \"to\"                \"make\"             \n[3088] \"it\"                \"all\"               \"happen\"           \n[3091] \"have\"              \"you\"               \"been\"             \n[3094] \"paying\"            \"attention\"         \"to\"               \n[3097] \"the\"               \"news\"              \"recently\"         \n[3100] \"see\"               \"how\"               \"many\"             \n[3103] \"of\"                \"these\"             \"10\"               \n[3106] \"questions\"         \"you\"               \"can\"              \n[3109] \"get\"               \"right\"             \"washington\"       \n[3112] \"senator\"           \"lisa\"              \"murkowski\"        \n[3115] \"republican\"        \"of\"                \"alaska\"           \n[3118] \"announced\"         \"on\"                \"friday\"           \n[3121] \"that\"              \"she\"               \"would\"            \n[3124] \"seek\"              \"re\"                \"election\"         \n[3127] \"formally\"          \"entering\"          \"what\"             \n[3130] \"is\"                \"expected\"          \"to\"               \n[3133] \"be\"                \"the\"               \"most\"             \n[3136] \"expensive\"         \"and\"               \"challenging\"      \n[3139] \"race\"              \"of\"                \"her\"              \n[3142] \"political\"         \"career\"            \"after\"            \n[3145] \"voting\"            \"to\"                \"impeach\"          \n[3148] \"former\"            \"president\"         \"donald\"           \n[3151] \"j\"                 \"trump\"             \"on\"               \n[3154] \"wednesday\"         \"night\"             \"ron\"              \n[3157] \"klain\"             \"the\"               \"white\"            \n[3160] \"house\"             \"chief\"             \"of\"               \n[3163] \"staff\"             \"went\"              \"on\"               \n[3166] \"television\"        \"to\"                \"proclaim\"         \n[3169] \"that\"              \"the\"               \"biden\"            \n[3172] \"administration\"    \"was\"               \"still\"            \n[3175] \"fighting\"          \"our\"               \"guts\"             \n[3178] \"out\"               \"to\"                \"get\"              \n[3181] \"neera\"             \"tanden\"            \"confirmed\"        \n[3184] \"as\"                \"the\"               \"head\"             \n[3187] \"of\"                \"the\"               \"office\"           \n[3190] \"of\"                \"management\"        \"and\"              \n[3193] \"budget\"            \"not\"               \"too\"              \n[3196] \"long\"              \"ago\"               \"maren\"            \n[3199] \"lundby\"            \"seemed\"            \"a\"                \n[3202] \"lock\"              \"for\"               \"a\"                \n[3205] \"gold\"              \"medal\"             \"at\"               \n[3208] \"the\"               \"2022\"              \"winter\"           \n[3211] \"olympics\"          \"lundby\"            \"27\"               \n[3214] \"a\"                 \"ski\"               \"jumper\"           \n[3217] \"from\"              \"norway\"            \"had\"              \n[3220] \"won\"               \"gold\"              \"at\"               \n[3223] \"the\"               \"2018\"              \"games\"            \n[3226] \"in\"                \"pyeongchang\"       \"south\"            \n[3229] \"korea\"             \"three\"             \"straight\"         \n[3232] \"world\"             \"cup\"               \"titles\"           \n[3235] \"from\"              \"2018\"              \"to\"               \n[3238] \"2020\"              \"and\"               \"gold\"             \n[3241] \"medals\"            \"in\"                \"the\"              \n[3244] \"2019\"              \"and\"               \"2021\"             \n[3247] \"world\"             \"championships\"     \"washington\"       \n[3250] \"the\"               \"senate\"            \"narrowly\"         \n[3253] \"approved\"          \"tracy\"             \"stone\"            \n[3256] \"manning\"           \"on\"                \"thursday\"         \n[3259] \"to\"                \"lead\"              \"the\"              \n[3262] \"bureau\"            \"of\"                \"land\"             \n[3265] \"management\"        \"capping\"           \"months\"           \n[3268] \"of\"                \"efforts\"           \"by\"               \n[3271] \"republican\"        \"lawmakers\"         \"to\"               \n[3274] \"block\"             \"her\"               \"confirmation\"     \n[3277] \"because\"           \"of\"                \"her\"              \n[3280] \"connection\"        \"to\"                \"a\"                \n[3283] \"decades\"           \"old\"               \"tree\"             \n[3286] \"spiking\"           \"incident\"          \"gray\"             \n[3289] \"wolves\"            \"will\"              \"regain\"           \n[3292] \"federal\"           \"protection\"        \"across\"           \n[3295] \"most\"              \"of\"                \"the\"              \n[3298] \"lower\"             \"48\"                \"united\"           \n[3301] \"states\"            \"following\"         \"a\"                \n[3304] \"court\"             \"ruling\"            \"thursday\"         \n[3307] \"that\"              \"struck\"            \"down\"             \n[3310] \"a\"                 \"trump\"             \"administration\"   \n[3313] \"decision\"          \"to\"                \"take\"             \n[3316] \"the\"               \"animals\"           \"off\"              \n[3319] \"the\"               \"endangered\"        \"species\"          \n[3322] \"list\"              \"members\"           \"of\"               \n[3325] \"congress\"          \"on\"                \"tuesday\"          \n[3328] \"threatened\"        \"to\"                \"withhold\"         \n[3331] \"tens\"              \"of\"                \"millions\"         \n[3334] \"of\"                \"dollars\"           \"in\"               \n[3337] \"federal\"           \"funding\"           \"from\"             \n[3340] \"four\"              \"native\"            \"american\"         \n[3343] \"tribes\"            \"in\"                \"oklahoma\"         \n[3346] \"adding\"            \"to\"                \"renewed\"          \n[3349] \"public\"            \"pressure\"          \"to\"               \n[3352] \"end\"               \"policies\"          \"that\"             \n[3355] \"discriminate\"      \"against\"           \"descendants\"      \n[3358] \"of\"                \"black\"             \"people\"           \n[3361] \"who\"               \"were\"              \"enslaved\"         \n[3364] \"by\"                \"the\"               \"tribes\"           \n[3367] \"before\"            \"the\"               \"civil\"            \n[3370] \"war\"               \"a\"                 \"federal\"          \n[3373] \"board\"             \"on\"                \"thursday\"         \n[3376] \"approved\"          \"the\"               \"renaming\"         \n[3379] \"of\"                \"16\"                \"sites\"            \n[3382] \"in\"                \"texas\"             \"whose\"            \n[3385] \"names\"             \"include\"           \"the\"              \n[3388] \"word\"              \"negro\"             \"a\"                \n[3391] \"change\"            \"long\"              \"sought\"           \n[3394] \"by\"                \"politicians\"       \"and\"              \n[3397] \"activists\"         \"in\"                \"the\"              \n[3400] \"state\"             \"but\"               \"one\"              \n[3403] \"that\"              \"will\"              \"affect\"           \n[3406] \"only\"              \"a\"                 \"small\"            \n[3409] \"fraction\"          \"of\"                \"the\"              \n[3412] \"hundreds\"          \"of\"                \"racist\"           \n[3415] \"names\"             \"of\"                \"towns\"            \n[3418] \"and\"               \"geographical\"      \"features\"         \n[3421] \"that\"              \"remain\"            \"in\"               \n[3424] \"the\"               \"united\"            \"states\"           \n[3427] \"did\"               \"you\"               \"follow\"           \n[3430] \"the\"               \"news\"              \"this\"             \n[3433] \"week\"              \"take\"              \"our\"              \n[3436] \"quiz\"              \"to\"                \"see\"              \n[3439] \"how\"               \"well\"              \"you\"              \n[3442] \"stack\"             \"up\"                \"with\"             \n[3445] \"other\"             \"times\"             \"readers\"          \n[3448] \"kathryn\"           \"dunn\"              \"tenpas\"           \n[3451] \"senior\"            \"fellow\"            \"at\"               \n[3454] \"the\"               \"university\"        \"of\"               \n[3457] \"virginia’s\"        \"miller\"            \"center\"           \n[3460] \"washington\"        \"a\"                 \"federal\"          \n[3463] \"judge\"             \"on\"                \"thursday\"         \n[3466] \"canceled\"          \"oil\"               \"and\"              \n[3469] \"gas\"               \"leases\"            \"of\"               \n[3472] \"more\"              \"than\"              \"80\"               \n[3475] \"million\"           \"acres\"             \"in\"               \n[3478] \"the\"               \"gulf\"              \"of\"               \n[3481] \"mexico\"            \"ruling\"            \"that\"             \n[3484] \"the\"               \"biden\"             \"administration\"   \n[3487] \"did\"               \"not\"               \"sufficiently\"     \n[3490] \"take\"              \"climate\"           \"change\"           \n[3493] \"into\"              \"account\"           \"when\"             \n[3496] \"it\"                \"auctioned\"         \"the\"              \n[3499] \"leases\"            \"late\"              \"last\"             \n[3502] \"year\"              \"washington\"        \"the\"              \n[3505] \"biden\"             \"administration\"    \"on\"               \n[3508] \"friday\"            \"announced\"         \"that\"             \n[3511] \"it\"                \"would\"             \"begin\"            \n[3514] \"the\"               \"formal\"            \"process\"          \n[3517] \"of\"                \"selling\"           \"leases\"           \n[3520] \"to\"                \"develop\"           \"offshore\"         \n[3523] \"wind\"              \"farms\"             \"in\"               \n[3526] \"shallow\"           \"waters\"            \"between\"          \n[3529] \"long\"              \"island\"            \"and\"              \n[3532] \"new\"               \"jersey\"            \"as\"               \n[3535] \"part\"              \"of\"                \"its\"              \n[3538] \"push\"              \"to\"                \"transition\"       \n[3541] \"the\"               \"nation\"            \"to\"               \n[3544] \"renewable\"         \"energy\"            \"sunday\"           \n[3547] \"puzzle\"            \"today\"             \"we\"               \n[3550] \"have\"              \"a\"                 \"visually\"         \n[3553] \"unusual\"           \"challenge\"         \"by\"               \n[3556] \"two\"               \"excellent\"         \"constructors\"     \n[3559] \"matthew\"           \"stock\"             \"and\"              \n[3562] \"will\"              \"nediger\"           \"who\"              \n[3565] \"are\"               \"collaborating\"     \"for\"              \n[3568] \"the\"               \"first\"             \"time\"             \n[3571] \"in\"                \"the\"               \"new\"              \n[3574] \"york\"              \"times\"             \"according\"        \n[3577] \"to\"                \"the\"               \"puzzle’s\"         \n[3580] \"print\"             \"introduction\"      \"the\"              \n[3583] \"two\"               \"connected\"         \"online\"           \n[3586] \"and\"               \"developed\"         \"a\"                \n[3589] \"theme\"             \"conceived\"         \"by\"               \n[3592] \"mr\"                \"stock\"             \"using\"            \n[3595] \"direct\"            \"messages\"          \"on\"               \n[3598] \"twitter\"           \"and\"               \"swapping\"         \n[3601] \"grids\"             \"until\"             \"they\"             \n[3604] \"were\"              \"both\"              \"happy\"            \n[3607] \"they\"              \"have\"              \"yet\"              \n[3610] \"to\"                \"meet\"              \"even\"             \n[3613] \"on\"                \"video\"             \"pride\"            \n[3616] \"celebrations\"      \"in\"                \"june\"             \n[3619] \"with\"              \"their\"             \"marches\"          \n[3622] \"parades\"           \"and\"               \"parties\"          \n[3625] \"can\"               \"trace\"             \"their\"            \n[3628] \"histories\"         \"back\"              \"to\"               \n[3631] \"a\"                 \"fiery\"             \"street\"           \n[3634] \"demonstration\"     \"in\"                \"1970\"             \n[3637] \"the\"               \"year\"              \"after\"            \n[3640] \"a\"                 \"violent\"           \"uprising\"         \n[3643] \"at\"                \"the\"               \"stonewall\"        \n[3646] \"inn\"               \"in\"                \"manhattan\"        \n[3649] \"that\"              \"galvanized\"        \"the\"              \n[3652] \"modern\"            \"gay\"               \"rights\"           \n[3655] \"movement\"          \"the\"               \"european\"         \n[3658] \"championship\"      \"generally\"         \"considered\"       \n[3661] \"the\"               \"biggest\"           \"soccer\"           \n[3664] \"tournament\"        \"after\"             \"the\"              \n[3667] \"world\"             \"cup\"               \"is\"               \n[3670] \"being\"             \"held\"              \"this\"             \n[3673] \"summer\"            \"after\"             \"a\"                \n[3676] \"year’s\"            \"delay\"             \"because\"          \n[3679] \"of\"                \"the\"               \"coronavirus\"      \n[3682] \"pandemic\"          \"here’s\"            \"a\"                \n[3685] \"rundown\"           \"on\"                \"the\"              \n[3688] \"teams\"             \"the\"               \"players\"          \n[3691] \"and\"               \"the\"               \"host\"             \n[3694] \"cities\"            \"for\"               \"what\"             \n[3697] \"is\"                \"still\"             \"being\"            \n[3700] \"called\"            \"euro\"              \"2020\"             \n[3703] \"as\"                \"the\"               \"ocean\"            \n[3706] \"rises\"             \"homeowners\"        \"in\"               \n[3709] \"avon\"              \"a\"                 \"tiny\"             \n[3712] \"town\"              \"on\"                \"the\"              \n[3715] \"outer\"             \"banks\"             \"of\"               \n[3718] \"north\"             \"carolina\"          \"are\"              \n[3721] \"confronting\"       \"a\"                 \"tax\"              \n[3724] \"increase\"          \"of\"                \"almost\"           \n[3727] \"50\"                \"percent\"           \"to\"               \n[3730] \"defend\"            \"the\"               \"only\"             \n[3733] \"road\"              \"into\"              \"town\"             \n[3736] \"residents\"         \"want\"              \"somebody\"         \n[3739] \"else\"              \"to\"                \"pay\"              \n[3742] \"for\"               \"it\"                \"local\"            \n[3745] \"officials\"         \"say\"               \"they’re\"          \n[3748] \"on\"                \"their\"             \"own\"              \n[3751] \"nearly\"            \"two\"               \"decades\"          \n[3754] \"ago\"               \"when\"              \"the\"              \n[3757] \"new\"               \"zealand\"           \"highway\"          \n[3760] \"authority\"         \"was\"               \"planning\"         \n[3763] \"the\"               \"waikato\"           \"expressway\"       \n[3766] \"people\"            \"from\"              \"the\"              \n[3769] \"māori\"             \"tribe\"             \"ngāti\"            \n[3772] \"naho\"              \"objected\"          \"the\"              \n[3775] \"highway\"           \"would\"             \"encroach\"         \n[3778] \"on\"                \"an\"                \"area\"             \n[3781] \"that\"              \"in\"                \"māori\"            \n[3784] \"tradition\"         \"was\"               \"governed\"         \n[3787] \"by\"                \"a\"                 \"water\"            \n[3790] \"dwelling\"          \"creature\"          \"a\"                \n[3793] \"taniwha\"           \"the\"               \"world\"            \n[3796] \"cup\"               \"draw\"              \"has\"              \n[3799] \"set\"               \"the\"               \"field\"            \n[3802] \"even\"              \"though\"            \"the\"              \n[3805] \"invitation\"        \"list\"              \"for\"              \n[3808] \"this\"              \"year’s\"            \"tournament\"       \n[3811] \"in\"                \"qatar\"             \"isn’t\"            \n[3814] \"yet\"               \"complete\"          \"yet\"              \n[3817] \"even\"              \"as\"                \"the\"              \n[3820] \"teams\"             \"now\"               \"know\"             \n[3823] \"who\"               \"and\"               \"when\"             \n[3826] \"they\"              \"will\"              \"play\"             \n[3829] \"there\"             \"are\"               \"still\"            \n[3832] \"plenty\"            \"of\"                \"questions\"        \n[3835] \"about\"             \"how\"               \"things\"           \n[3838] \"will\"              \"play\"              \"out\"              \n[3841] \"in\"                \"soccer’s\"          \"first\"            \n[3844] \"winter\"            \"world\"             \"cup\"              \n[3847] \"here’s\"            \"a\"                 \"primer\"           \n[3850] \"on\"                \"the\"               \"world’s\"          \n[3853] \"greatest\"          \"sporting\"          \"spectacle\"        \n[3856] \"president\"         \"biden\"             \"has\"              \n[3859] \"proclaimed\"        \"monday\"            \"oct\"              \n[3862] \"11\"                \"as\"                \"indigenous\"       \n[3865] \"peoples\"           \"day\"               \"becoming\"         \n[3868] \"the\"               \"first\"             \"u.s\"              \n[3871] \"president\"         \"to\"                \"formally\"         \n[3874] \"recognize\"         \"the\"               \"day\"              \n[3877] \"the\"               \"biden\"             \"administration\"   \n[3880] \"has\"               \"defended\"          \"a\"                \n[3883] \"contentious\"       \"pipeline\"          \"project\"          \n[3886] \"that\"              \"would\"             \"carry\"            \n[3889] \"hundreds\"          \"of\"                \"thousands\"        \n[3892] \"of\"                \"barrels\"           \"of\"               \n[3895] \"oil\"               \"through\"           \"minnesota’s\"      \n[3898] \"delicate\"          \"watersheds\"        \"urging\"           \n[3901] \"in\"                \"a\"                 \"court\"            \n[3904] \"brief\"             \"that\"              \"a\"                \n[3907] \"challenge\"         \"brought\"           \"by\"               \n[3910] \"local\"             \"tribes\"            \"and\"              \n[3913] \"environmental\"     \"groups\"            \"be\"               \n[3916] \"thrown\"            \"out\"               \"a\"                \n[3919] \"northern\"          \"branch\"            \"of\"               \n[3922] \"the\"               \"gulf\"              \"stream\"           \n[3925] \"the\"               \"vast\"              \"ocean\"            \n[3928] \"current\"           \"that\"              \"runs\"             \n[3931] \"from\"              \"west\"              \"africa\"           \n[3934] \"to\"                \"the\"               \"americas\"         \n[3937] \"up\"                \"the\"               \"east\"             \n[3940] \"coast\"             \"and\"               \"back\"             \n[3943] \"across\"            \"the\"               \"atlantic\"         \n[3946] \"to\"                \"the\"               \"british\"          \n[3949] \"isles\"             \"has\"               \"served\"           \n[3952] \"for\"               \"ages\"              \"as\"               \n[3955] \"a\"                 \"kind\"              \"of\"               \n[3958] \"planetary\"         \"heat\"              \"pump\"             \n[3961] \"that\"              \"helps\"             \"regulate\"         \n[3964] \"the\"               \"planet’s\"          \"climate\"          \n[3967] \"hi\"                \"welcome\"           \"to\"               \n[3970] \"on\"                \"politics\"          \"your\"             \n[3973] \"wrap\"              \"up\"                \"of\"               \n[3976] \"the\"               \"week\"              \"in\"               \n[3979] \"national\"          \"politics\"          \"i’m\"              \n[3982] \"lisa\"              \"lerer\"             \"your\"             \n[3985] \"host\"              \"washington\"        \"president\"        \n[3988] \"biden\"             \"may\"               \"be\"               \n[3991] \"forced\"            \"to\"                \"hold\"             \n[3994] \"a\"                 \"new\"               \"lease\"            \n[3997] \"sale\"              \"for\"               \"oil\"              \n[4000] \"drilling\"          \"in\"                \"the\"              \n[4003] \"pristine\"          \"arctic\"            \"national\"         \n[4006] \"wildlife\"          \"refuge\"            \"despite\"          \n[4009] \"his\"               \"vows\"              \"to\"               \n[4012] \"slash\"             \"fossil\"            \"fuel\"             \n[4015] \"pollution\"         \"and\"               \"his\"              \n[4018] \"action\"            \"this\"              \"week\"             \n[4021] \"to\"                \"suspend\"           \"arctic\"           \n[4024] \"drilling\"          \"leases\"            \"that\"             \n[4027] \"had\"               \"been\"              \"awarded\"          \n[4030] \"in\"                \"the\"               \"final\"            \n[4033] \"days\"              \"of\"                \"the\"              \n[4036] \"trump\"             \"administration\"    \"construction\"     \n[4039] \"on\"                \"the\"               \"nation’s\"         \n[4042] \"first\"             \"commercial\"        \"scale\"            \n[4045] \"offshore\"          \"wind\"              \"farm\"             \n[4048] \"is\"                \"expected\"          \"to\"               \n[4051] \"begin\"             \"this\"              \"summer\"           \n[4054] \"after\"             \"the\"               \"biden\"            \n[4057] \"administration\"    \"gave\"              \"final\"            \n[4060] \"approval\"          \"tuesday\"           \"to\"               \n[4063] \"a\"                 \"project\"           \"it\"               \n[4066] \"hopes\"             \"will\"              \"herald\"           \n[4069] \"a\"                 \"new\"               \"era\"              \n[4072] \"of\"                \"wind\"              \"energy\"           \n[4075] \"across\"            \"the\"               \"united\"           \n[4078] \"states\"            \"albuquerque\"       \"this\"             \n[4081] \"year\"              \"new\"               \"mexican\"          \n[4084] \"officials\"         \"have\"              \"a\"                \n[4087] \"message\"           \"for\"               \"farmers\"          \n[4090] \"who\"               \"depend\"            \"on\"               \n[4093] \"irrigation\"        \"water\"             \"from\"             \n[4096] \"the\"               \"rio\"               \"grande\"           \n[4099] \"and\"               \"other\"             \"rivers\"           \n[4102] \"unless\"            \"you\"               \"absolutely\"       \n[4105] \"have\"              \"to\"                \"plant\"            \n[4108] \"this\"              \"year\"              \"don’t\"            \n[4111] \"good\"              \"morning\"           \"washington\"       \n[4114] \"the\"               \"biden\"             \"administration\"   \n[4117] \"is\"                \"defending\"         \"a\"                \n[4120] \"huge\"              \"trump\"             \"era\"              \n[4123] \"oil\"               \"and\"               \"gas\"              \n[4126] \"project\"           \"in\"                \"the\"              \n[4129] \"north\"             \"slope\"             \"of\"               \n[4132] \"alaska\"            \"designed\"          \"to\"               \n[4135] \"produce\"           \"more\"              \"than\"             \n[4138] \"100,000\"           \"barrels\"           \"of\"               \n[4141] \"oil\"               \"a\"                 \"day\"              \n[4144] \"for\"               \"the\"               \"next\"             \n[4147] \"30\"                \"years\"             \"despite\"          \n[4150] \"president\"         \"biden’s\"           \"pledge\"           \n[4153] \"to\"                \"pivot\"             \"the\"              \n[4156] \"country\"           \"away\"              \"from\"             \n[4159] \"fossil\"            \"fuels\"             \"it\"               \n[4162] \"was\"               \"the\"               \"year\"             \n[4165] \"public\"            \"figures\"           \"crept\"            \n[4168] \"back\"              \"into\"              \"the\"              \n[4171] \"spotlight\"         \"first\"             \"with\"             \n[4174] \"a\"                 \"tiptoe\"            \"out\"              \n[4177] \"of\"                \"doors\"             \"at\"               \n[4180] \"the\"               \"presidential\"      \"inauguration\"     \n[4183] \"then\"              \"a\"                 \"socially\"         \n[4186] \"distanced\"         \"red\"               \"carpet\"           \n[4189] \"at\"                \"the\"               \"oscars\"           \n[4192] \"and\"               \"then\"              \"a\"                \n[4195] \"full\"              \"throttle\"          \"ball\"             \n[4198] \"gowns\"             \"to\"                \"the\"              \n[4201] \"max\"               \"explosion\"         \"of\"               \n[4204] \"pent\"              \"up\"                \"dressing\"         \n[4207] \"up\"                \"at\"                \"the\"              \n[4210] \"cannes\"            \"film\"              \"festival\"         \n[4213] \"as\"                \"if\"                \"to\"               \n[4216] \"compensate\"        \"for\"               \"the\"              \n[4219] \"previous\"          \"year\"              \"of\"               \n[4222] \"enforced\"          \"isolation\"         \"want\"             \n[4225] \"to\"                \"get\"               \"this\"             \n[4228] \"newsletter\"        \"in\"                \"your\"             \n[4231] \"inbox\"             \"here’s\"            \"the\"              \n[4234] \"sign\"              \"up\"                \"a\"                \n[4237] \"month\"             \"ago\"               \"the\"              \n[4240] \"school\"            \"board\"             \"in\"               \n[4243] \"a\"                 \"northern\"          \"new\"              \n[4246] \"jersey\"            \"suburb\"            \"followed\"         \n[4249] \"the\"               \"lead\"              \"of\"               \n[4252] \"at\"                \"least\"             \"six\"              \n[4255] \"other\"             \"states\"            \"and\"              \n[4258] \"scores\"            \"of\"                \"municipalities\"   \n[4261] \"when\"              \"it\"                \"voted\"            \n[4264] \"unanimously\"       \"to\"                \"rename\"           \n[4267] \"columbus\"          \"day\"               \"as\"               \n[4270] \"indigenous\"        \"peoples\"           \"day\"              \n[4273] \"want\"              \"to\"                \"get\"              \n[4276] \"this\"              \"newsletter\"        \"in\"               \n[4279] \"your\"              \"inbox\"             \"here’s\"           \n[4282] \"the\"               \"sign\"              \"up\"               \n[4285] \"want\"              \"to\"                \"get\"              \n[4288] \"this\"              \"newsletter\"        \"in\"               \n[4291] \"your\"              \"inbox\"             \"here’s\"           \n[4294] \"the\"               \"sign\"              \"up\"               \n[4297] \"washington\"        \"the\"               \"notion\"           \n[4300] \"of\"                \"wind\"              \"farms\"            \n[4303] \"churning\"          \"in\"                \"the\"              \n[4306] \"pacific\"           \"ocean\"             \"creating\"         \n[4309] \"clean\"             \"energy\"            \"to\"               \n[4312] \"power\"             \"homes\"             \"and\"              \n[4315] \"businesses\"        \"has\"               \"long\"             \n[4318] \"been\"              \"dismissed\"         \"because\"          \n[4321] \"of\"                \"logistical\"        \"challenges\"       \n[4324] \"posed\"             \"by\"                \"a\"                \n[4327] \"deep\"              \"ocean\"             \"floor\"            \n[4330] \"and\"               \"opposition\"        \"from\"             \n[4333] \"the\"               \"military\"          \"which\"            \n[4336] \"prefers\"           \"no\"                \"obstacles\"        \n[4339] \"for\"               \"its\"               \"navy\"             \n[4342] \"ships\"             \"want\"              \"to\"               \n[4345] \"get\"               \"this\"              \"newsletter\"       \n[4348] \"in\"                \"your\"              \"inbox\"            \n[4351] \"here’s\"            \"the\"               \"sign\"             \n[4354] \"up\"                \"washington\"        \"president\"        \n[4357] \"biden\"             \"said\"              \"on\"               \n[4360] \"wednesday\"         \"that\"              \"states\"           \n[4363] \"could\"             \"draw\"              \"from\"             \n[4366] \"350\"               \"billion\"           \"in\"               \n[4369] \"federal\"           \"stimulus\"          \"money\"            \n[4372] \"to\"                \"shore\"             \"up\"               \n[4375] \"police\"            \"departments\"       \"and\"              \n[4378] \"vowed\"             \"to\"                \"crack\"            \n[4381] \"down\"              \"on\"                \"gun\"              \n[4384] \"dealers\"           \"who\"               \"fail\"             \n[4387] \"to\"                \"run\"               \"background\"       \n[4390] \"checks\"            \"as\"                \"the\"              \n[4393] \"white\"             \"house\"             \"seeks\"            \n[4396] \"to\"                \"combat\"            \"the\"              \n[4399] \"alarming\"          \"rise\"              \"in\"               \n[4402] \"homicide\"          \"rates\"             \"in\"               \n[4405] \"america’s\"         \"cities\"            \"washington\"       \n[4408] \"when\"              \"president\"         \"biden\"            \n[4411] \"nominated\"         \"judge\"             \"ketanji\"          \n[4414] \"brown\"             \"jackson\"           \"to\"               \n[4417] \"a\"                 \"prestigious\"       \"appeals\"          \n[4420] \"court\"             \"last\"              \"year\"             \n[4423] \"senator\"           \"lisa\"              \"murkowski\"        \n[4426] \"of\"                \"alaska\"            \"a\"                \n[4429] \"centrist\"          \"known\"             \"for\"              \n[4432] \"her\"               \"willingness\"       \"to\"               \n[4435] \"break\"             \"with\"              \"her\"              \n[4438] \"party\"             \"was\"               \"one\"              \n[4441] \"of\"                \"only\"              \"three\"            \n[4444] \"republicans\"       \"to\"                \"vote\"             \n[4447] \"to\"                \"confirm\"           \"her\"              \n[4450] \"washington\"        \"president\"         \"biden’s\"          \n[4453] \"nominee\"           \"to\"                \"head\"             \n[4456] \"the\"               \"office\"            \"of\"               \n[4459] \"management\"        \"and\"               \"budget\"           \n[4462] \"continued\"         \"to\"                \"face\"             \n[4465] \"senate\"            \"opposition\"        \"on\"               \n[4468] \"thursday\"          \"narrowing\"         \"her\"              \n[4471] \"chances\"           \"of\"                \"confirmation\"     \n[4474] \"and\"               \"sending\"           \"white\"            \n[4477] \"house\"             \"officials\"         \"on\"               \n[4480] \"a\"                 \"frantic\"           \"search\"           \n[4483] \"for\"               \"at\"                \"least\"            \n[4486] \"one\"               \"republican\"        \"vote\"             \n[4489] \"to\"                \"salvage\"           \"her\"              \n[4492] \"nomination\"        \"the\"               \"indian\"           \n[4495] \"health\"            \"service\"           \"announced\"        \n[4498] \"this\"              \"week\"              \"that\"             \n[4501] \"black\"             \"native\"            \"americans\"        \n[4504] \"in\"                \"the\"               \"seminole\"         \n[4507] \"nation\"            \"known\"             \"as\"               \n[4510] \"the\"               \"freedmen\"          \"will\"             \n[4513] \"now\"               \"be\"                \"eligible\"         \n[4516] \"for\"               \"health\"            \"care\"             \n[4519] \"through\"           \"the\"               \"federal\"          \n[4522] \"agency\"            \"which\"             \"previously\"       \n[4525] \"denied\"            \"them\"              \"coronavirus\"      \n[4528] \"vaccinations\"      \"and\"               \"other\"            \n[4531] \"care\"              \"washington\"        \"after\"            \n[4534] \"a\"                 \"year\"              \"that\"             \n[4537] \"provided\"          \"stark\"             \"new\"              \n[4540] \"evidence\"          \"of\"                \"how\"              \n[4543] \"racial\"            \"inequities\"        \"and\"              \n[4546] \"a\"                 \"lack\"              \"of\"               \n[4549] \"federal\"           \"funding\"           \"had\"              \n[4552] \"left\"              \"tribal\"            \"communities\"      \n[4555] \"and\"               \"indigenous\"        \"people\"           \n[4558] \"especially\"        \"vulnerable\"        \"to\"               \n[4561] \"crises\"            \"like\"              \"the\"              \n[4564] \"pandemic\"          \"president\"         \"biden\"            \n[4567] \"and\"               \"democrats\"         \"in\"               \n[4570] \"congress\"          \"are\"               \"seeking\"          \n[4573] \"to\"                \"address\"           \"those\"            \n[4576] \"longstanding\"      \"issues\"            \"with\"             \n[4579] \"a\"                 \"huge\"              \"infusion\"         \n[4582] \"of\"                \"federal\"           \"aid\"              \n[4585] \"we\"                \"have\"              \"selected\"         \n[4588] \"six\"               \"times\"             \"articles\"         \n[4591] \"published\"         \"in\"                \"the\"              \n[4594] \"last\"              \"year\"              \"that\"             \n[4597] \"feature\"           \"stories\"           \"of\"               \n[4600] \"native\"            \"americans\"         \"who\"              \n[4603] \"are\"               \"fighting\"          \"to\"               \n[4606] \"ensure\"            \"representation\"    \"repatriation\"     \n[4609] \"and\"               \"recognition\"       \"durango\"          \n[4612] \"colo\"              \"the\"               \"last\"             \n[4615] \"day\"               \"dzabahe\"           \"remembers\"        \n[4618] \"praying\"           \"in\"                \"the\"              \n[4621] \"way\"               \"of\"                \"her\"              \n[4624] \"ancestors\"         \"was\"               \"on\"               \n[4627] \"the\"               \"morning\"           \"in\"               \n[4630] \"the\"               \"1950s\"             \"when\"             \n[4633] \"she\"               \"was\"               \"taken\"            \n[4636] \"to\"                \"the\"               \"boarding\"         \n[4639] \"school\"            \"don\"               \"young\"            \n[4642] \"the\"               \"alaska\"            \"congressman\"      \n[4645] \"who\"               \"secured\"           \"pork\"             \n[4648] \"barrel\"            \"billions\"          \"for\"              \n[4651] \"his\"               \"state\"             \"over\"             \n[4654] \"nearly\"            \"a\"                 \"half\"             \n[4657] \"century\"           \"and\"               \"became\"           \n[4660] \"the\"               \"longest\"           \"serving\"          \n[4663] \"republican\"        \"in\"                \"the\"              \n[4666] \"house\"             \"of\"                \"representatives\"  \n[4669] \"and\"               \"the\"               \"oldest\"           \n[4672] \"current\"           \"member\"            \"of\"               \n[4675] \"both\"              \"the\"               \"house\"            \n[4678] \"and\"               \"senate\"            \"died\"             \n[4681] \"on\"                \"friday\"            \"he\"               \n[4684] \"was\"               \"88\"                \"northern\"         \n[4687] \"cheyenne\"          \"reservation\"       \"mont\"             \n[4690] \"the\"               \"knock\"             \"on\"               \n[4693] \"the\"               \"door\"              \"came\"             \n[4696] \"at\"                \"3\"                 \"a.m\"              \n[4699] \"when\"              \"pauline\"           \"highwolf\"         \n[4702] \"opened\"            \"it\"                \"to\"               \n[4705] \"see\"               \"a\"                 \"police\"           \n[4708] \"officer\"           \"from\"              \"the\"              \n[4711] \"bureau\"            \"of\"                \"indian\"           \n[4714] \"affairs\"           \"don’t\"             \"tell\"             \n[4717] \"me\"                \"she\"               \"said\"             \n[4720] \"backing\"           \"away\"              \"it\"               \n[4723] \"was\"               \"the\"               \"no\"               \n[4726] \"8\"                 \"who\"               \"first\"            \n[4729] \"caught\"            \"the\"               \"eye\"              \n[4732] \"he\"                \"was\"               \"tall\"             \n[4735] \"languid\"           \"just\"              \"on\"               \n[4738] \"the\"               \"border\"            \"between\"          \n[4741] \"rangy\"             \"and\"               \"ungainly\"         \n[4744] \"it\"                \"was\"               \"not\"              \n[4747] \"the\"               \"way\"               \"he\"               \n[4750] \"moved\"             \"so\"                \"much\"             \n[4753] \"but\"               \"the\"               \"way\"              \n[4756] \"he\"                \"did\"               \"not\"              \n[4759] \"in\"                \"the\"               \"middle\"           \n[4762] \"of\"                \"all\"               \"the\"              \n[4765] \"bustle\"            \"and\"               \"hurry\"            \n[4768] \"he\"                \"was\"               \"unusually\"        \n[4771] \"still\"             \"he\"                \"did\"              \n[4774] \"not\"               \"sprint\"            \"he\"               \n[4777] \"did\"               \"not\"               \"dash\"             \n[4780] \"he\"                \"did\"               \"not\"              \n[4783] \"even\"              \"run\"               \"not\"              \n[4786] \"really\"            \"he\"                \"strolled\"         \n[4789] \"he\"                \"meandered\"         \"he\"               \n[4792] \"moseyed\"           \"sign\"              \"up\"               \n[4795] \"for\"               \"rory\"              \"smith’s\"          \n[4798] \"weekly\"            \"newsletter\"        \"on\"               \n[4801] \"world\"             \"soccer\"            \"delivered\"        \n[4804] \"every\"             \"friday\"            \"at\"               \n[4807] \"nytimes.com\"       \"rory\"              \"yellow\"           \n[4810] \"pine\"              \"idaho\"             \"net\"              \n[4813] \"in\"                \"hand\"              \"louis\"            \n[4816] \"reuben\"            \"waded\"             \"into\"             \n[4819] \"the\"               \"frigid\"            \"waters\"           \n[4822] \"where\"             \"his\"               \"ancestors\"        \n[4825] \"once\"              \"fished\"            \"long\"             \n[4828] \"before\"            \"idaho’s\"           \"rivers\"           \n[4831] \"were\"              \"dammed\"            \"and\"              \n[4834] \"contaminated\"      \"before\"            \"the\"              \n[4837] \"nez\"               \"perce\"             \"were\"             \n[4840] \"driven\"            \"off\"               \"their\"            \n[4843] \"land\"              \"when\"              \"white\"            \n[4846] \"miners\"            \"struck\"            \"gold\"             \n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nUh oh, who knows what we need to do here?\n\n\ndata(stop_words)\nstop_words\n\n\n# A tibble: 1,149 × 2\n   word        lexicon\n   <chr>       <chr>  \n 1 a           SMART  \n 2 a's         SMART  \n 3 able        SMART  \n 4 about       SMART  \n 5 above       SMART  \n 6 according   SMART  \n 7 accordingly SMART  \n 8 across      SMART  \n 9 actually    SMART  \n10 after       SMART  \n# … with 1,139 more rows\n\ntokenized <- tokenized %>%\n  anti_join(stop_words)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nOK, but look at the most common words. Does one stick out?\n\n\n#inspect the list of tokens (words)\ntokenized$word\n\nclean_tokens <- str_replace_all(tokenized$word,\"land[a-z,A-Z]*\",\"land\") #stem land* words\nclean_tokens <- str_remove_all(clean_tokens, \"[:digit:]\") #remove all numbers\n\nclean_tokens <- gsub(\"’s\", '', clean_tokens)\n\ntokenized$clean <- clean_tokens\n\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n#remove the empty strings\ntib <-subset(tokenized, clean!=\"\")\n\n#reassign\ntokenized <- tib\n\n#try again\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% \n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\nAssignment (Due by Week 3)\nCreate a free New York Times account (https://developer.nytimes.com/get-started)\nPick an interesting environmental key word(s) and use the\njsonlite package to query the API. Pick something high profile enough\nand over a large enough time frame that your query yields enough\narticles for an interesting examination.\nRecreate the publications per day and word frequency plots using\nthe first paragraph\nMake some (at least 3) transformations to the corpus (add\nstopword(s), stem a key term and its variants, remove numbers)\nRecreate the publications per day and word frequency plots using the\nheadlines variable (response.docs.headline.main). Compare the\ndistributions of word frequencies between the first paragraph and\nheadlines. Do you see any difference?\n\n\n\n",
      "last_modified": "2022-04-29T14:36:54-07:00"
    },
    {
      "path": "topic_3.html",
      "title": "Topic 3: Sentiment Analysis I",
      "author": [],
      "contents": "\nOverview\nSentiment analysis is a tool for assessing the mood of a piece of\ntext. For example, we can use sentiment analysis to understand public\nperceptions of topics in environmental policy like energy, climate, and\nconservation.\n\n\nlibrary(tidyr) #text analysis in R\nlibrary(lubridate) #working with date data\nlibrary(pdftools) #read in pdfs\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(here)\nlibrary(LexisNexisTools) #Nexis Uni data wrangling\nlibrary(sentimentr)\nlibrary(readr)\n\n\n\nIntro to sentiment analysis\nexample\nFor this introductory example, I selected a text excerpt from the\nNational Book Award winning book, The Overstory by Richard\nPowers.\nThe excerpt is in .pdf format, so we’ll need a tool (the pdftools\npackage) to help us deal with that.\n\n\n# over <- pdf_text('dat/overstory_exerpt.pdf')\n# over_df <- data.frame(text = over) %>% #create 1-column df with 'text' variable\n#   mutate(page = 1:n()) #add a page number variable, 'page'\n\n#examine the beginning of the data frame\n\n# over_text <- over_df %>%\n#   filter(page %in% 8:41)%>%\n#   mutate(text = str_split(text, '\\n')) %>% #this splits by page. \n#   unnest(text) %>%  #this splits by line\n#   mutate(line = str_to_lower(text)) #and convert to all lower case\n\n#write_csv(over_text, \"dat/over_text.csv\")\n\n#Note: \\n, used above, is an example of an \"escape sequence\", which allow you to include characters that would otherwise break the code\n\n#***Update: Skip the PDF steps and use this already-constructed .csv\nover_text <- read_csv(\"dat/over_text.csv\")\n\n\n\nWe’ll start by using the Bing sentiment analysis lexicon.\n\n\nbing_sent <- get_sentiments('bing') #grab the bing sentiment lexicon from tidytext\nhead(bing_sent, n = 20)\n\n\n# A tibble: 20 × 2\n   word          sentiment\n   <chr>         <chr>    \n 1 2-faces       negative \n 2 abnormal      negative \n 3 abolish       negative \n 4 abominable    negative \n 5 abominably    negative \n 6 abominate     negative \n 7 abomination   negative \n 8 abort         negative \n 9 aborted       negative \n10 aborts        negative \n11 abound        positive \n12 abounds       positive \n13 abrade        negative \n14 abrasive      negative \n15 abrupt        negative \n16 abruptly      negative \n17 abscond       negative \n18 absence       negative \n19 absent-minded negative \n20 absentee      negative \n\nHere is the starting point for reading in the data as a .csv. We need\nto unnest the text to the word level so we can label the individual\nsentiment words. Let’s also remove stop words as standard text cleaning\nprocedure. Note: Not every English word is in the lexicons because many\nEnglish words are pretty neutral.\n\n\nover_text <- read_csv('dat/over_text.csv')\n\n#unnest to word-level tokens, remove stop words, and join sentiment words\n text_words <- over_text  %>%\n  unnest_tokens(output = word, input = text, token = 'words')\n \n sent_words <- text_words%>% #break text into individual words\n  anti_join(stop_words, by = 'word') %>% #returns only the rows without stop words\n  inner_join(bing_sent, by = 'word') #joins and retains only sentiment words\n\n\n\nCreate a sentiment score by counting the number of sentiment words\noccurring per page. We can center the scores around an offset point\nequal to the average page sentiment score. This lets us measure the\nsentiment of a given page relative to the overall sentiment of the\nbook.\n\n\nsent_scores <- sent_words %>%\n  count(sentiment, page) %>%\n  spread(sentiment, n) %>%\n  mutate(raw_score = positive - negative, #single sentiment score per page\n  offset = mean(positive - negative), #what is the average sentiment per page?\n  offset_score = (positive - negative) - offset) %>% #how does this page's sentiment compare to that of the average page?\n  arrange(desc(raw_score))\nsent_scores\n\n\n# A tibble: 34 × 6\n    page negative positive raw_score offset offset_score\n   <dbl>    <int>    <int>     <int>  <dbl>        <dbl>\n 1    21        6       12         6     -3            9\n 2    17        4        8         4     -3            7\n 3    18        9       13         4     -3            7\n 4     8        1        3         2     -3            5\n 5    13        6        8         2     -3            5\n 6    22       10       12         2     -3            5\n 7    35       10       11         1     -3            4\n 8    39        4        5         1     -3            4\n 9     9        3        3         0     -3            3\n10    40       14       14         0     -3            3\n# … with 24 more rows\n\n\n\nggplot(sent_scores, aes(x = page)) +\n  theme_classic() +\n  geom_bar(aes(y = raw_score), stat = 'identity', fill = 'slateblue3') +\n  geom_bar(aes(y = offset_score), stat = 'identity', fill = 'red4') +\n  geom_hline(yintercept = sent_scores$offset[1], linetype = 'dashed', size = .5) +\n  #coord_flip() +\n  theme(axis.title.y = element_blank()) +\n  labs(title = 'Sentiment analysis: The Overstory',\n       y = 'Sentiment score')\n\n\n\n\nOrigin of the NRC lexicon\n“These guys selected about 10,000 words from an existing thesaurus…\nand then created a set of five questions to ask about each word that\nwould reveal the emotions and polarity associated with it. That’s a\ntotal of over 50,000 questions.\nThey then asked these questions to over 2000 people, or Turkers, on\nAmazon’s Mechanical Turk website, paying 4 cents for each set of\nproperly answered questions.\nThe result is a comprehensive word-emotion lexicon for over 10,000\nwords.”\nLet’s take a look at the most common sentiment words in the data\nset\n\n\nnrc_sent <- get_sentiments('nrc') #requires downloading a large dataset via prompt\n\nnrc_fear <- get_sentiments(\"nrc\") %>% \n  filter(sentiment == \"fear\")\n\n#most common words by sentiment\nfear_words <- over_text  %>%\n  unnest_tokens(output = word, input = text, token = 'words') %>%\n  inner_join(nrc_fear) %>%\n  count(word, sort = TRUE)\n\n\n\n\n\nnrc_word_counts <- text_words %>%\n  inner_join(get_sentiments(\"nrc\")) %>%\n  count(word, sentiment, sort = TRUE) %>%\n  ungroup()\n\n\n\nLet’s break it out and plot the contributions by particular sentiment\ncategories\n\n\nbook_sent_counts <- text_words %>%\n        group_by(page) %>%\n        # mutate(page_num = 1:n(),\n        #        index = round(page_num / n(), 2)) %>%\n        #unnest_tokens(word, line) %>%\n        inner_join(get_sentiments(\"nrc\")) %>%\n        group_by(sentiment) %>%\n        count(word, sentiment, sort = TRUE) %>%\n        ungroup()\n\nbook_sent_counts %>%\n  group_by(sentiment) %>%\n  slice_max(n, n = 10) %>% \n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free_y\") +\n  labs(x = \"Contribution to sentiment\",\n       y = NULL)\n\n\n\n\nIntroduction to the\nNexis Uni data source\n\n\nsetwd(\"/Users/mateorobbins/Desktop/Git/EDS_231-text-sentiment/nexis_dat\")\n#to follow along with this example, download this .docx to your working directory: \n#https://github.com/MaRo406/EDS_231-text-sentiment/blob/main/nexis_dat/Nexis_IPCC_Results.docx\nmy_files <- list.files(pattern = \".docx\", path = getwd(),\n                       full.names = TRUE, recursive = TRUE, ignore.case = TRUE)\n\ndat <- lnt_read(my_files) #Object of class 'LNT output'\n\n\nmeta_df <- dat@meta\narticles_df <- dat@articles\nparagraphs_df <- dat@paragraphs\n\ndat2<- data_frame(element_id = seq(1:length(meta_df$Headline)), Date = meta_df$Date, Headline = meta_df$Headline)\n\n#May be of use for assignment: using the full text from the articles\n# paragraphs_dat <- data_frame(element_id = paragraphs_df$Art_ID, Text  = paragraphs_df$Paragraph)\n# \n# dat3 <- inner_join(dat2,paragraphs_dat, by = \"element_id\")\n\n\n\n\n\n#can we create a similar graph to Figure 1A from Froelich et al.? \nmytext <- get_sentences(dat2$Headline)\nsent <- sentiment(mytext)\n\nsent_df <- inner_join(dat2, sent, by = \"element_id\")\n\nsentiment <- sentiment_by(sent_df$Headline)\n\nsent_df %>%\n  arrange(sentiment)\n\n\n# A tibble: 109 × 6\n   element_id Date       Headline     sentence_id word_count sentiment\n        <int> <date>     <chr>              <int>      <int>     <dbl>\n 1         66 2022-04-04 Scientists …           1          7    -0.756\n 2         91 2022-04-07 The 'climat…           1          9    -0.75 \n 3         28 2022-04-09 The Dread 1…           1          6    -0.714\n 4         43 2022-04-06 India's ban…           1          7    -0.510\n 5         34 2022-04-08 Dangerous r…           1          6    -0.449\n 6         14 2022-04-04 'Now or nev…           1          8    -0.442\n 7         78 2022-04-07 Statewide G…           1         10    -0.427\n 8         50 2022-04-04 Guardian: M…           1          8    -0.407\n 9         62 2022-04-06 Governor Yo…           1         11    -0.377\n10          7 2022-04-05 Narrow path…           1          8    -0.354\n# … with 99 more rows\n\nsent_df$polarity <- ifelse(sent_df$sentiment <0, -1, ifelse(sent_df$sentiment > 0, 1, 0))\n\n\n\nPseudo code for Froelich et al plot: mean sentiment by day, summarize\nplot x = day, y = sentiment\n\n\ncustom_stop_words <- bind_rows(tibble(word = c(\"your_word\"),  \n                                      lexicon = c(\"custom\")), \n                               stop_words)\n\n\n\nAssignment (Due 4/19 by 11:59\nPM)\nUsing the “IPCC” Nexis Uni data set from the class presentation\nand the pseudo code we discussed, recreate Figure 1A from Froelich et\nal. (Date x # of 1) positive, 2) negative, 3) neutral headlines):\nNewspaper ‘aquaculture’ media sentiment.\nSentiment over time based on the frequency of newspaper Fig. 1.\nHeadlines with negative (red), positive (blue), and neutral (gray)\ntitles\nAccess the Nexis Uni database through the UCSB library: https://www.library.ucsb.edu/research/db/211\nChoose a key search term or terms to define a set of\narticles.\nUse your search term along with appropriate filters to obtain and\ndownload a batch of at least 100 full text search results\n(.docx).\nRead your Nexis article document into RStudio.\nThis time use the full text of the articles for the analysis.\nFirst clean any artifacts of the data collection process (hint: this\ntype of thing should be removed: “Apr 04, 2022( Biofuels Digest: http://www.biofuelsdigest.com/ Delivered by\nNewstex”))\nExplore your data a bit and try to replicate some of the analyses\nabove presented in class if you’d like (not necessary).\nPlot the amount of emotion words (the 8 from nrc) as a percentage\nof all the emotion words used each day (aggregate text from articles\npublished on the same day). How does the distribution of emotion words\nchange over time? Can you think of any reason this would be the\ncase?\n\n\n\n",
      "last_modified": "2022-04-29T14:37:09-07:00"
    },
    {
      "path": "topic_4.html",
      "title": "Topic 4: Sentiment Analysis II",
      "author": [],
      "contents": "\nThis .Rmd available here: https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/topic_4.Rmd\nIPCC Report Twitter\n\n\nlibrary(quanteda)\n#devtools::install_github(\"quanteda/quanteda.sentiment\") #not available currently through CRAN\nlibrary(quanteda.sentiment)\nlibrary(quanteda.textstats)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(lubridate)\nlibrary(wordcloud) #visualization of common words in the data set\nlibrary(reshape2)\n\n\n\nLast week we used the tidytext approach to sentiment analysis for\nNexis Uni .pdf data on coverage of the recent IPCC report. This week we\nwill look at the conversation on Twitter about the same report. We’ll\nstart with the familiar tidy approach, and then introduce the quanteda\npackage later.\n\n\nraw_tweets <- read.csv(\"https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/IPCC_tweets_April1-10_sample.csv\", header=TRUE)\n\ndat<- raw_tweets[,c(4,6)] # Extract Date and Title fields\n\ntweets <- tibble(text = dat$Title,\n                  id = seq(1:length(dat$Title)),\n                 date = as.Date(dat$Date,'%m/%d/%y'))\n\n\nhead(tweets$text, n = 10)\n\n\n [1] \"thank you, followers, for the great photo suggestions for our upcoming IPCC report - on Monday you will find the lucky one selected for our cover from among your submissions!\\n\\nwe now need a good picture on #biofuels . any suggestions please for which we can get copyrights fast?\"        \n [2] \"Greenpeace: The real solution to the climate crisis will require a rapid transition away from fossil fuels. \\n\\nWhat else we expect from the upcoming #IPCC report on climate solutions, set for publication on Monday, 4 April ⬇️  https://t.co/EC6a25S7tY\"                                      \n [3] \"Governments have a responsibility to ensure that #IPCCReport is grounded in rapid phaseout of fossil fuel use and production — not #FalseClimateSolutions. \\n\\nRead more in our open letter: https://t.co/4larBPgeba https://t.co/Fv1OphPmac\"                                                    \n [4] \"Next week, the IPCC will publish a new report detailing their new models and policy pathways. \\n\\nWant to study up before the headlines? Read @bertrandhb's second long read on CCS, explaining how and why IPCC models use so much saviour tech.\\n\\nhttps://t.co/6yBf0j7UWA\"                    \n [5] \"Live stream of virtual IPCC press conference releasing the report on mitigation of climate change, 9 a.m. GMT o... https://t.co/IqRCvvQxyX\"                                                                                                                                                      \n [6] \"Attention journalists: The deadline for embargoed materials for the upcoming @IPCC_CH report on climate mitigation has been extended to TODAY at 5:59 pm EDT. Register here: https://t.co/fLc4eHcOmm https://t.co/0eIlPb21kz\"                                                                    \n [7] \"The IPCC Report and “The Physics of Climate Change” https://t.co/xnxP3fup2a\"                                                                                                                                                                                                                     \n [8] \"With time running short and most of the Summary for Policymakers yet to be approved, #IPCC Working Group III added a fourth plenary to Thursday’s packed schedule in an attempt to make headway.\\n\\nMore ➡️ https://t.co/CRKNFzykYE\\n\\n#ClimateChange #AR6 #ClimateReport https://t.co/EoaasmOEZf\"\n [9] \"A helpful perspective on how to talk about the scenarios discussed in the forthcoming IPCC report https://t.co/Kpiim9NgNw\"                                                                                                                                                                       \n[10] \"The private sector is an integral component of the water cycle and has much to lose as critical climate and water risks grow. \\n\\nThis presents an opportunity for collective action, writes Kirsten James of the sustainability nonprofit @CeresNews.  \\n\\nhttps://t.co/pC3kiJ6R1t\"             \n\n#simple plot of tweets per day\ntweets %>%\n  count(date) %>%\n  ggplot(aes(x = date, y = n))+\n  geom_line()\n\n\n\n\n\n\n#let's clean up the URLs from the tweets\ntweets$text <- gsub(\"http[^[:space:]]*\", \"\",tweets$text)\ntweets$text <- str_to_lower(tweets$text)\n\n#load sentiment lexicons\nbing_sent <- get_sentiments('bing')\nnrc_sent <- get_sentiments('nrc')\n\n#tokenize tweets to individual words\nwords <- tweets %>%\n  select(id, date, text) %>%\n  unnest_tokens(output = word, input = text, token = \"words\") %>%\n  anti_join(stop_words, by = \"word\") %>%\n  left_join(bing_sent, by = \"word\") %>%\n  left_join(\n    tribble(\n      ~sentiment, ~sent_score,\n      \"positive\", 1,\n      \"negative\", -1),\n    by = \"sentiment\")\n\n\n\n\n\n#take average sentiment score by tweet\ntweets_sent <- tweets %>%\n  left_join(\n    words %>%\n      group_by(id) %>%\n      summarize(\n        sent_score = mean(sent_score, na.rm = T)),\n    by = \"id\")\n\nneutral <- length(which(tweets_sent$sent_score == 0))\npositive <- length(which(tweets_sent$sent_score > 0))\nnegative <- length(which(tweets_sent$sent_score < 0))\n\nSentiment <- c(\"Positive\",\"Neutral\",\"Negative\")\nCount <- c(positive,neutral,negative)\noutput <- data.frame(Sentiment,Count)\noutput$Sentiment<-factor(output$Sentiment,levels=Sentiment)\nggplot(output, aes(x=Sentiment,y=Count))+\n  geom_bar(stat = \"identity\", aes(fill = Sentiment))+\n  scale_fill_manual(\"legend\", values = c(\"Positive\" = \"green\", \"Neutral\" = \"black\", \"Negative\" = \"red\"))+\n  ggtitle(\"Barplot of Sentiment in IPCC tweets\")\n\n\n\n\n\n\n# tally sentiment score per day\ndaily_sent <- tweets_sent %>%\n  group_by(date) %>%\n  summarize(sent_score = mean(sent_score, na.rm = T))\n\ndaily_sent %>%\n  ggplot( aes(x = date, y = sent_score)) +\n  geom_line() +\n    labs(x = \"Date\",\n    y = \"Avg Sentiment Score\",\n    title = \"Daily Tweet Sentiment\",\n    subtitle = \"IPCC Tweets\")\n\n\n\n\nNow let’s try a new type of text visualization: the wordcloud.\n\n\nwords %>%\n   anti_join(stop_words) %>%\n   count(word) %>%\n   with(wordcloud(word, n, max.words = 100))\n\n\n\n\n\n\nwords %>%\ninner_join(get_sentiments(\"bing\")) %>%\ncount(word, sentiment, sort = TRUE) %>%\nacast(word ~ sentiment, value.var = \"n\", fill = 0) %>%\ncomparison.cloud(colors = c(\"gray20\", \"gray80\"),\n                   max.words = 100)\n\n\n\n\nThe quanteda package\nquanteda is a package (actually a family of packages) full of tools\nfor conducting text analysis. quanteda.sentiment (not yet on CRAN,\ndownload from github) is the quanteda modular package for conducting\nsentiment analysis.\nquanteda has its own built in functions for cleaning text data. Let’s\ntake a look at some. First we have to clean the messy tweet data:\n\n\ncorpus <- corpus(dat$Title) #enter quanteda\nsummary(corpus)\n\n\nCorpus consisting of 2411 documents, showing 100 documents:\n\n    Text Types Tokens Sentences\n   text1    43     53         2\n   text2    37     42         2\n   text3    31     32         2\n   text4    42     49         3\n   text5    21     25         2\n   text6    30     33         1\n   text7    10     12         1\n   text8    40     42         2\n   text9    16     17         1\n  text10    36     42         2\n  text11    16     16         1\n  text12    34     44         6\n  text13    35     46         3\n  text14    46     52         2\n  text15    42     51         1\n  text16     7      7         1\n  text17    42     48         2\n  text18    17     17         2\n  text19    43     60         1\n  text20    27     34         3\n  text21    40     43         3\n  text22    44     50         3\n  text23    28     30         2\n  text24    35     38         3\n  text25    36     41         3\n  text26    37     43         4\n  text27    21     23         1\n  text28    29     31         1\n  text29    12     13         1\n  text30    45     47         2\n  text31    38     42         1\n  text32    31     36         1\n  text33    14     14         1\n  text34    41     49         1\n  text35     7      7         1\n  text36    44     54         2\n  text37    26     28         1\n  text38    13     13         1\n  text39    13     13         1\n  text40    31     37         2\n  text41    47     54         4\n  text42    38     46         1\n  text43    42     46         2\n  text44    22     24         2\n  text45    38     46         1\n  text46    16     16         1\n  text47    30     32         1\n  text48    17     17         1\n  text49    13     13         1\n  text50    23     23         1\n  text51    23     25         1\n  text52    25     27         1\n  text53    13     13         1\n  text54    34     35         3\n  text55    38     46         1\n  text56    38     46         1\n  text57    38     46         1\n  text58    38     46         1\n  text59    38     46         1\n  text60    38     46         1\n  text61    19     19         2\n  text62    17     18         1\n  text63    11     11         1\n  text64    13     13         1\n  text65    14     16         1\n  text66    12     12         2\n  text67    18     18         1\n  text68    38     46         1\n  text69    15     16         1\n  text70    12     13         1\n  text71    30     35         2\n  text72    22     23         1\n  text73    38     46         1\n  text74    39     46         1\n  text75    13     13         1\n  text76    32     35         1\n  text77    38     46         1\n  text78    39     45         2\n  text79    38     46         1\n  text80    36     41         1\n  text81    33     33         2\n  text82    18     19         1\n  text83    38     46         1\n  text84    38     46         1\n  text85    38     46         1\n  text86    39     43         2\n  text87    13     13         1\n  text88    13     13         1\n  text89    38     46         1\n  text90    38     46         1\n  text91    38     46         1\n  text92    40     43         1\n  text93    11     11         1\n  text94    41     49         1\n  text95    38     46         1\n  text96    15     15         1\n  text97    29     31         1\n  text98    11     11         1\n  text99    13     13         1\n text100    38     46         1\n\n\n\ntokens <- tokens(corpus) #tokenize the text so each doc (page, in this case) is a list of tokens (words)\n\n#examine the uncleaned version\ntokens\n\n\nTokens consisting of 2,411 documents.\ntext1 :\n [1] \"thank\"       \"you\"         \",\"           \"followers\"  \n [5] \",\"           \"for\"         \"the\"         \"great\"      \n [9] \"photo\"       \"suggestions\" \"for\"         \"our\"        \n[ ... and 41 more ]\n\ntext2 :\n [1] \"Greenpeace\" \":\"          \"The\"        \"real\"       \"solution\"  \n [6] \"to\"         \"the\"        \"climate\"    \"crisis\"     \"will\"      \n[11] \"require\"    \"a\"         \n[ ... and 30 more ]\n\ntext3 :\n [1] \"Governments\"    \"have\"           \"a\"             \n [4] \"responsibility\" \"to\"             \"ensure\"        \n [7] \"that\"           \"#IPCCReport\"    \"is\"            \n[10] \"grounded\"       \"in\"             \"rapid\"         \n[ ... and 20 more ]\n\ntext4 :\n [1] \"Next\"      \"week\"      \",\"         \"the\"       \"IPCC\"     \n [6] \"will\"      \"publish\"   \"a\"         \"new\"       \"report\"   \n[11] \"detailing\" \"their\"    \n[ ... and 37 more ]\n\ntext5 :\n [1] \"Live\"       \"stream\"     \"of\"         \"virtual\"    \"IPCC\"      \n [6] \"press\"      \"conference\" \"releasing\"  \"the\"        \"report\"    \n[11] \"on\"         \"mitigation\"\n[ ... and 13 more ]\n\ntext6 :\n [1] \"Attention\"   \"journalists\" \":\"           \"The\"        \n [5] \"deadline\"    \"for\"         \"embargoed\"   \"materials\"  \n [9] \"for\"         \"the\"         \"upcoming\"    \"@IPCC_CH\"   \n[ ... and 21 more ]\n\n[ reached max_ndoc ... 2,405 more documents ]\n\n#clean it up\ntokens <- tokens(tokens, remove_punct = TRUE,\n                      remove_numbers = TRUE)\n\ntokens <- tokens_select(tokens, stopwords('english'),selection='remove') #stopwords lexicon built in to quanteda\n\n#tokens <- tokens_wordstem(tokens) #stem words down to their base form for comparisons across tense and quantity\n\ntokens <- tokens_tolower(tokens)\n\n\n\nWe can use the kwic function (keywords-in-context) to briefly examine\nthe context in which certain words or patterns appear.\n\n\nhead(kwic(tokens, pattern = \"climate\", window = 3))\n\n\nKeyword-in-context with 6 matches.                                                     \n   [text2, 4]    greenpeace real solution | climate |\n  [text2, 17]        upcoming#ipcc report | climate |\n  [text5, 10] releasing report mitigation | climate |\n   [text6, 9]     upcoming@ipcc_ch report | climate |\n   [text7, 4]         ipcc report physics | climate |\n [text10, 10]          much lose critical | climate |\n                               \n crisis require rapid          \n solutions set publication     \n change a.m gmt                \n mitigation extended today     \n change https://t.co/xnxp3fup2a\n water risks grow              \n\nhead(kwic(tokens, pattern = phrase(\"climate change\"), window = 3))\n\n\nKeyword-in-context with 6 matches.                                                               \n  [text5, 10:11] releasing report mitigation | climate change |\n    [text7, 4:5]         ipcc report physics | climate change |\n [text14, 15:16]         avert worst effects | climate change |\n [text15, 10:11]   s#climatereport emissions | climate change |\n   [text20, 1:2]                             | climate change |\n   [text24, 6:7]      report revealed threat | climate change |\n                                \n a.m gmt o                      \n https://t.co/xnxp3fup2a        \n anyone think revolution        \n meenakshi raman@sahabatalammsia\n want learn 100s                \n team weighed findings          \n\n\n\nhash_tweets <- tokens(corpus, remove_punct = TRUE) %>% \n               tokens_keep(pattern = \"#*\")\n\ndfm_hash<- dfm(hash_tweets)\n\ntstat_freq <- textstat_frequency(dfm_hash, n = 100)\nhead(tstat_freq, 10)\n\n\n             feature frequency rank docfreq group\n1              #ipcc       464    1     460   all\n2     #climatechange       137    2     135   all\n3     #climatecrisis       118    3     117   all\n4     #climatereport        97    4      97   all\n5        #ipccreport        87    5      87   all\n6           #climate        68    6      67   all\n7  #climateemergency        45    7      45   all\n8     #climateaction        44    8      44   all\n9     #globalwarming        24    9      24   all\n10 #climateactionnow        23   10      23   all\n\n#tidytext gives us tools to convert to tidy from non-tidy formats\nhash_tib<- tidy(dfm_hash)\n\nhash_tib %>%\n   count(term) %>%\n   with(wordcloud(term, n, max.words = 100))\n\n\n\n\nCreate the sparse matrix representation known as the document-feature\nmatrix. quanteda’s textstat_polarity function has multiple ways to\ncombine polarity to a single score. The sent_logit value to fun argument\nis the log of (pos/neg) counts.\n\n\ndfm <- dfm(tokens)\n\ntopfeatures(dfm, 12)\n\n\n   climate       ipcc     report     change        now      #ipcc \n      1396       1243       1225        651        505        464 \n     world  emissions      never        new     latest scientists \n       346        333        291        279        279        274 \n\ndfm.sentiment <- dfm_lookup(dfm, dictionary = data_dictionary_LSD2015)\n\nhead(textstat_polarity(tokens, data_dictionary_LSD2015, fun = sent_logit))\n\n\n  doc_id sentiment\n1  text1  2.197225\n2  text2 -1.098612\n3  text3  1.945910\n4  text4  0.000000\n5  text5  1.098612\n6  text6  1.098612\n\nAssignment\nYou will use the tweet data from class today for each part of the\nfollowing assignment.\nThink about how to further clean a twitter data set. Let’s assume\nthat the mentions of twitter accounts is not useful to us. Remove them\nfrom the text field of the tweets tibble.\nCompare the ten most common terms in the tweets per day. Do you\nnotice anything interesting?\nAdjust the wordcloud in the “wordcloud” chunk by coloring the\npositive and negative words so they are identifiable.\nLet’s say we are interested in the most prominent entities in the\nTwitter discussion. Which are the top 10 most tagged accounts in the\ndata set. Hint: the “explore_hashtags” chunk is a good starting\npoint.\nThe Twitter data download comes with a variable called\n“Sentiment” that must be calculated by Brandwatch. Use your own method\nto assign each tweet a polarity score (Positive, Negative, Neutral) and\ncompare your classification to Brandwatch’s (hint: you’ll need to\nrevisit the “raw_tweets” data frame).\n\n\n\n",
      "last_modified": "2022-04-29T14:37:30-07:00"
    },
    {
      "path": "topic_5.html",
      "title": "Topic 5: Word Relationships",
      "author": [],
      "contents": "\n\n\nlibrary(tidyr) #text analysis in R\nlibrary(pdftools)\nlibrary(lubridate) #working with date data\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(readr)\nlibrary(quanteda)\nlibrary(readtext) #quanteda subpackage for reading pdf\nlibrary(quanteda.textstats)\nlibrary(quanteda.textplots)\nlibrary(ggplot2)\nlibrary(forcats)\nlibrary(stringr)\nlibrary(quanteda.textplots)\nlibrary(widyr)# pairwise correlations\nlibrary(igraph) #network plots\nlibrary(ggraph)\n\n\n\n#import EPA EJ Data\n\n\nsetwd('dat/')\nfiles <- list.files(pattern = \"pdf$\")\n\nej_reports <- lapply(files, pdf_text)\n\nej_pdf <- readtext(\"*.pdf\", docvarsfrom = \"filenames\", \n                    docvarnames = c(\"type\", \"year\"),\n                    sep = \"_\")\n\n#creating an initial corpus containing our data\nepa_corp <- corpus(x = ej_pdf, text_field = \"text\" )\nsummary(epa_corp)\n\n\nCorpus consisting of 6 documents, showing 6 documents:\n\n           Text Types Tokens Sentences  type year\n EPAEJ_2015.pdf  2136   8944       263 EPAEJ 2015\n EPAEJ_2016.pdf  1599   7965       176 EPAEJ 2016\n EPAEJ_2017.pdf  2774  16658       447 EPAEJ 2017\n EPAEJ_2018.pdf  3973  30564       653 EPAEJ 2018\n EPAEJ_2019.pdf  3773  22648       672 EPAEJ 2019\n EPAEJ_2020.pdf  4493  30523       987 EPAEJ 2020\n\n#I'm adding some additional, context-specific stop words to stop word lexicon\nmore_stops <-c(\"2015\",\"2016\", \"2017\", \"2018\", \"2019\", \"2020\", \"www.epa.gov\", \"https\")\nadd_stops<- tibble(word = c(stop_words$word, more_stops)) \nstop_vec <- as_vector(add_stops)\n\n\n\nNow we’ll create some different data objects that will set us up for\nthe subsequent analyses\n\n\n#convert to tidy format and apply my stop words\nraw_text <- tidy(epa_corp)\n\n#Distribution of most frequent words across documents\nraw_words <- raw_text %>%\n  mutate(year = as.factor(year)) %>%\n  unnest_tokens(word, text) %>%\n  anti_join(add_stops, by = 'word') %>%\n  count(year, word, sort = TRUE)\n\n#number of total words by document  \ntotal_words <- raw_words %>% \n  group_by(year) %>% \n  summarize(total = sum(n))\n\nreport_words <- left_join(raw_words, total_words)\n \npar_tokens <- unnest_tokens(raw_text, output = paragraphs, input = text, token = \"paragraphs\")\n\npar_tokens <- par_tokens %>%\n mutate(par_id = 1:n())\n\npar_words <- unnest_tokens(par_tokens, output = word, input = paragraphs, token = \"words\")\n\n\n\nLet’s see which words tend to occur close together in the text. This\nis a way to leverage word relationships (in this case, co-occurence in a\nsingle paragraph) to give us some understanding of the things discussed\nin the documents.\n\n\nword_pairs <- par_words %>% \n  pairwise_count(word, par_id, sort = TRUE, upper = FALSE) %>%\n  anti_join(add_stops, by = c(\"item1\" = \"word\")) %>%\n  anti_join(add_stops, by = c(\"item2\" = \"word\"))\n\n\n\nNow we can visualize\n\n\nword_pairs %>%\n  filter(n >= 100) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = n, edge_width = n), edge_colour = \"dodgerblue\") +\n  geom_node_point(size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, \n                 point.padding = unit(0.2, \"lines\")) +\n  theme_void()\n\n\n\n\nHmm, interesting, but maybe we further subset the word pairs to get a\ncleaner picture of the most common ones by raising the cutoff for number\nof occurrences (n).\nPairs like “environmental” and “justice” are the most common\nco-occurring words, but that doesn’t give us the full picture asthey’re\nalso the most common individual words. We can also look at correlation\namong words, which tells us how often they appear together relative to\nhow often they appear separately.\n\n\nword_cors <- par_words %>% \n  add_count(par_id) %>% \n  filter(n >= 50) %>% \n  select(-n) %>%\n  pairwise_cor(word, par_id, sort = TRUE)\n\njust_cors <- word_cors %>% \n  filter(item1 == \"justice\")\n\n  word_cors %>%\n  filter(item1 %in% c(\"environmental\", \"justice\", \"equity\", \"income\"))%>%\n  group_by(item1) %>%\n  top_n(6) %>%\n  ungroup() %>%\n  mutate(item1 = as.factor(item1),\n  name = reorder_within(item2, correlation, item1)) %>%\n  ggplot(aes(y = name, x = correlation, fill = item1)) + \n  geom_col(show.legend = FALSE) +\n  facet_wrap(~item1, ncol = 2, scales = \"free\")+\n  scale_y_reordered() +\n  labs(y = NULL,\n         x = NULL,\n         title = \"Correlations with key words\",\n         subtitle = \"EPA EJ Reports\")\n\n\n\n  #let's zoom in on just one of our key terms\n   justice_cors <- word_cors %>% \n  filter(item1 == \"justice\") %>%\n   mutate(n = 1:n())\n\n\n\nNot surprisingly, the correlation between “environmental” and\n“justice” is by far the highest, which makes sense given the nature of\nthese reports. How might we visualize these correlations to develop of\nsense of the context in which justice is discussed here?\n\n\njustice_cors  %>%\n  filter(n <= 50) %>%\n  graph_from_data_frame() %>%\n  ggraph(layout = \"fr\") +\n  geom_edge_link(aes(edge_alpha = correlation, edge_width = correlation), edge_colour = \"cyan4\") +\n  geom_node_point(size = 5) +\n  geom_node_text(aes(label = name), repel = TRUE, \n                 point.padding = unit(0.2, \"lines\")) +\n  theme_void()\n\n\n\n\nNow let’s look at the tf-idf term we talked about. Remember, this\nstatistic goes beyond simple frequency calculations within a document to\ncontrol for overall commonality across documents\n\n\nreport_tf_idf <- report_words %>%\n  bind_tf_idf(word, year, n) %>%\n  select(-total) %>%\n  arrange(desc(tf_idf))\n\nreport_tf_idf %>%\n  group_by(year) %>%\n  slice_max(tf_idf, n = 10) %>%\n  ungroup() %>%\n  filter(nchar(word) > 2)%>%\n  ggplot(aes(tf_idf, fct_reorder(word, tf_idf), fill = year)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~year, ncol = 2, scales = \"free\") +\n  labs(x = \"tf-idf\", y = NULL)\n\n\n\n\nSo that gives an idea which words are frequent and unique to certain\ndocuments.\nNow let’s switch gears to quanteda for some additional word\nrelationship tools. We’ll also get into some ways to assess the\nsimilarity of documents.\n\n\ntokens <- tokens(epa_corp, remove_punct = TRUE)\ntoks1<- tokens_select(tokens, min_nchar = 3)\ntoks1 <- tokens_tolower(toks1)\ntoks1 <- tokens_remove(toks1, pattern = (stop_vec))\ndfm <- dfm(toks1)\n\n#first the basic frequency stat\ntstat_freq <- textstat_frequency(dfm, n = 5, groups = year)\nhead(tstat_freq, 10)\n\n\n         feature frequency rank docfreq group\n1  environmental       127    1       1  2015\n2    communities        99    2       1  2015\n3            epa        92    3       1  2015\n4        justice        84    4       1  2015\n5      community        47    5       1  2015\n6  environmental       109    1       1  2016\n7    communities        85    2       1  2016\n8        justice        71    3       1  2016\n9            epa        48    4       1  2016\n10       federal        31    5       1  2016\n\nAnother useful word relationship concept is that of the n-gram, which\nessentially means tokenizing at the multi-word level\n\n\ntoks2 <- tokens_ngrams(toks1, n=2)\ndfm2 <- dfm(toks2)\ndfm2 <- dfm_remove(dfm2, pattern = c(stop_vec))\nfreq_words2 <- textstat_frequency(dfm2, n=20)\nfreq_words2$token <- rep(\"bigram\", 20)\n#tokens1 <- tokens_select(tokens1,pattern = stopwords(\"en\"), selection = \"remove\")\n\n\n\nNow we can upgrade that by using all of the frequencies for each word\nin each document and calculating a chi-square to see which words occur\nsignificantly more or less within a particular target document\n\n\nkeyness <- textstat_keyness(dfm2, target = 2)\ntextplot_keyness(keyness)\n\n\n\n\nAnd finally, we can run a hierarchical clustering algorithm to assess\ndocument similarity. This tends to be more informative when you are\ndealing with a larger number of documents, but we’ll add it here for\nfuture reference.\n\n\ndist <- as.dist(textstat_dist(dfm))\nclust <- hclust(dist)\nplot(clust, xlab = \"Distance\", ylab = NULL)\n\n\n\n\nAssignment\nWhat are the most frequent trigrams in the dataset? How does this\ncompare to the most frequent bigrams? Which n-gram seems more\ninformative here, and why?\nChoose a new focal term to replace “justice” and recreate the\ncorrelation table and network (see corr_paragraphs and corr_network\nchunks). Explore some of the plotting parameters in the cor_network\nchunk to see if you can improve the clarity or amount of information\nyour plot conveys. Make sure to use a different color for the\nties!\nWrite a function that allows you to conduct a keyness analysis to\ncompare two individual EPA reports (hint: that means target and\nreference need to both be individual reports). Run the function on 3\npairs of reports, generating 3 keyness plots.\nSelect a word or multi-word term of interest and identify words\nrelated to it using windowing and keyness comparison. To do this you\nwill create to objects: one containing all words occurring within a\n10-word window of your term of interest, and the second object\ncontaining all other words. Then run a keyness comparison on these\nobjects. Which one is the target, and which the reference? Hint\n\n\n\n",
      "last_modified": "2022-04-29T14:38:44-07:00"
    },
    {
      "path": "topic_6.html",
      "title": "Topic 6: Topic Analysis",
      "author": [],
      "contents": "\n\n\nlibrary(here)\nlibrary(pdftools)\nlibrary(quanteda)\nlibrary(tm)\nlibrary(topicmodels)\nlibrary(ldatuning)\nlibrary(tidyverse)\nlibrary(tidytext)\nlibrary(reshape2)\n\n\n\nLoad the data\n\n\n##Topic 6 .Rmd here:https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/topic_6.Rmd\n#grab data here: \ncomments_df<-read_csv(\"https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/main/dat/comments_df.csv\")\n\n#comments_df <- read_csv(here(\"dat\", \"comments_df.csv\")) #if reading from local\n\n\n\nNow we’ll build and clean the corpus\n\n\nepa_corp <- corpus(x = comments_df, text_field = \"text\")\nepa_corp.stats <- summary(epa_corp)\nhead(epa_corp.stats, n = 25)\n\n\n     Text Types Tokens Sentences\n1   text1  1196   3973       178\n2   text2   830   2509       111\n3   text3   279    571        31\n4   text4  1745   6904       251\n5   text5   581   1534        49\n6   text6   469   1187        53\n7   text7   424    903        38\n8   text8  3622  22270       655\n9   text9   373    717        25\n10 text10   404    971        42\n11 text11   710   2190        77\n12 text12   636   1896        82\n13 text13   146    206         3\n14 text14  1124   3197        86\n15 text15   914   2943        90\n16 text16    13     45         1\n17 text17  1043   3190       103\n18 text18   313    601        24\n19 text19   152    229         6\n20 text20   341    786        35\n21 text21   211    403        15\n22 text22   186    322        12\n23 text23   211    398        14\n24 text24   325    696        33\n25 text25  1749   5382       115\n                                                Document\n1                                     1_Air Alliance.pdf\n2                                         10_Bus NEJ.pdf\n3                                   11_Carlton Ginny.pdf\n4                                    15_City Project.pdf\n5                                   16_Corporate EEC.pdf\n6                             17_Detriot Sierra Club.pdf\n7                                    18_District DOE.pdf\n8                                   19_Earth Justice.pdf\n9                                        2_Alex Kidd.pdf\n10                               20_Elizabeth Mooney.pdf\n11                                        21_Env COS.pdf\n12                                   22_Env Def Fund.pdf\n13                               23_Env Health Watch.pdf\n14 24_Env Justice Leadership Forum on Climate Change.pdf\n15                                25_Env Law at Duke.pdf\n16                                 26_Farm worker AF.pdf\n17                            27_Farm Worker Justice.pdf\n18                                 28_Faulker County.pdf\n19                                  29_First Peoples.pdf\n20                              3_Alliance for Metro.pdf\n21                                     30_Gage Blasi.pdf\n22                                      31_Gull Leon.pdf\n23                                  32_Hilary Kramer.pdf\n24                             33_Housing Land Advoc.pdf\n25                                   34_Human rights.pdf\n\ntoks <- tokens(epa_corp, remove_punct = TRUE, remove_numbers = TRUE)\n#I added some project-specific stop words here\nadd_stops <- c(stopwords(\"en\"),\"environmental\", \"justice\", \"ej\", \"epa\", \"public\", \"comment\")\ntoks1 <- tokens_select(toks, pattern = add_stops, selection = \"remove\")\n\n\n\nAnd now convert to a document-feature matrix\n\n\ndfm_comm<- dfm(toks1, tolower = TRUE)\ndfm <- dfm_wordstem(dfm_comm)\ndfm <- dfm_trim(dfm, min_docfreq = 2) #remove terms only appearing in one doc (min_termfreq = 10)\n\nprint(head(dfm))\n\n\nDocument-feature matrix of: 6 documents, 2,781 features (82.75% sparse) and 1 docvar.\n       features\ndocs    charl lee deputi associ assist administr usepa offic 2201-a\n  text1     1   2      1      1      6         6     1     7      1\n  text2     1   1      1      4      3         1     0     5      0\n  text3     0   0      0      0      1         0     0     2      0\n  text4     0   0      0      0      1         9     0     1      0\n  text5     4   5      1      1      1         1     0     1      1\n  text6     1   1      1      3      1         3     0     4      0\n       features\ndocs    pennsylvania\n  text1            1\n  text2            0\n  text3            0\n  text4            0\n  text5            1\n  text6            0\n[ reached max_nfeat ... 2,771 more features ]\n\n#remove rows (docs) with all zeros\nsel_idx <- slam::row_sums(dfm) > 0 \ndfm <- dfm[sel_idx, ]\n#comments_df <- dfm[sel_idx, ]\n\n\n\nWe somehow have to come up with a value for k,the number of latent\ntopics present in the data. How do we do this? There are multiple\nmethods. Let’s use what we already know about the data to inform a\nprediction. The EPA has 9 priority areas: Rulemaking, Permitting,\nCompliance and Enforcement, Science, States and Local Governments,\nFederal Agencies, Community-based Work, Tribes and Indigenous People,\nNational Measures. Maybe the comments correspond to those areas?\n\n\nk <- 9 \n\ntopicModel_k9 <- LDA(dfm, k, method=\"Gibbs\", control=list(iter = 500, verbose = 25))\n\n\nK = 9; V = 2781; M = 77\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\n#nTerms(dfm_comm) \n\ntmResult <- posterior(topicModel_k9)\nattributes(tmResult)\n\n\n$names\n[1] \"terms\"  \"topics\"\n\n#nTerms(dfm_comm)   \nbeta <- tmResult$terms   # get beta from results\ndim(beta)                # K distributions over nTerms(DTM) terms# lengthOfVocab\n\n\n[1]    9 2781\n\nterms(topicModel_k9, 10)\n\n\n      Topic 1     Topic 2   Topic 3   Topic 4      Topic 5  \n [1,] \"communiti\" \"state\"   \"prison\"  \"communiti\"  \"program\"\n [2,] \"enforc\"    \"permit\"  \"facil\"   \"water\"      \"feder\"  \n [3,] \"monitor\"   \"consid\"  \"peopl\"   \"comment\"    \"agenc\"  \n [4,] \"air\"       \"use\"     \"sourc\"   \"polici\"     \"issu\"   \n [5,] \"health\"    \"comment\" \"popul\"   \"clean\"      \"titl\"   \n [6,] \"includ\"    \"air\"     \"work\"    \"econom\"     \"state\"  \n [7,] \"comment\"   \"feder\"   \"sent\"    \"pollut\"     \"polici\" \n [8,] \"requir\"    \"organ\"   \"project\" \"new\"        \"vi\"     \n [9,] \"complianc\" \"grant\"   \"one\"     \"overburden\" \"will\"   \n[10,] \"action\"    \"meet\"    \"center\"  \"energi\"     \"act\"    \n      Topic 6     Topic 7     Topic 8     Topic 9 \n [1,] \"framework\" \"communiti\" \"communiti\" \"health\"\n [2,] \"draft\"     \"plan\"      \"impact\"    \"right\" \n [3,] \"action\"    \"local\"     \"pollut\"    \"peopl\" \n [4,] \"communiti\" \"govern\"    \"state\"     \"citi\"  \n [5,] \"effort\"    \"use\"       \"rule\"      \"park\"  \n [6,] \"agenc\"     \"juli\"      \"health\"    \"civil\" \n [7,] \"develop\"   \"comment\"   \"also\"      \"nation\"\n [8,] \"tool\"      \"particip\"  \"provid\"    \"law\"   \n [9,] \"state\"     \"agenda\"    \"air\"       \"includ\"\n[10,] \"epa\"       \"strategi\"  \"popul\"     \"see\"   \n\nSome of those topics seem related to the cross-cutting and additional\ntopics identified in the EPA’s response to the public comments:\n1. Title VI of the Civil Rights Act of 1964\n2.EJSCREEN\n3. climate change, climate adaptation and promoting greenhouse gas\nreductions co-benefits\n4. overburdened communities and other stakeholders to meaningfully,\neffectively, and transparently participate in aspects of EJ 2020, as\nwell as other agency processes\n5. utilize multiple Federal Advisory Committees to better obtain\noutside environmental justice perspectives\n6. environmental justice and area-specific training to EPA staff\n7. air quality issues in overburdened communities\nSo we could guess that there might be a 16 topics (9 priority + 7\nadditional). Or we could calculate some metrics from the data.\n\n\n#\nresult <- FindTopicsNumber(\n  dfm,\n  topics = seq(from = 2, to = 20, by = 1),\n  metrics = c(\"CaoJuan2009\",  \"Deveaud2014\"),\n  method = \"Gibbs\",\n  control = list(seed = 77),\n  verbose = TRUE\n)\n\n\nfit models... done.\ncalculate metrics:\n  CaoJuan2009... done.\n  Deveaud2014... done.\n\nFindTopicsNumber_plot(result)\n\n\n\nk <- 7\n\ntopicModel_k7 <- LDA(dfm, k, method=\"Gibbs\", control=list(iter = 500, verbose = 25))\n\n\nK = 7; V = 2781; M = 77\nSampling 500 iterations!\nIteration 25 ...\nIteration 50 ...\nIteration 75 ...\nIteration 100 ...\nIteration 125 ...\nIteration 150 ...\nIteration 175 ...\nIteration 200 ...\nIteration 225 ...\nIteration 250 ...\nIteration 275 ...\nIteration 300 ...\nIteration 325 ...\nIteration 350 ...\nIteration 375 ...\nIteration 400 ...\nIteration 425 ...\nIteration 450 ...\nIteration 475 ...\nIteration 500 ...\nGibbs sampling completed!\n\ntmResult <- posterior(topicModel_k7)\nterms(topicModel_k7, 10)\n\n\n      Topic 1     Topic 2     Topic 3   Topic 4     Topic 5    \n [1,] \"right\"     \"framework\" \"program\" \"communiti\" \"pollut\"   \n [2,] \"health\"    \"communiti\" \"feder\"   \"state\"     \"impact\"   \n [3,] \"civil\"     \"draft\"     \"polici\"  \"use\"       \"state\"    \n [4,] \"vi\"        \"action\"    \"requir\"  \"permit\"    \"communiti\"\n [5,] \"titl\"      \"agenc\"     \"state\"   \"comment\"   \"air\"      \n [6,] \"peopl\"     \"develop\"   \"will\"    \"like\"      \"rule\"     \n [7,] \"practic\"   \"effort\"    \"comment\" \"consid\"    \"also\"     \n [8,] \"communiti\" \"state\"     \"work\"    \"plan\"      \"health\"   \n [9,] \"act\"       \"agenda\"    \"follow\"  \"resourc\"   \"popul\"    \n[10,] \"nation\"    \"epa\"       \"regul\"   \"mani\"      \"provid\"   \n      Topic 6     Topic 7    \n [1,] \"prison\"    \"communiti\"\n [2,] \"water\"     \"enforc\"   \n [3,] \"communiti\" \"monitor\"  \n [4,] \"new\"       \"includ\"   \n [5,] \"center\"    \"comment\"  \n [6,] \"site\"      \"data\"     \n [7,] \"popul\"     \"air\"      \n [8,] \"can\"       \"permit\"   \n [9,] \"sourc\"     \"action\"   \n[10,] \"power\"     \"use\"      \n\ntheta <- tmResult$topics\nbeta <- tmResult$terms\nvocab <- (colnames(beta))\n\n\n\nThere are multiple proposed methods for how to measure the best k\nvalue. You can go down the\nrabbit hole here\n\n\ncomment_topics <- tidy(topicModel_k7, matrix = \"beta\")\n\ntop_terms <- comment_topics %>%\n  group_by(topic) %>%\n  top_n(10, beta) %>%\n  ungroup() %>%\n  arrange(topic, -beta)\n\ntop_terms\n\n\n# A tibble: 72 × 3\n   topic term        beta\n   <int> <chr>      <dbl>\n 1     1 right     0.0287\n 2     1 health    0.0252\n 3     1 civil     0.0211\n 4     1 vi        0.0196\n 5     1 titl      0.0156\n 6     1 peopl     0.0150\n 7     1 practic   0.0124\n 8     1 communiti 0.0117\n 9     1 act       0.0113\n10     1 nation    0.0110\n# … with 62 more rows\n\n\n\ntop_terms %>%\n  mutate(term = reorder(term, beta)) %>%\n  ggplot(aes(term, beta, fill = factor(topic))) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ topic, scales = \"free\") +\n  coord_flip()\n\n\n\n\nLet’s assign names to the topics so we know what we are working with.\nWe can name them by their top terms\n\n\ntop5termsPerTopic <- terms(topicModel_k7, 5)\ntopicNames <- apply(top5termsPerTopic, 2, paste, collapse=\" \")\n\n\n\nWe can explore the theta matrix, which contains the distribution of\neach topic over each document\n\n\nexampleIds <- c(1, 2, 3)\nN <- length(exampleIds)\n\n#lapply(epa_corp[exampleIds], as.character) #uncomment to view example text\n# get topic proportions form example documents\ntopicProportionExamples <- theta[exampleIds,]\ncolnames(topicProportionExamples) <- topicNames\nvizDataFrame <- melt(cbind(data.frame(topicProportionExamples), document=factor(1:N)), variable.name = \"topic\", id.vars = \"document\")  \nggplot(data = vizDataFrame, aes(topic, value, fill = document), ylab = \"proportion\") +\n  geom_bar(stat=\"identity\") +\n  theme(axis.text.x = element_text(angle = 90, hjust = 1)) +  \n  coord_flip() +\n  facet_wrap(~ document, ncol = N)\n\n\n\n\nHere’s a neat JSON-based model visualizer\n\n\nlibrary(LDAvis)\nlibrary(\"tsne\")\nsvd_tsne <- function(x) tsne(svd(x)$u)\njson <- createJSON(\n  phi = tmResult$terms, \n  theta = tmResult$topics, \n  doc.length = rowSums(dfm), \n  vocab = colnames(dfm), \n  term.frequency = colSums(dfm),\n  mds.method = svd_tsne,\n  plot.opts = list(xlab=\"\", ylab=\"\")\n)\nserVis(json)\n\n\n\nAssignment:\nEither:\ncontinue on with the analysis we started:\nRun three more models and select the overall best value for k (the\nnumber of topics) - include some justification for your selection:\ntheory, FindTopicsNumber() optimization metrics, interpretability,\nLDAvis\nOR\nuse the data you plan to use for your final project:\nPrepare the data so that it can be analyzed in the topicmodels\npackage\nRun three more models and select the overall best value for k (the\nnumber of topics) - include some justification for your selection:\ntheory, FindTopicsNumber() optimization metrics, interpretability,\nLDAvis\n\n\n\n",
      "last_modified": "2022-05-09T09:34:53-07:00"
    },
    {
      "path": "topic_7.html",
      "title": "Topic 7: Word Embeddings",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\nEDS 231\n\n\nHome\n\n\nTopics\n \n▾\n\n\nTopic 2: Text Data in R\nTopic 3: Sentiment Analysis I\nTopic 4: Sentiment Analysis II\nTopic 5: Word Relationships\nTopic 6: Topic Analysis\nTopic 7: Word Embeddings\n\n\nResources\nGroup Project\n☰\n\n\n  \n    \n      \n        \n        \n        \n      \n      \n    \n    \n      \n  Home\n\n\n  \n    Topics\n     \n    \n  \n  \n      Topic 2: Text Data in R\n    \n    \n      Topic 3: Sentiment Analysis I\n    \n    \n      Topic 4: Sentiment Analysis II\n    \n    \n      Topic 5: Word Relationships\n    \n    \n      Topic 6: Topic Analysis\n    \n    \n      Topic 7: Word Embeddings\n    \n  \n\n  Resources\n\n\n  Group Project\n\n      \n  \n\n\n\n\n\n\nTopic 7: Word Embeddings\n\n\n\n\nThis week’s Rmd file here: https://github.com/MaRo406/EDS_231-text-sentiment/blob/main/topic_7.Rmd\nToday we are using climbing incident data from this repo: https://github.com/ecaroom/climbing-accidents. Some\nanalysis (in Excel) on the data was written up into a Rock and Ice\nmagazine article.\nBut I’ve constructed our data set (link below) by pulling a few key\nvariables including the full text of each incident report.\nincidents_df<-read_csv(\"https://raw.githubusercontent.com/MaRo406/EDS_231-text-sentiment/825b159b6da4c7040ce8295b9eae2fbbe9991ffd/dat/climbing_report_text.csv\")\n## Rows: 2770 Columns: 4\n## ── Column specification ────────────────────────────────────────────────────────\n## Delimiter: \",\"\n## chr (3): ID, Accident Title, Text\n## dbl (1): Publication Year\n## \n## ℹ Use `spec()` to retrieve the full column specification for this data.\n## ℹ Specify the column types or set `show_col_types = FALSE` to quiet this message.\nFirst, let’s calculate the unigram probabilities, how often we see\neach word in this corpus.\nunigram_probs <- incidents_df %>%\n    unnest_tokens(word, Text) %>%\n    anti_join(stop_words, by = 'word') %>%\n    count(word, sort = TRUE) %>%\n    mutate(p = n / sum(n)) \nunigram_probs \n## # A tibble: 25,205 × 3\n##    word         n       p\n##    <chr>    <int>   <dbl>\n##  1 rope      5129 0.00922\n##  2 feet      5101 0.00917\n##  3 climbing  4755 0.00855\n##  4 route     4357 0.00783\n##  5 climbers  3611 0.00649\n##  6 climb     3209 0.00577\n##  7 fall      3168 0.00569\n##  8 climber   2964 0.00533\n##  9 rescue    2928 0.00526\n## 10 source    2867 0.00515\n## # … with 25,195 more rows\nNext, we need to know how often we find each word near each other\nword – the skipgram probabilities. This is where we use the sliding\nwindow.\nskipgrams <- incidents_df %>%\n    unnest_tokens(ngram, Text, token = \"ngrams\", n = 5) %>%\n    mutate(ngramID = row_number()) %>% \n    tidyr::unite(skipgramID, ID, ngramID) %>%\n    unnest_tokens(word, ngram) %>%\n    anti_join(stop_words, by = 'word')\n\nskipgrams\n## # A tibble: 2,737,146 × 4\n##    skipgramID `Accident Title`                            `Publication Y…` word \n##    <chr>      <chr>                                                  <dbl> <chr>\n##  1 1_1        Failure of Rappel Setup (Protection Pulled…             1990 colo…\n##  2 1_1        Failure of Rappel Setup (Protection Pulled…             1990 rocky\n##  3 1_1        Failure of Rappel Setup (Protection Pulled…             1990 moun…\n##  4 1_1        Failure of Rappel Setup (Protection Pulled…             1990 nati…\n##  5 1_1        Failure of Rappel Setup (Protection Pulled…             1990 park \n##  6 1_2        Failure of Rappel Setup (Protection Pulled…             1990 rocky\n##  7 1_2        Failure of Rappel Setup (Protection Pulled…             1990 moun…\n##  8 1_2        Failure of Rappel Setup (Protection Pulled…             1990 nati…\n##  9 1_2        Failure of Rappel Setup (Protection Pulled…             1990 park \n## 10 1_3        Failure of Rappel Setup (Protection Pulled…             1990 moun…\n## # … with 2,737,136 more rows\n#calculate probabilities\nskipgram_probs <- skipgrams %>%\n    pairwise_count(word, skipgramID, diag = TRUE, sort = TRUE) %>%\n    mutate(p = n / sum(n))\n## Warning: `distinct_()` was deprecated in dplyr 0.7.0.\n## Please use `distinct()` instead.\n## See vignette('programming') for more help\n## This warning is displayed once every 8 hours.\n## Call `lifecycle::last_lifecycle_warnings()` to see where this warning was generated.\nHaving all the skipgram windows lets us calculate how often words\ntogether occur within a window, relative to their total occurrences in\nthe data. We do this using the point-wise mutual information (PMI). It’s\nthe logarithm of the probability of finding two words together,\nnormalized for the probability of finding each of the words alone. PMI\ntells us which words occur together more often than expected based on\nhow often they occurred on their own.\n#normalize probabilities\nnormalized_prob <- skipgram_probs %>%\n    filter(n > 20) %>%\n    rename(word1 = item1, word2 = item2) %>%\n    left_join(unigram_probs %>%\n                  select(word1 = word, p1 = p),\n              by = \"word1\") %>%\n    left_join(unigram_probs %>%\n                  select(word2 = word, p2 = p),\n              by = \"word2\") %>%\n    mutate(p_together = p / p1 / p2)\n\n#Which words are most associated with \"rope\"?   \nnormalized_prob %>% \n    filter(word1 == \"rope\") %>%\n    arrange(-p_together)\n## # A tibble: 295 × 7\n##    word1 word2        n          p      p1        p2 p_together\n##    <chr> <chr>    <dbl>      <dbl>   <dbl>     <dbl>      <dbl>\n##  1 rope  rope     25494 0.00340    0.00922 0.00922         40.0\n##  2 rope  lengths    101 0.0000135  0.00922 0.0000575       25.4\n##  3 rope  skinny      24 0.00000320 0.00922 0.0000144       24.2\n##  4 rope  drag       211 0.0000281  0.00922 0.000138        22.1\n##  5 rope  taut        98 0.0000131  0.00922 0.0000701       20.2\n##  6 rope  coiled      60 0.00000800 0.00922 0.0000431       20.1\n##  7 rope  thicker     21 0.00000280 0.00922 0.0000162       18.8\n##  8 rope  trailing    68 0.00000907 0.00922 0.0000539       18.3\n##  9 rope  fed         48 0.00000640 0.00922 0.0000413       16.8\n## 10 rope  70m         31 0.00000414 0.00922 0.0000270       16.6\n## # … with 285 more rows\nNow we convert to a matrix so we can use matrix factorization and\nreduce the dimensionality of the data.\npmi_matrix <- normalized_prob %>%\n    mutate(pmi = log10(p_together)) %>%\n    cast_sparse(word1, word2, pmi)    \n \n#remove missing data\npmi_matrix@x[is.na(pmi_matrix@x)] <- 0\n#run SVD using irlba() which is good for sparse matrices\npmi_svd <- irlba(pmi_matrix, 100, maxit = 500) #Reducing to 100 dimensions\n#next we output the word vectors:\nword_vectors <- pmi_svd$u\nrownames(word_vectors) <- rownames(pmi_matrix)\nsearch_synonyms <- function(word_vectors, selected_vector) {\ndat <- word_vectors %*% selected_vector\n    \nsimilarities <- dat %>%\n        tibble(token = rownames(dat), similarity = dat[,1])\n\nsimilarities %>%\n       arrange(-similarity) %>%\n        select(c(2,3))\n}\nfall <- search_synonyms(word_vectors,word_vectors[\"fall\",])\nslip <- search_synonyms(word_vectors,word_vectors[\"slip\",])\nslip %>%\n    mutate(selected = \"slip\") %>%\n    bind_rows(fall %>%\n                  mutate(selected = \"fall\")) %>%\n    group_by(selected) %>%\n    top_n(15, similarity) %>%\n    ungroup %>%\n    mutate(token = reorder(token, similarity)) %>%\n    ggplot(aes(token, similarity, fill = selected)) +\n    geom_col(show.legend = FALSE) +\n    facet_wrap(~selected, scales = \"free\") +\n    coord_flip() +\n    theme(strip.text=element_text(hjust=0, size=12)) +\n    scale_y_continuous(expand = c(0,0)) +\n    labs(x = NULL, title = \"What word vectors are most similar to slip or fall?\")\n\nsnow_danger <- word_vectors[\"snow\",] + word_vectors[\"danger\",] \nsearch_synonyms(word_vectors, snow_danger)\n## # A tibble: 9,104 × 2\n##    token      similarity\n##    <chr>           <dbl>\n##  1 snow           0.396 \n##  2 avalanche      0.131 \n##  3 conditions     0.0918\n##  4 soft           0.0806\n##  5 wet            0.0783\n##  6 ice            0.0769\n##  7 icy            0.0735\n##  8 slope          0.0703\n##  9 fresh          0.0604\n## 10 blindness      0.0596\n## # … with 9,094 more rows\nno_snow_danger <- word_vectors[\"danger\",] - word_vectors[\"snow\",] \nsearch_synonyms(word_vectors, no_snow_danger)\n## # A tibble: 9,104 × 2\n##    token     similarity\n##    <chr>          <dbl>\n##  1 avalanche     0.0882\n##  2 danger        0.0547\n##  3 rockfall      0.0540\n##  4 gulch         0.0534\n##  5 class         0.0507\n##  6 hazard        0.0403\n##  7 hazards       0.0394\n##  8 occurred      0.0376\n##  9 potential     0.0373\n## 10 mph           0.0361\n## # … with 9,094 more rows\n\nAssignment\nDownload a set of pretrained vectors, GloVe, and explore them.\nGrab data here:\n\n\n\nRecreate the analyses in the last three chunks (find-synonyms,\nplot-synonyms, word-math) with the GloVe embeddings. How are they\ndifferent from the embeddings created from the climbing accident data?\nWhy do you think they are different?\nRun the classic word math equation, “king” - “man” = ?\nThink of three new word math equations. They can involve any\nwords you’d like, whatever catches your interest.\n\n\n\n\n\n\n\n\n\n\n  This website was made with distill by RStudio.\n\n\n\n\n\n\n\n\n\n\n\n// add bootstrap table styles to pandoc tables\nfunction bootstrapStylePandocTables() {\n  $('tr.odd').parent('tbody').parent('table').addClass('table table-condensed');\n}\n$(document).ready(function () {\n  bootstrapStylePandocTables();\n});\n\n\n\n$(document).ready(function () {\n  window.buildTabsets(\"TOC\");\n});\n\n$(document).ready(function () {\n  $('.tabset-dropdown > .nav-tabs > li').click(function () {\n    $(this).parent().toggleClass('nav-tabs-open');\n  });\n});\n\n  (function () {\n    var script = document.createElement(\"script\");\n    script.type = \"text/javascript\";\n    script.src  = \"https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-AMS-MML_HTMLorMML\";\n    document.getElementsByTagName(\"head\")[0].appendChild(script);\n  })();\n",
      "last_modified": "2022-05-11T08:54:11-07:00"
    },
    {
      "path": "topic_8.html",
      "title": "Topic 8: Dropdown list from a navigation bar item",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_8.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nIf you look at this template, you’ll notice that some navigation bar\nitems go directly to a single page, while the Modules item takes you to\na dropdown menu.\nTo create a dropdown menu of pages:\nCreate and save the pages as individual R Markdown documents as\ndescribed here.\nOpen the _site.yml file. In the navbar\nsection, create a menu with the pages as linked items using a structure\nlike this:\n   - text: \"Dropdown menu\"\n      menu:\n        - text: \"First dropdown item\"\n          href: item_1.html\n        - text: \"Second dropdown item\"\n          href: item_2.html\nThe example above would only work if the new pages were created as\nitem_1.Rmd and item_2.Rmd, so that when the\nsite is built the rendered item_1.html and\nitem_2.html files exist in the docs output\ndirectory.\n\n\n\n",
      "last_modified": "2022-04-29T14:38:48-07:00"
    },
    {
      "path": "topic_9.html",
      "title": "Topic 9: Changing site fonts",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_9.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nYou are welcome to use any fonts you want on your website. Here, only\nusing Google fonts is described (there are other methods for downloading\nfonts and adding, not included here).\nFonts are\nimported and specified in theme.css\nIn your Project in RStudio, open the theme.css file.\nNear the top, you’ll see some lines that look like this:\n/* Header font */\n@import url('https://fonts.googleapis.com/css2?family=Sanchez&display=swap');\n\n/* Body font */\n@import url('https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400&display=swap');\n\n/* Code font (Roboto Mono) */\n@import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@300;400&display=swap');\nThose are the import command to get 3 different Google fonts\n(Sanchez, Nunito Sans, and Roboto Mono). You can explore many different\nGoogle fonts here.\nUse different fonts\nFind a Google Font you\nlike.\nClick on the font. To the right of the font example text, you\nshould see an option to ‘+ Select this style’. Click the one(s) you want\nto select.\nThat will probably bring up a side menu. If you don’t\nsee that side menu, you can see your selected styles at any time by\nclicking on the top-right menu icon that is a grid with 3 squares and a\nplus sign - hovering reveals this is to ‘View your selected families’.\nIn the Use on the web section of the side menu that appears,\nselect the radio button for @import. It’ll look weird like this\n(for the Zen Dots Google Font):\n    <style>\n    @import url('https://fonts.googleapis.com/css2?family=Zen+Dots&display=swap');\n    <\/style> \nCopy everything BETWEEN (but excluding) the ending\n<style> and <\/style>tags\nPaste the @import line you’ve copied into the top\nsection of theme.css near the other fonts imported there.\nIt is now available for use in your theme.\nUpdate the css with your new fonts, replacing the existing font\nnames with the name you’ve imported. You might want to use a Find &\nReplace All if you want to make sure you’re updating a font everywhere\nit appears in the current theme.\nRepeat for as many different fonts as you want to update in your\ntheme.\n\n\n\n",
      "last_modified": "2022-04-29T14:38:49-07:00"
    }
  ],
  "collections": []
}
