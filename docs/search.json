{
  "articles": [
    {
      "path": "assignments.html",
      "title": "Assignments",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the assignments.Rmd\nfile, in the project root, to delete this placeholder text and customize\nwith your own!\n\nAssignment materials and\ndates\nAssignment materials\nAssigned\nDue date\nA link to the repo\n2021-01-01\n2021-01-08\nA link to the repo\n2021-01-15\n2021-01-23\nA link to the repo\n2021-02-01\n2021-02-09\nA link to the repo\n2021-02-14\n2021-02-23\nAssignment expectations and\ngrading\nThis might also go on the home page & in syllabus\nOr could reinforce here\n\n\n\n\n",
      "last_modified": "2022-04-06T08:48:49-07:00"
    },
    {
      "path": "index.html",
      "title": "EDS 231: Text and Sentiment Analysis for Environmental Problems",
      "description": "Master’s of Environmental Data Science Program, UC Santa Barbara",
      "author": [],
      "contents": "\n\n\n\nCourse description\nThis course will cover foundations and applications of natural\nlanguage processing. Problem sets and class projects will leverage\ncommon and emerging text-based data sources relevant to environmental\nproblems, including but not limited to social media feeds (e.g.,\nTwitter) and text documents (e.g., agency reports), and will build\ncapacity and experience in common tools, including text processing and\nclassification, semantics, and natural language parsing.\nInstructor\nMateo Robbins (mateorobbins@gmail.com)\nOffice: MEDS Group Office\nOffice hours: Wednesday 11:00 - 12:00\nImportant links\nLink\nto full course syllabus\nWeekly course schedule\nLectures: W 9:30 - 10:45am (NCEAS Classroom)\nLearning objectives\nThe goal of EDS 231 (Text and Sentiment Analysis for Environmental\nProblems) is to expose students to a range of text analysis and natural\nlanguage processing data sources, techniques and tools for analysis that\ncan be applied to environmental problems. During this course, students\nwill:\nBecome familiar with the R packages used in text-as-data\napplications\nConduct and explain each step in the text data collection,\nanalysis, and presentation pipeline\nEvaluate examples of text analysis in the environmental science\nliterature\nWork with peers on a group text analysis project, then\ncommunicate the analysis to the rest of the class\nCourse requirements\nComputing\nMinimum MEDS device requirements (bring to all sessions +\ncharger!)\nUp-to-date R and RStudio\nPython version 3.x installed (although most, if not all, work\nwill be done in RStudio)\nTextbook\nReadings will be drawn from free online ebooks and the academic\nliterature\nTentative topics\nWeek | Lecture | Reading | Assignment\n||———–|—————————|———————–|———–|\n1 (3/30) | Course\nIntro and Text Analysis Overview | n/a | n/a |\n2 (4/06) | Text\nData in R | RDS (14.1 -\n14.3.1), TMR\nCh. 1 | Topic\n2 |\n3 (4/13) | Sentiment analysis I | link | TBA |\n4 (4/20) | Sentiment analysis II | link | TBA |\n5 (4/27) | Word relationship analysis | link | TBA |\n6 (5/4) | Topic analysis | link | TBA |\n7 (5/11) | Word Embeddings | link | TBA |\n8 (5/18) | Classification | link | TBA |\n9 (5/25) | TBD | link | TBA |\n10 (6/01) | Group Presentations | link | TBA |\n\n\n\n",
      "last_modified": "2022-04-06T08:48:50-07:00"
    },
    {
      "path": "Reading_Reactions.html",
      "title": "Reading Responses",
      "author": [],
      "contents": "\nReading Response Assignments\nPrompts\nA key part of this course is exploring the way the text analysis\nconcepts and tools that we’ll be learning about are being put to work in\nthe field. Each week there will be an assigned paper from the academic\nliterature. Your job is to read the paper and respond to the following\nprompts:\nMotivation - What is the main question the\nauthors are seeking to answer using text analysis?\nMethods - Describe the data and text analysis\nmethods used.\nResults - What did the authors discover through\ntheir analysis?\nCritique - What are the limitations of the\nauthors approach? What could they have improved? Do any unanswered\nquestions come to mind that further analysis could elucidate?\nEveryone will turn in their responses to these prompts by the end of\nthe day on Tuesday on GauchoSpace via online text submission.\nLead Group\nYou have been randomly assigned groups that will serve as both\nreading response teams as well as final presentation groups (more on\nthat next week). When it is your group’s week, you will coordinate as a\ngroup to present your answers to the 4 prompts above and facilitate a\ndiscussion with the rest of the class.\nWeekly Schedule\nWeek\nLead Group\n3\nA\n4\nB\n5\nC\n6\nD\n7\nE\n8\nF\nGroup Composition\nGroup\nMembers\nA\nPaloma, Allie, Wylie, Ben, Julia\nB\nSteven, Joe, Connor, Shale, Grace\nC\nAlex, Halina, Desik, Alexandra\nD\nJuliet, Charles, Scout, Peter\nE\nClarissa, Jake, Daniel, Ryan, Ilia\nF\nFelicia, Mia, Cullen, Marie\n\n\n\n",
      "last_modified": "2022-04-06T08:48:51-07:00"
    },
    {
      "path": "resources.html",
      "title": "Course resources",
      "author": [],
      "contents": "\n\nData sources\nUCSB Library\nAll\nlibrary text mining resources\nScholarly\nJournals\nTwitter\ndata\nKaggle\n#NLP\nAPIs\nNew York Times\nGoogle\nGoogle\nTrends\n\n\n\n",
      "last_modified": "2022-04-06T08:48:51-07:00"
    },
    {
      "path": "topic_1.html",
      "title": "Topic 1: Add/remove a site page",
      "author": [],
      "contents": "\nNOTE: There are 10 toy Topic sections here,\nexpecting that some teachers may want to have one page per week (for a\n10 week course). You are encouraged to structure your course\nhowever works best for your class. All of your course\ninformation could be on a single page, or you might have a different\nnumber of topics, or organize weekly, or any other organization that\nworks for you.\n\nTO UPDATE THIS PAGE: Open and edit the topic_1.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nAdd a site page\nThe quick version:\nMake a new R Markdown document, save\nAdd the rendered .html to _site.yaml so the page exists on the\nsite\nBuild to see updated site\nBelow for a bit more detail…\nMake each page an R\nMarkdown document\nTo make a new page:\nWithin your website Project, create a new .Rmd (File > New\nFile > R Markdown). Save it to the project root. For this example,\nlet’s say you’ve saved it as new_page.Rmd.\nIn that .Rmd file, remove everything but the title (which you can\nchange) from the YAML - that’s the top section of the .Rmd, where by\ndefault it has title, author, date, etc.\nUpdate the .Rmd to contain whatever you want to have on that\npage. Don’t know a lot about markdown? Considering switching over to the\nVisual\nEditor in RStudio (versions >= 1.4).\nSave the .Rmd\nAdd it to your navigation\nbar\nOpen the _site.yml file in your Project\nAdd the information to the YAML navbar section, which will almost\nalways be the text that you want to appear in the navigation bar, and\nthe file name of the knitted html that will be\nautomatically rendered to /docs when you Build your website. That would\nbe new_page.html for this example (since the .Rmd it is\nrendered from is new_page.Rmd). So in the\n_site.yml I would need to add this to the navbar\nsection:\n    - text: \"A new page!\"\n      href: new_page.html\nNote: YAML is space & indentation specific.\nFollow the structure that already exists in this template to avoid YAML\nerrors.\nHow is the website finding the html? Notice in the\n_site.yml file, the output_dir is set to\ndocs. That means when we press ‘Build website’ (in the\nBuild tab in RStudio), our .Rmd pages are knitted to HTML & sent to\nthe _docs folder. This is also important because when we\ndeploy the site (make it live), we will want to deploy from that\ndocs folder using GitHub pages.\nTake a look at some other pages in this template (Resources,\nAssignments, etc.) to see the structure, & give it a shot!\nDelete/disappear a site page\nThe safest thing to do if you don’t want a page to show up\nis to remove it from the _site.yml navbar listings. That\nway, the material on the page still exists as a file in your project,\nbut doesn’t show up on the website – don’t delete a page file unless you\nare REALLY SURE that you’re never going to want the material on that\npage ever again.\n\n\n\n",
      "last_modified": "2022-04-06T08:48:52-07:00"
    },
    {
      "path": "topic_10.html",
      "title": "Topic 10: Inserting tables",
      "author": [],
      "contents": "\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n",
      "last_modified": "2022-04-06T08:48:52-07:00"
    },
    {
      "path": "topic_2_copy.html",
      "title": "topic_2_copy",
      "author": [],
      "contents": "\nToday we will be grabbing some data from the New York Times database\nvia their API, then running some basic string manipulations, trying out\nthe tidytext format, and creating some basic plots.\nConnect to\nthe New York Times API and send a query\n\n\nlibrary(jsonlite) #convert results from API queries into R-friendly formats \nlibrary(tidyverse) \nlibrary(tidytext) #text data management and analysis\nlibrary(ggplot2) #plot word frequencies and publication dates\n\n\n\nWe have to decide which New York Times articles we are interested in\nexamining. For this exercise, I chose articles about Deb Haaland, the\ncurrent US Secretary of the Interior. As a member of the Laguna Pueblo\nTribe, Haaland is the first Native American to serve as Cabinet\nsecretary. Very cool!\n\n\n\n#create an object called x with the results of our query (\"haaland\")\n# the from JSON flatten the JSON object, then convert to a data frame\nt <- fromJSON(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", flatten = TRUE) #the string following \"key=\" is your API key \n\nclass(t) #what type of object is x?\n\nt <- t %>% \n  data.frame()\n\n\n#Inspect our data\nclass(t) #now what is it?\ndim(t) # how big is it?\nnames(t) # what variables are we working with?\n#t <- readRDS(\"nytDat.rds\") #in case of emergency :)\n\n\n\nThe name format, response.xxx.xxx…, is a legacy of the json nested\nhierarchy.\nLet’s look at a piece of text. Our data object has a variable called\n“response.docs.snippet” that contains a short excerpt, or “snippet” from\nthe article. Let’s grab a snippet and try out some basic ‘stringr’\nfunctions.\n\n\nt$response.docs.snippet[9]\n\n#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.\n\nx <- \"Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance.\" \n\ntolower(x)\nstr_split(x, ','); str_split(x, 't')\nstr_replace(x, 'historic', 'without precedent')\nstr_replace(x, ' ', '_') #first one\nstr_replace_all(x, ' ', '_') #all of them\nstr_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F\nstr_locate(x, 't'); str_locate_all(x, 'as')\n\n\n\nOK,\nit’s working but we want more data. Let’s set some parameters for a\nbigger query\n\n\nterm <- \"Haaland\" # Need to use + to string together separate words\nbegin_date <- \"20210120\"\nend_date <- \"20220401\"\n\n#construct the query url using API operators\nbaseurl <- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term,\n                  \"&begin_date=\",begin_date,\"&end_date=\",end_date,\n                  \"&facet_filter=true&api-key=\",\"NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", sep=\"\")\n\n#examine our query url\nbaseurl\n\n\n[1] \"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=Haaland&begin_date=20210120&end_date=20220401&facet_filter=true&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\"\n\n\n\n initialQuery <- fromJSON(baseurl)\nmaxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) \n\npages <- list()\nfor(i in 0:maxPages){\n  nytSearch <- fromJSON(paste0(baseurl, \"&page=\", i), flatten = TRUE) %>% data.frame() \n  message(\"Retrieving page \", i)\n  pages[[i+1]] <- nytSearch \n  Sys.sleep(1) \n}\nclass(nytSearch)\n\nnytDat <- rbind_pages(pages)\nnytDat <- as_tibble(nytDat)\n\n\n\n\n\nnytDat <- read.csv(\"nytDat.csv\")\n\nnytDat %>% \n  group_by(response.docs.type_of_material) %>%\n  summarize(count=n()) %>%\n  mutate(percent = (count / sum(count))*100) %>%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = \"identity\") + coord_flip()\n\n\n\n\n\n\nnytDat %>%\n  mutate(pubDay=gsub(\"T.*\",\"\",response.docs.pub_date)) %>%\n  group_by(pubDay) %>%\n  summarise(count=n()) %>%\n  filter(count >= 2) %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(pubDay, count), y=count), stat=\"identity\") + coord_flip()\n\n\n\n\nThe New York Times doesn’t make full text of the articles available\nthrough the API. But we can use the first paragraph of each article.\n\n\nnames(nytDat)\n\n\n [1] \"status\"                               \n [2] \"copyright\"                            \n [3] \"response.docs.abstract\"               \n [4] \"response.docs.web_url\"                \n [5] \"response.docs.snippet\"                \n [6] \"response.docs.lead_paragraph\"         \n [7] \"response.docs.source\"                 \n [8] \"response.docs.multimedia\"             \n [9] \"response.docs.keywords\"               \n[10] \"response.docs.pub_date\"               \n[11] \"response.docs.document_type\"          \n[12] \"response.docs.news_desk\"              \n[13] \"response.docs.section_name\"           \n[14] \"response.docs.type_of_material\"       \n[15] \"response.docs._id\"                    \n[16] \"response.docs.word_count\"             \n[17] \"response.docs.uri\"                    \n[18] \"response.docs.print_section\"          \n[19] \"response.docs.print_page\"             \n[20] \"response.docs.subsection_name\"        \n[21] \"response.docs.headline.main\"          \n[22] \"response.docs.headline.kicker\"        \n[23] \"response.docs.headline.content_kicker\"\n[24] \"response.docs.headline.print_headline\"\n[25] \"response.docs.headline.name\"          \n[26] \"response.docs.headline.seo\"           \n[27] \"response.docs.headline.sub\"           \n[28] \"response.docs.byline.original\"        \n[29] \"response.docs.byline.person\"          \n[30] \"response.docs.byline.organization\"    \n[31] \"response.meta.hits\"                   \n[32] \"response.meta.offset\"                 \n[33] \"response.meta.time\"                   \n\nparagraph <- names(nytDat)[6] #The 6th column, \"response.doc.lead_paragraph\", is the one we want here.  \ntokenized <- nytDat %>%\n  unnest_tokens(word, paragraph)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>% #illegible with all the words displayed\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nUh oh, who knows what we need to do here?\n\n\ndata(stop_words)\n\ntokenized <- tokenized %>%\n  anti_join(stop_words)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nOK, but look at the most common words. Does one stick out?\n\n\n#inspect the list of tokens (words)\ntokenized$word\n\nclean_tokens <- str_replace_all(tokenized$word,\"land[a-z,A-Z]*\",\"land\") #stem tribe words\nclean_tokens <- str_remove_all(clean_tokens, \"[:digit:]\") #remove all numbers\nclean_tokens <- str_remove_all(clean_tokens, \"washington\")\nclean_tokens <- gsub(\"’s\", '', clean_tokens)\n\ntokenized$clean <- clean_tokens\n\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n#remove the empty strings\ntib <-subset(tokenized, clean!=\"\")\n\n#reassign\ntokenized <- tib\n\n#try again\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\nAssignment (Due by Week 3)\nCreate a free New York Times account (https://developer.nytimes.com/get-started)\nPick an interesting environmental key word(s) and use the\njsonlite package to query the API. Pick something high profile enough\nand over a large enough time frame that your query yields enough\narticles for an interesting examination.\nRecreate the publications per day and word frequency plots using\nthe first paragraph\nMake some (at least 3) transformations to the corpus (add\nstopword(s), stem a key term and its variants, remove numbers)\nRecreate the publications per day and word frequency plots using the\nheadlines variable (response.docs.headline.main). Compare the\ndistributions of word frequencies between the first paragraph and\nheadlines. Do you see any difference?\n\n\n\n",
      "last_modified": "2022-04-06T08:49:04-07:00"
    },
    {
      "path": "topic_2.html",
      "title": "Topic 2: Text Data in R",
      "author": [],
      "contents": "\nToday we will be grabbing some data from the New York Times database\nvia their API, then running some basic string manipulations, trying out\nthe tidytext format, and creating some basic plots.\nConnect to\nthe New York Times API and send a query\n\n\nlibrary(jsonlite) #convert results from API queries into R-friendly formats \nlibrary(tidyverse) \nlibrary(tidytext) #text data management and analysis\nlibrary(ggplot2) #plot word frequencies and publication dates\n\n\n\nWe have to decide which New York Times articles we are interested in\nexamining. For this exercise, I chose articles about Deb Haaland, the\ncurrent US Secretary of the Interior. As a member of the Laguna Pueblo\nTribe, Haaland is the first Native American to serve as Cabinet\nsecretary. Very cool!\n\n\n\n#create an object called x with the results of our query (\"haaland\")\n# the from JSON flatten the JSON object, then convert to a data frame\nt <- fromJSON(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=haaland&api-key=NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", flatten = TRUE) #the string following \"key=\" is your API key \n\nclass(t) #what type of object is x?\n\nt <- t %>% \n  data.frame()\n\n\n#Inspect our data\nclass(t) #now what is it?\ndim(t) # how big is it?\nnames(t) # what variables are we working with?\n#t <- readRDS(\"nytDat.rds\") #in case of API emergency :)\n\n\n\nThe name format, response.xxx.xxx…, is a legacy of the json nested\nhierarchy.\nLet’s look at a piece of text. Our data object has a variable called\n“response.docs.snippet” that contains a short excerpt, or “snippet” from\nthe article. Let’s grab a snippet and try out some basic ‘stringr’\nfunctions.\n\n\nt$response.docs.snippet[9]\n\n#assign a snippet to x to use as fodder for stringr functions.  You can follow along using the sentence on the next line.\n\nx <- \"Her nomination as secretary of the interior is historic, but as the first Native cabinet member, she would have to strike a delicate balance.\" \n\ntolower(x)\nstr_split(x, ','); str_split(x, 't')\nstr_replace(x, 'historic', 'without precedent')\nstr_replace(x, ' ', '_') #first one\n#how do we replace all of them?\n\nstr_detect(x, 't'); str_detect(x, 'tive') ### is pattern in the string? T/F\nstr_locate(x, 't'); str_locate_all(x, 'as')\n\n\n\nOK,\nit’s working but we want more data. Let’s set some parameters for a\nbigger query\n\n\nterm <- \"Haaland\" # Need to use + to string together separate words\nbegin_date <- \"20210120\"\nend_date <- \"20220401\"\n\n#construct the query url using API operators\nbaseurl <- paste0(\"http://api.nytimes.com/svc/search/v2/articlesearch.json?q=\",term,\n                  \"&begin_date=\",begin_date,\"&end_date=\",end_date,\n                  \"&facet_filter=true&api-key=\",\"NTKBHbsb6XFEkGymGumAiba7n3uBvs8V\", sep=\"\")\n\n#examine our query url\n\n\n\n\n\n#this code allows for obtaining multiple pages of query results \n initialQuery <- fromJSON(baseurl)\nmaxPages <- round((initialQuery$response$meta$hits[1] / 10)-1) \n\npages <- list()\nfor(i in 0:maxPages){\n  nytSearch <- fromJSON(paste0(baseurl, \"&page=\", i), flatten = TRUE) %>% data.frame() \n  message(\"Retrieving page \", i)\n  pages[[i+1]] <- nytSearch \n  Sys.sleep(1) \n}\nclass(nytSearch)\n\n#need to bind the pages and create a tibble from nytDa\n\n\n\n\n\nnytDat <- read.csv(\"nytDat.csv\") # obtained from \n\nnytDat %>% \n  group_by(response.docs.type_of_material) %>%\n  summarize(count=n()) %>%\n  mutate(percent = (count / sum(count))*100) %>%\n  ggplot() +\n  geom_bar(aes(y=percent, x=response.docs.type_of_material, fill=response.docs.type_of_material), stat = \"identity\") + coord_flip()\n\n\n\n\n\n\nnytDat %>%\n  mutate(pubDay=gsub(\"T.*\",\"\",response.docs.pub_date)) %>%\n  group_by(pubDay) %>%\n  summarise(count=n()) %>%\n  filter(count >= 2) %>%\n  ggplot() +\n  geom_bar(aes(x=reorder(pubDay, count), y=count), stat=\"identity\") + coord_flip()\n\n\n\n\nThe New York Times doesn’t make full text of the articles available\nthrough the API. But we can use the first paragraph of each article.\n\n\nnames(nytDat)\n\n\n [1] \"status\"                               \n [2] \"copyright\"                            \n [3] \"response.docs.abstract\"               \n [4] \"response.docs.web_url\"                \n [5] \"response.docs.snippet\"                \n [6] \"response.docs.lead_paragraph\"         \n [7] \"response.docs.source\"                 \n [8] \"response.docs.multimedia\"             \n [9] \"response.docs.keywords\"               \n[10] \"response.docs.pub_date\"               \n[11] \"response.docs.document_type\"          \n[12] \"response.docs.news_desk\"              \n[13] \"response.docs.section_name\"           \n[14] \"response.docs.type_of_material\"       \n[15] \"response.docs._id\"                    \n[16] \"response.docs.word_count\"             \n[17] \"response.docs.uri\"                    \n[18] \"response.docs.print_section\"          \n[19] \"response.docs.print_page\"             \n[20] \"response.docs.subsection_name\"        \n[21] \"response.docs.headline.main\"          \n[22] \"response.docs.headline.kicker\"        \n[23] \"response.docs.headline.content_kicker\"\n[24] \"response.docs.headline.print_headline\"\n[25] \"response.docs.headline.name\"          \n[26] \"response.docs.headline.seo\"           \n[27] \"response.docs.headline.sub\"           \n[28] \"response.docs.byline.original\"        \n[29] \"response.docs.byline.person\"          \n[30] \"response.docs.byline.organization\"    \n[31] \"response.meta.hits\"                   \n[32] \"response.meta.offset\"                 \n[33] \"response.meta.time\"                   \n\nparagraph <- names(nytDat)[6] #The 6th column, \"response.doc.lead_paragraph\", is the one we want here.  \ntokenized <- nytDat %>%\n  unnest_tokens(word, paragraph)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>% #illegible with all the words displayed\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nUh oh, who knows what we need to do here?\n\n\ndata(stop_words)\n\ntokenized <- tokenized %>%\n  anti_join(stop_words)\n\ntokenized %>%\n  count(word, sort = TRUE) %>%\n  filter(n > 5) %>%\n  mutate(word = reorder(word, n)) %>%\n  ggplot(aes(n, word)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\n\nOK, but look at the most common words. Does one stick out?\n\n\n#inspect the list of tokens (words)\ntokenized$word\n\nclean_tokens <- str_replace_all(tokenized$word,\"land[a-z,A-Z]*\",\"land\") #stem tribe words\nclean_tokens <- str_remove_all(clean_tokens, \"[:digit:]\") #remove all numbers\nclean_tokens <- str_remove_all(clean_tokens, \"washington\")\nclean_tokens <- gsub(\"’s\", '', clean_tokens)\n\ntokenized$clean <- clean_tokens\n\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n#remove the empty strings\ntib <-subset(tokenized, clean!=\"\")\n\n#reassign\ntokenized <- tib\n\n#try again\ntokenized %>%\n  count(clean, sort = TRUE) %>%\n  filter(n > 10) %>% #illegible with all the words displayed\n  mutate(clean = reorder(clean, n)) %>%\n  ggplot(aes(n, clean)) +\n  geom_col() +\n  labs(y = NULL)\n\n\n\nAssignment (Due by Week 3)\nCreate a free New York Times account (https://developer.nytimes.com/get-started)\nPick an interesting environmental key word(s) and use the\njsonlite package to query the API. Pick something high profile enough\nand over a large enough time frame that your query yields enough\narticles for an interesting examination.\nRecreate the publications per day and word frequency plots using\nthe first paragraph\nMake some (at least 3) transformations to the corpus (add\nstopword(s), stem a key term and its variants, remove numbers)\nRecreate the publications per day and word frequency plots using the\nheadlines variable (response.docs.headline.main). Compare the\ndistributions of word frequencies between the first paragraph and\nheadlines. Do you see any difference?\n\n\n\n",
      "last_modified": "2022-04-06T08:49:09-07:00"
    },
    {
      "path": "topic_3.html",
      "title": "Topic 3: Adding images",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_3.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nWhere should I save my\nimages?\nDrop images you want to include on your site into the existing\nimg folder within your R Project (or make your own folder\nfor images, set up a subfolder structure, etc.) - as long as you can\neasily point to them within your project.\nHow do I make images\nshow up on my site?\nThere are a bunch of ways to add images in an R Markdown document\n(see more examples from the R Markdown Cookbook here).\nI recommend using\nknitr::include_graphics(\"path_to_image\").\nFor example, if there is an image saved as cool_dogs.jpg\nin the img folder, then you can add this to your R Markdown\ndocument by adding (within a code chunk):\n\n```{r, echo = FALSE}\n  knitr::include_graphics(\"img/cool_dogs.jpg\")\n```\nYou can also update the size of the image, add a caption, etc. by\nupdating the code chunk header, e.g.:\n\n```{r, echo = FALSE, out.width = \"50%\", fig.cap = \"Some cool dogs!\"}\n  knitr::include_graphics(\"img/cool_dogs.jpg\")\n```\nFor example, the image usgs_sat.jpeg is in the\nimg folder of this template. I can have it show up here by\ninclude a code chunk that looks like this:\n\n```{r, echo = FALSE, out.width = \"80%\", fig.cap = \"USGS image of Mississippi River.\"}\n  knitr::include_graphics(\"img/usgs_sat.jpeg\")\n```\nWhich creates:\n\n\n\nFigure 1: USGS image of Mississippi River.\n\n\n\n\n\n\n",
      "last_modified": "2022-04-06T08:49:09-07:00"
    },
    {
      "path": "topic_4.html",
      "title": "Topic 4: Change the theme",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_4.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nThe bells and whistles of this MEDS template (colors, fonts, spacing,\netc.) are customized in the theme.css file in your Project\nroot.\nIf you love css or want to make changes, you’re encouraged to\ncustomize your site! There is minor annotation in the css file, but\nhonestly this will probably just take quite a bit of Inspect Element to\nfigure out what to change.\nFonts: If you specifically want to know how to\nchange the site fonts, see here.\n\n\n\n",
      "last_modified": "2022-04-06T08:49:10-07:00"
    },
    {
      "path": "topic_5.html",
      "title": "Topic 5: Basic formatting",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_5.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nIf you’re familiar with markdown, use standard syntax to update font\nstyles, add bulleted lists, subscripts/superscripts, etc. (or learn more\nfrom the R Markdown\ncheatsheet).\nIf you’re not familiar with markdown and want an easier way to update\nformatting, there is a Visual Editor in RStudio versions > 1.4. It is\nabsolutely worth it to check it out for easier formatting of text,\nimages, tables, citations, and more. Read more about the new RStudio\nVisual Editor here.\n\n\n\n",
      "last_modified": "2022-04-06T08:49:11-07:00"
    },
    {
      "path": "topic_6.html",
      "title": "Topic 6: Internal & external links",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_6.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nLinking to internal\nparts of your site\nSometimes you’ll want to have a link to another page in your\nwebsite. How? Use the format\n[text](link_to_this_page.html).\nFor example, if I want to link here to the Resources page, within my\n.Rmd that looks like this:\n  Find more resources [here](resources.html). \nWhich, when built, looks like this:\nFind more resources here.\nExternal links\nSame thing, just add the full URL within the parentheses. For\nexample, to link to UCSB’s home page that would look like:\n  Learn more about [UCSB](https://www.ucsb.edu/).\n  \nWhich when built looks like this:\nLearn more about UCSB.\n\n\n\n",
      "last_modified": "2022-04-06T08:49:11-07:00"
    },
    {
      "path": "topic_7.html",
      "title": "Topic 7: Remove tables of contents",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_7.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nIn this template, you’ll notice that a Table of Contents is\nautomatically created on pages with headings (starting with Level 2\nheaders as top-level items).\nTo remove the auto-table of contents, open the _site.yml\nfile. In the output section, change the toc:\nand toc_float: fields to false.\nThat section in _site.yml would then look like this, and\nthe table of contents won’t appear:\noutput:\n  distill::distill_article:\n    toc: false\n    toc_float: false\n\n\n\n",
      "last_modified": "2022-04-06T08:49:12-07:00"
    },
    {
      "path": "topic_8.html",
      "title": "Topic 8: Dropdown list from a navigation bar item",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_8.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nIf you look at this template, you’ll notice that some navigation bar\nitems go directly to a single page, while the Modules item takes you to\na dropdown menu.\nTo create a dropdown menu of pages:\nCreate and save the pages as individual R Markdown documents as\ndescribed here.\nOpen the _site.yml file. In the navbar\nsection, create a menu with the pages as linked items using a structure\nlike this:\n   - text: \"Dropdown menu\"\n      menu:\n        - text: \"First dropdown item\"\n          href: item_1.html\n        - text: \"Second dropdown item\"\n          href: item_2.html\nThe example above would only work if the new pages were created as\nitem_1.Rmd and item_2.Rmd, so that when the\nsite is built the rendered item_1.html and\nitem_2.html files exist in the docs output\ndirectory.\n\n\n\n",
      "last_modified": "2022-04-06T08:49:12-07:00"
    },
    {
      "path": "topic_9.html",
      "title": "Topic 9: Changing site fonts",
      "author": [],
      "contents": "\n\nTO UPDATE THIS PAGE: Open and edit the topic_9.Rmd file,\nin the project root, to delete this placeholder text and customize with\nyour own!\n\nYou are welcome to use any fonts you want on your website. Here, only\nusing Google fonts is described (there are other methods for downloading\nfonts and adding, not included here).\nFonts are\nimported and specified in theme.css\nIn your Project in RStudio, open the theme.css file.\nNear the top, you’ll see some lines that look like this:\n/* Header font */\n@import url('https://fonts.googleapis.com/css2?family=Sanchez&display=swap');\n\n/* Body font */\n@import url('https://fonts.googleapis.com/css2?family=Nunito+Sans:wght@300;400&display=swap');\n\n/* Code font (Roboto Mono) */\n@import url('https://fonts.googleapis.com/css2?family=Roboto+Mono:wght@300;400&display=swap');\nThose are the import command to get 3 different Google fonts\n(Sanchez, Nunito Sans, and Roboto Mono). You can explore many different\nGoogle fonts here.\nUse different fonts\nFind a Google Font you\nlike.\nClick on the font. To the right of the font example text, you\nshould see an option to ‘+ Select this style’. Click the one(s) you want\nto select.\nThat will probably bring up a side menu. If you don’t\nsee that side menu, you can see your selected styles at any time by\nclicking on the top-right menu icon that is a grid with 3 squares and a\nplus sign - hovering reveals this is to ‘View your selected families’.\nIn the Use on the web section of the side menu that appears,\nselect the radio button for @import. It’ll look weird like this\n(for the Zen Dots Google Font):\n    <style>\n    @import url('https://fonts.googleapis.com/css2?family=Zen+Dots&display=swap');\n    <\/style> \nCopy everything BETWEEN (but excluding) the ending\n<style> and <\/style>tags\nPaste the @import line you’ve copied into the top\nsection of theme.css near the other fonts imported there.\nIt is now available for use in your theme.\nUpdate the css with your new fonts, replacing the existing font\nnames with the name you’ve imported. You might want to use a Find &\nReplace All if you want to make sure you’re updating a font everywhere\nit appears in the current theme.\nRepeat for as many different fonts as you want to update in your\ntheme.\n\n\n\n",
      "last_modified": "2022-04-06T08:49:13-07:00"
    }
  ],
  "collections": []
}
